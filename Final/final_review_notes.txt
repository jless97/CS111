// 6/10/17
// CS111: Final Review

=========================================================================================================================================================================
Topic 12: Device I/O and Drivers:
---------------------------------

- I/O bus:
A bus is a communication system that transfers data between components inside a computer or between computers. Main memory is attached to the CPU via the memory bus. Devices are connected to the system via an I/O bus. Connected to the I/O bus are devices such as PCI and some graphics. Lastly, there is the peripheral bus, upon which USB, SCSI, SATA are 
connected to, and the devices connected are disks, mice, etc. Faster buses, have to be shorter in length; and thus, a hierarchial system design was created, such that components
that require high performance are closer to the CPU, and those that don't need high performance are lower down on the hierarchy. Thus, disks and slow I/O devices are near the 
bottom of the system hierarchy, as they don't require high performance, aren't being used every second of every operation, and this allows for a large number of these slow 
performance devices to be placed on the peripheral bus. This is beneficial, as now we can implement RAID memory systems, which require a larger number of disks to be placed 
together to create the illusion of a single memory storage system. These systems can't be placed higher up on the hierarchy as they are slow, aren't being used as frequently, and
would take up space for the higher performance deviecs to be. 

Thus, to recap, at the top of the system hierarchy are the CPUs (which are used to carry out the functionality of the system), caches (which are used to make going to memory 
faster, and to make processing information quicker by reducing the amount of time, and the frequency of going to the slower main memory or disk), memory (main memory or RAM, which
is not persistent, and will be lost at a system power-off). Connecting the CPU to main memory is the memory bus, which is the top bus on the hierarchy. Next down the chain is the
general I/O bus, upon which is the PCI and graphics are connected. These are higher performance I/O devices, and such, are closer to the CPU. Lastly, is the peripheral I/O bus, 
upon which the slower I/O devices are connected to. These are the SCSI, SATA, and USB ports. But also connected to this bus are disks that are used for persistent storage, and can
be in the form of single disks, or RAID (Redundant Array of Inexpensive Disks), and other slow I/O devices. These are slow devices, and it takes a large amount of time to access 
these, which is why they are placed at the bottom of the hierarchy, which is fine as they are used less frequently. In addition, being placed this low on the hierarchy allows for
a large amount of them to be present. Lastly, there can be a network bus (or secondary device bus), upon which remote devices are connected to (i.e. internet connection via packets
being sent to and from the bus master).

- I/O Bus Terms:
So, as with every connection, there are two sides to the bus. On one side is the bus master, who is the one requesting the bus for use (i.e. a device such as the CPU). The bus 
master is the one that is doing the processing and makes request onto the bus to talk to the devices that are connected to the bus, to get some information or action from them. On
the other side, are the bus slaves, which are the devices connected to the bus. They answer requests from the bus master (i.e. CPU asks to store some information to disk 1, so it
makes the request, which travels along the bus, arrives at disk 1, which takes the memory given, and stores it, and then an interrupt occurs, upon which the CPU sees that the 
request was completed). Now, there can be multiple bus masters or users of the bus (i.e. if there is a multi-core system). Depending on how the system is configured, there could
be multiple CPUs or devices using the same bus, and thus there has to be a form of "bus arbitration" to determine who can use the bus at a given time. These are policies, and as
such there are mechanisms to enact the policies. Some policies could be based on priority, time, or bus cycles, to cycle through the devices that want to use the bus. 

- Device Interface:
Devices come with two components: 1. the interface (so that the software can communicate and take advantage of its operations), and 2. the internal structure (implementation).
Like with software, hardware has to have some form of interface such that other devices can communicate with them. The hardware devices have interfaces that allow the software to
not only communicate with them, but also to control the interface to perform the operation. For example, if the CPU wants to load some memory from the disk device, there needs to
be some interface such that the CPU knows to generate the address, the number of bytes to read, etc. The second part of the device: the internal structure consists of how the 
device implements the said interface setup with the software communicating with it. The device usually consists of some registers (i.e. the status, command, and data), which serve
as the interface. The CPU or OS of the bus master can make use of the registers to perform the necessary actions with the I/O device. First of all, the status register can be
read to get the status of the device (i.e. is it fetching some data, is it storing some data etc.). Next, the command register can be used by the OS to tell the device to perform
a given command (i.e. go fetch the data at address: 0x300). Lastly, (and note that this is for the simplest of devices), the data register is used to fetch the data from the device
and pass it to the bus master, or to transfer device provided by the bus master to go store it in some location on the device. 

- Device controllers:
So, what was just talked about was the device interface (i.e. a set of registers, and specifics on how to use/communicate with the device). The second piece of the device is the
internal structure and how the interface is implemented. This may consist of some forms of controllers (which may contain firmware => software within hardware), which control the
interaction between the bus master and the device, some memory (DRAM or SRAM or both), which store the persistent memory, or some light-weight memory perhaps in the form of 
caching, and lastly, some hardware-specific chips that perform various device operations. 

The device controllers are the pieces of the device that connect the device to the bus, and carry out the interaction between the bus master and the device. So, when a bus master
makes use of the device interface (i.e. using the device registers), the device controller oversees this interaction. For example, the bus master uses the command register and says
"hey bro, can you get me the info you stored at address 0x300 por favor?". And the device controller sees that the command register was initiated, and takes care of the operation.
In fact, the device controller is actually the device interface, and contains the registers mentioned above (i.e. status, command, and data). So when the bus master tries to
interface with the device, the device controller is the one that does the communication. Carrying on with the example above, the bus master initiates a load operation from the 
command register, so the device controller tells the device to go fetch the data at the given address. The device does so, and returns with some information, which the device 
controller then stores in the data register. The controller then generates an interrupt, which travels along the bus, and arrives at the bus master, which tells it "hey the I/O 
request has completed, I have the information stored in the data register". In addition, the device controller manages the Direct Memory Access (DMA) transfers for the device. 
DMA is the process that allows an I/O device to send/receive data directly from main memory, without going through the CPU. This process thus speeds up memory operations, as the 
there is no need for the CPU or process that is requesting the I/O operation to sleep on the operation, and wait for the I/O request to be completed, and for an interrupt to wake
the said process up to process the data. Instead, the process may initiate a DMA request, which goes to the device controller (or DMA controller), which will take care of the load
or store and transfer or retrieve said data to/from main memory without going back to the CPU. Thus, the CPU can carry on with its other operations, and not be blocked due to I/O.
Therefore, the device can be communicated with to the bus master: via the device controller registers (but can be done via polling or memory-mapped interactions). 

- Types of I/O: Polled I/O or Interrupt-based I/O or Direct Memory Access I/O:

Polled I/O:
Polled I/O is the process where the OS polls the I/O device throughout the I/O operation. First, the OS waits for the device to be ready to receive the I/O command request by
repeatedly reading the status register. Remember that the status register tells the current status of the device (i.e. busy doing some other request or ready to initiate new 
request). So, the OS polls the device (i.e. repeatedly reading the status register) until the device is ready to accept the request. When the device is free, the OS then sends the
data to the data register. This data can perhaps be some data that the OS is asking the device to store in the disk perhaps. Or, it could be some address, number of bytes, offset
that the OS wants to read from the device and then store the data back in main memory on the bus master. Remember, that while the OS is interacting with these device registers, 
that these registers are a part of the device interface, which are present on the device controller, which handles the interaction between the bus (the OS), and the device. Now,
after storing the data in the data register, the OS would then make use of the command register by storing a given command (i.e. a load or fetch). Basically, it tells the device
controller, the data is stored in the data register, and here is your command to execute. The OS then waits for the I/O request to be completed by again reading the status 
register. Therefore, the Polled I/O begins with polling the status register to wait for the device to be ready for the new I/O request, and then again polls the device to wait for
the device to complete the said I/O request. The last polling may get indication of success (i.e. here is the data you requested, success or it may get an error stating the data
you requested isn't there, or there was a device failure). 

    Polled I/O (or Direct I/O) Analysis:
Pros => Polled I/O is very simple and it works. 
Cons => Polled I/O used in this way is incredibly inefficient and inconvenient. The act of polling is a waste of CPU cycles, as it just keeps reading the status register until it
gets what it is waiting for. While the polling is going on, the CPU could switch to a ready process, and block this one, as it is wasting CPU cycles doing nothing. In addition,
polled I/O is very CPU intensive as each byte/word that is transferred or requested requires multiple instructions. Therefore, due to the wasted CPU cycles, idle processes which
are waiting to use the CPU, but can't do to this polling I/O request of a process, CPU intensive waste, we need another form of I/O interaction that is more efficient.

Interrupt-based I/O:
To make I/O interaction with a device more efficient, people came up with the interrupt. Instead of the process continually reading the status register (i.e. polling the device),
which wastes CPU cycles, we can have an interrupt, which the device presents to the process waiting for the I/O when it completes the I/O request. Thus, the process waiting for
the I/O request to complete can be blocked (upon initiating the I/O request), and another process can take control of the CPU, and do it's operations. When the I/O request for the
said process completes, then the device sends an interrupt to the CPU, which says that the I/O request is finished, at which time the CPU scheduler puts the process back on the
ready queue, and when the process gets its chance to run, it will deal with the I/O data received. 

When the I/O request is completed, the device raises a hardware interrupt, at which time the CPU traps into the OS to go to the interrupt handler (or the interrupt service routine)
to deal with the interrupt. Remember that there are many different types of interrupts, and usually the interrupt has some number, which the OS can use as an index into some
OS data interrupt signal structure that was pre-configured at boot time to know which interrupt handler to jump to at some pre-determined address. The OS then jumps to the 
interrupt handler to service the interrupt, and wake the process that was blocked on I/O up, and then return control back to the CPU. 

By using interrupts, the CPU can now save those wasted CPU cycles on polling, and jump to ready processes to run, and then jump back to the process waiting for the I/O upon its
completion. This allows for overlapping of processes to take place and I/O to devices to take place. In addition, there is another interrupt-based I/O optimization that can be
used which comes in the form of coalescing. In this optimizatino, when a device completes the I/O request, instead of immediately sending the interrupt to the OS stating that
it is done, it waits a given amount of time. By waiting, other requests to the same device for I/O perhaps may complete, and thus multiple interrupts can be coalesced together, and
sent at the same time. This would lower the overhead of interrupt processing, as the OS can now just process all these interrupts at the same time, instead of having to trap into
the OS to the interrupt handler upon each interrupt request. Therefore, the process, upon being unblocked, can process multiple I/O responses at the same time, instead of each
individually.

	    Interrupt-based I/O Analysis:
Pros => Allows for other processes to run, while a process blocks for I/O
Cons => Can have its downsides as well. For example, if there are fast devices that present their I/O requests very quickly, then an interrupt actually slows down the CPU 
execution, as the process requesting the I/O blocks, and another process runs, and then the interrupt comes in quickly, and then the process goes back to the first process. This
incurs a lot of overhead between context switches, and trapping into the OS via system calls as the device was able to finish the I/O operation quickly. Another problem with
interrupt-based I/O can be in the form of livelock (where the OS is stuck responding to a flood of interrupts, without actually making progress on servicing the interrupts). This
problem can be seen when communicating wth networks in which the OS gets flooded with a bunch of packets, all of which generate interrupts. The OS then is stuck processing all
of the interrupts, without ever getting back to the process which requested the I/O to process the I/O response. 

Lessons => Thus, if an I/O device is slow, then it is better to use interrupt-based I/O, so that other processes can run on the CPU, instead of the one process polling, and wasting
CPU cycles. On the other hand, if an I/O device is fast, then we can use polling, as the device will likely finish the I/O request within the first couple polls. Lastly, if a
device's speed is not known (i.e. it could be fast at times, and then slow), then a hybrid of the two can be used. For example, you could poll the device for a few clock cycles, 
and if the I/O request is still not completed, then you can block the process, let another process, and wait for the interrupt to be received from the device (two-phase approach).

Traps vs. Interrupts:
Traps and interrupts are very similar. However, traps come from the CPU, and usually are related to some form of error or fault that a process tried to do, which goes to the
specific trap handler. These traps can be like segfaults, in which a process tried to access memory that is either out of bounds or not allocated, which traps into the OS, then
to the proper trap handler, which services the trap. On the other hand, interrupts are caused externally, perhaps from some I/O device which signals an interrupt, which raises
a pin on the CPU, which then causes the OS to read the pin, see which interrupt it is, go to the proper interrupt handler, lower the pin, and then process the interrupt. Moreover,
traps cannot be disabled. They are always required to be on, as they deal with faults and errors that could put the system in a bad state, or could lead to the system breaking or
doing undefined behavior. Interrupts can be disabled, in which the device is told that it can't generate interrupts, or the CPU stops accepting interrupts for some period of time.
It may choose to drop the interrupt requests or it could have them as pending, and get to them at a later time. 

Direct Memory Access (DMA):
When using either Polled I/O or even interrupt-based I/O (i.e. using programmed I/O => direct involvement from the CPU), there would be a lot of overhead if dealing with a large
chunk of data that is either being sent to the device to store, or perhaps it is loading in a ton of data from the device. This would lead to the CPU wasting a lot of CPU cycles
for this process to continuously read or write the large chunk of data from the device. During this time, other processes could have gotten a chance to run perhaps. To solve this
problem, another form of I/O communication was devised, which is called "Direct Memory Access (DMA)". DMA involves the I/O interaction between the bus master and the device, but
does so with little CPU intervention, and can transfer data to or from the device directly to main memory, skipping the CPU. For example, to write data from the main memory of the
system to the device, the OS would interact with the device controller or DMA controller, and tell it that the memory it wants transferred to the device exists at this location
in main memory, and consists of this many bytes. After issuing the command to the DMA controller, the OS is done, and can then work on other things, like blocking this process and
moving to other processes. At this point, the DMA controller takes care of the I/O request, and upon completion raises an interrupt, which tells the OS that the DMA transfer is
complete, and then the interrupt is processed as normal. 

	  DMA Analysis:
Pros => Saves more CPU cycles. If large chunk of data is sent/received from the device, then the CPU would waste a lot of cycles processing the data and storing/writing it to 
main memory or the device driver. Instead, have the DMA controller deal with the I/O request (serve as the bus master), while the OS can choose to block the process and run 
another. Then just issue an interrupt, which tells the system that the DMA request has completed.
Cons => No negatives were discussed in lecture or in the book. However, I believe that DMA gives considerable control to the device, allowing it to act as the bus master, and then
directly access the main memory of the other system. If the device were corrupted, or malicious, then this would lead to significant problems. However, I am sure that a bunch of
security measures were taken to first authenticate that the device were safe, as well as other security (i.e. only allowing the device to access the memory location that were given
to it to access) measures taken to ensure DMA were in fact secure.

- Keeping Key Devices Busy:
It is important to keep key devices utilized and busy as much as possible. If a key device (such as a file system) were idle or was delayed, then this would affect the overall
run-time and throughput of the system. Thus, to keep the key devices busy, a possible solution would be to allow for multiple requests to the same device to be active at the same
time, instead of just issuing a request at a time. Therefore, you can queue the requests in a ready queue, and thus the device should never be in a state where it is waiting for
more requests, but should always be processing or doing work. Then, the OS after initiating the I/O requests can use DMA transfers, so that the CPU can be free to run other
processes, and block the processes requesting the I/O. This would allow for ready processes to run, and also reduce the overhead. Lastly, the device when complete can send an 
interrupt stating that the I/O request has completed. Moreover, the device, which now has a bunch of I/O requests, can use interrupt coalescing to chain a bunch of interrupts
together and send it as a single  interrupt to the system, which would process multiple interrupts at the same time, reducing context switch and interrupt handling overhead. 

- Chain Scheduled I/O:
This is the process of keeping devices busy processing I/O requests by sending multiple I/O requests to the same device (or other devices) and then having the devices store the
requests in a ready queue, while they process one request. In addition, have the devices and OS make use of DMA to save CPU time. Lastly, have the devices send interrupts to the
system when they complete their operations, and can make use of coalescing to send multiple interrupts at the same time as a single interrupt to reduce interrupt handling overhead
and context switching overhead. Using this chain scheduled I/O, the OS can multi-task (i.e. run multiple processes at the same time, or rather overlap these processes utilizing
the CPU, in addition to overlapping I/O operations), and then making use of interrupts to awake the blocked process. 

- Mechanisms of Device Interaction (I/O):
As of now we have talked about the policies used to communicate with the I/O devices. These can come in the form of polled I/O, interrupt-based I/O, making use of DMA, and using
coalescing to chain multiple interrupts together, and chain scheduled I/O (i.e. having multiple requests sent to devices for I/O, and having them store them in queues, and process
multiple I/O's one after the other to keep key devices busy). Theses policies make I/O interaction possible, and the system utilizes the device interface by communicating with the
various registers (status, command, and data) of the device controller or DMA controller of the device. Now, what exactly are the mechanisms that allow these policies to take 
place?

There are two methods of device communication: 1. Explicit I/O Instructions (i.e. OS sends/receives data to/from device registers) and 2. Memory-mapped I/O

- Explicit I/O instructions:
This method is what was talked about above (i.e. communicating with device registers of the device controller to do the I/O). For example, on x86, the in and out instructions
can be used to communicate with devices. So, the OS would fill the data register, talk to the command register by issuing a command specifying the port name of the device, and
the data to be sent. These instructions are obviously privileged instructions as they allow the OS to directly communicate with the OS.

- Memory-mapped I/O:
This method involves the device registers being available as if they were memory locations. This, the OS would issue the loads/stores to the registesr as if they were addresses.
Therefore, no new instructions would need to be created, and the interface would be exactly the same as talking to addresses (as talking to devices). 

- DMA vs. Memory-mapped I/O:
DMA is designed for large transfers of data. It allows for the CPU to be utilized efficiently, and doesn't involve the use of the CPU to do its transfers. However, there is some
overhead involved in DMA, as it has to deal with processing the interrupts. On the other hand, memory-mapped i/o allows for every byte to be transferred via CPU instructions. So,
the CPU is taken up by the said process to deal with sending every byte to the device (or processing every byte from the device). However, there is no interrupts to service and
there is low overhead. This is useful when updating some display via pixels, in which you process small bytes at a time (not a large transfer of data). Graphics adaptors involve
updating individual pixels, so you don't want to transfer a large amount of data, but instead you want to update like a million pixels (so DMA doesn't make sense). Don't need to
wait for the device at all, but you just store directly into the device via the pixels. 

- Intelligent I/O Devices/Device controllers:
The device driver can issue an I/O request to the device, at which time the device controller then processes it. It still contains its registers (i.e. status, command, and data), 
but it also has a memory pointer now, which may point to some shared buffer between the device and the system. This concept builds on memory-mapped I/O as the device and the 
system can now communicate using normal store/load instructions. They both share the buffer that was created, and the device driver can say something like "Hey for buffers 1 and 2
fill them up with the data you have at address 0x300." The device controller can then just fetch the memory, and store directly into the buffer. Therefore, normal instructions are
being used in this memory-mapped version. The use of queues can take place here as well, as the device driver can store many requests in the queue, and then the device driver can
process them one after the other, and store them in the buffers specified by the device driver. This allows for I/O to be done efficiently, but can also implement some network
protocols such as TCP/IP. 

Now, DMA is taking place, but in the background. No explicit DMA request was initiated, but both the device and device driver communicated that they would share this memory, and
that memory transfers would take place. In addition, we might not even care about the completion interrupts, because the device controller can just go onto the next I/O request, 
and then store it in the shared memory. These smart devices are using the primitives (i.e. DMA, completion interrupts, polling), but they are actually very complex. 

Pros => Smart controllers can reduce the overhead of starting new DMA requests. Using the shared buffers in the memory of the system (bus master), the device driver can initiate
many I/O requests one after the other and store them in a queue. The device controller, which also has access to the shared memory, can process the requests one by one from the 
queue and then directly store the data into the addresses (or buffers) specified by the device driver. Therefore, the system doesn't need to initiate any DMA requests, but can
just store requests in a queue (which the device controller can also see), and then the device can process them after completing the next one. Moreover, this eliminates the
need for the CPU to service interrupts, as they are using this shared memory, and the OS doesn't need to be notified that the data has been added to memory, as they both can
see the changes. 

- Device drivers:
These are pieces of software in the OS that know how a specific device works. There are many types of devices, all that may have different interfaces, operations, etc. Therefore,
the OS needs a way to know how to communicate with each of the different devices. Thus, the OS can store device-specific device drivers in the OS (which take up a large chunk of
the OS code) to deal with each device. 

==========
- Scatter/Gather:
Take a process' virtual address space, and there is some buffer in the virtual address space. This buffer is getting mapped to some number of pages in physical memory. The buffer
in physical memory can be separated into many different buffers. So, in the virtual address space, the buffer appears to be contiguous, but in physical memory, it actually could
be mapped into many different (discontiguous pages). Now, if someone wanted to do a DMA operation in and out of the virtual address buffer, it isn't just one DMA operation because
you would have to write or read from many different, not in the same place, pages. Thus there needs to be some method to get the distinct buffer in physical memory to appear to
be a single buffer for I/O purposes.

Scatter => I'm doing a read from the device, and I want to scatter the data around physical memory. So, lets say you want to write 10 bytes from the disk to physical memory, but
in physical memory, each byte has a different location. Therefore, the contiguous buffer in virtual memory, you want to take those 10 bytes, and scatter them to the different 
locations (pages) in physical memory.

Gather => I'm doing a write to the device, and I want to gather the individual pieces from physical memory into a buffer, and write them to the virtual address space of the device.
So, using the same 10 bytes to read from physical memory, you want to go to each of the 10 pages, and gather the bytes into a contiguous buffer, and then write these bytes to
the device's virtual address space.

How to do scatter-gather:
Have a MMU unit for the I/O, so every address for the DMA goes through the MMU. Or you can say that the DMA operation (using the same 10 byte example), is not actually a single
DMA operation, but is in fact 10 different DMA operations. And therefore, you can use chain-scheduled I/O to do the 10 operations by storing them in the queue, and processing
them one after the other. In the worst case, the DMA controller requires that transfers be page-aligned, in which you can copy the memory into a page-aligned portion in physical
memory, and then you can do the DMA operations from that portion in physical memory to their actual pages.

==========
- Hard disk drives:
These drives have been the main form of persistent data storage in computer systems for decades. Disk drives consists of a large number of sectors (512-byte blocks), each of
which can be read or written. The disk can be viewed as an array of sectors, which is the "address space" of the drive. Drives can have multi-sector operations that can read
or write 4KB at a time. However, drive manufacturers only guarantee that a sector write is atomic (i.e. 512-bytes), and that if there is an untimely power-off, the other bytes
of the large transfer (i.e. 4KB) will be lost, which is called a "torn write". 

- Disk Geometry:
Platter => circular hard surface on which data is stored persistently by inducing magnetic changes to it. Disks can have 1 or more platters, and each platter consists of 2 sides
called surfaces
Surface => a side of the platter, which is made up of hard material (e.g. aluminum), and is coated with a thin magnetic layer that enables the drive to persistently store bits
even when the drive is powered off. As drives are meant for persistent storage (i.e. retain the information even when powered off), their magnetic coat allows them to do so. 
Spindle => The platters are bound together around a spindle, which is connected to a motor that spins the platters around (when powered on) at a fixed rate (usually 15000 RPM). 
Track => Data is encoded on each surface in concentric circles of sectors, where one such circle is a track. A single surface contains many 1000s of tracks. 
Disk head => To read/write from the surface, there is a mechanism that allows us to sense (i.e. read) the magnetic patters on the disk or to induce a change (i.e. write) them. The
disk head allows this sensing or changing of data to take place, and there is one disk head per surface.
Disk arm => the disk head is attached to a single disk arm, which moves across the surface to position the head over the desired track. 
Cylinder => the corresponding track of all surfaces (i.e. track 1 for all surfaces might be on cylinder 1)
Disk address => The disk address is a tuple of the cylinder, head, and sector => Ex: <Cylinder 1 / Head 2 / Sector 4>. In this example, lets say that cylinder 1 is the outermost
circe that can be traced on a surface, and that there are 3 platters. Therefore, there are 6 surfaces, and thus, there are 6 possible tracks for this cylinder. As there are 6 
surfaces, there has to be 6 heads to read each surface, so head 2 states which track is being read. Finally, each track is made up of a different number of sectors. For example,
if this track has 2048 bytes, then there are 4 sectors. 

- Measuring disk performance: Rotational delay (of a single track), and seek time (of multiple tracks):

Single-track latency: Rotational delay:
So, each track consists of a certain number of bytes. Each track therefore consists of a certain number of sectors. For example, if the track consisted of 2048 bytes, then the
track also consits of 4 sectors. The platters are being rotated at a fixed RPM, and thus, to read a certain sector, one has to wait for the desired sector to rotate under the disk
head. This is known as rotational delay, as it involves waiting for a given sector to rotate under the disk head, so that reads/writes can take place. In the worst case, the disk
head just missed the sector when it received the request, and you have to wait for the surface to do a full rotation.

Multiple tracks: seek time:
Now, every surface has a certain number of tracks (not just a single track). Thus to read any given track, the disk head needs to position itself over the desired track, in 
addition to being positioned over the desired sector. Therefore, the disk arm needs to move (which is at a fixed speed) to hover over the desired track. This action of positioning
the disk head over the desired track is known as seeking, and in the worst case, the seek has to go from the outermost track to the innermost track, or vice versa. There are 
several phases involved in a seek: 1. acceleration (starting the disk arm to get moving), 2. coasting (once at full speed, traveling to the track), 3. deceleration (slowing the
arm down as the desired track draws near), and 4. settling (positioning the head carefully over the desired track). Note that tracks are often skewed, meaning that on a given
track, the sectors aren't sequential (i.e. 0, 1, 2, 3...). They are in fact skewed (i.e. 0, 2, 1, 3, 4, ...) to allow for sequential reads to take place. They are skewed because
it takes some time for the head to be positioned over the desired track when moving from sector to sector, thus it makes up for this time to allow for sequential reads. 

Maximizing performance, reducing overhead:
Disks have dominated file systems, and thus we need to organize the file systems into cylinder cycles so that the we can reduce the seek overhead needed to do I/O. We want to
stay in the cylinder as much as possible before swapping to next swap. Also, we want to maximize transfer sizes (i.e. 1 MB perphaps) to reduce rotational latency delays as well. 
Poor latency (i.e. wait for a given track to be as close as possible before doing a read), but high throughput (because now we can read 1 MB of data). The latency (i.e. the time
it takes for a single operation to take place) is slower, but the throughput (i.e. the time it takes for multiple or all operations to take place) is higher as we now read in more
data at once. 

- Performance: SSDs vs Disks:
In modern times, people are making more and more use of SSDs vs. disks. They have been around for a short time, but make use of semi-conductors, and have no moving parts, vs. disks
which are made up of the mechanical problems discussed earlier. Therefore, they can reduce the latency significantly (no seeks, rotational delay), and can thus increase throughput
as well. Moreover, they could be more reliable as disks fail. Note though that SSDs eventually wear out due to overuse. But SSDs can be more scalable as well, as they can give
us as much storage as we need now. And more available than the drives as they fail every 7 years or so. SSDs are more expensive (by about 3x), but this may be worth it, if you
are willing to get more storage, more reliability, and improved performance and efficiency.

- Making use of caches when reading from disk:
All disk drives have caches, which on disks are called track buffers. The cache is a very small amount of memory (i.e. 8-16 MB), which the drive uses to hold data read from or
written to the disk. Disk reads or writes are always going to be slow (or slower) as there are mechanical parts involved (i.e. seek time and rotational delay). Therefore, to make
them as fast as possible, we make optimizations (i.e. reading larger number of bytes, or waiting until the last possible second for the track to be near before starting the read),
but we also make use of caching. For example, when reading a sector from the dis, the drive might decide to read in all the sectors on the track, and store them in the cache. 
Therefore, if subsequent requests (making use of temporal and spatial locality) are to the same track, then we remove the time to read from the disk, and instead just read from 
the cache. 

For writes, the process is different. There are two options that the disk acknowledges that a write has completed: 1. when it has put the data in its memory or 2. when it has
actually been written to disk. The first one is known as "write back caching" or "immediate reporting" as it doesn't actually have the data stored in the disk yet, but instead
has the data stored in the cache. This can make the drive appear to be "faster", but can be dangerous for example, if the data needs to be written in a desired order, or if some
failure takes place, where the data was reported that it was already committed to disk, when in fact it was just lost now (as it was only in the cache). The other type of write
acknowledgement is known as "write through" where the write actually occurred, and was committed to disk. This is more secure, as if a failure occurs, then the data was actually
written to disk, but can be less efficient as writing to disk is very slow. In database systems, this time of write could be more necessary, as it is important that the data was
actually committed to disk. In these systems, we can reduce the performance, if we can ensure success of the operation. However, it should be noted, that even with write-through,
the writes don't have to occur immediately (like in database systems), but multiple writes can be buffered to accumulate a large number of writes, and then write all at once. 

In summary, write-back caching is the process of writing the data to the cache, and not yet to disk, and then telling the system that the data had been written immediately, which
boosts performance, but can be risky if failures occur, or if you want writes to happen in a certain order. On the other hand, write-through caching involves writing to the cache,
and then writing immediately to the disk. This takes longer, but ensures that the write actually took place, and also ensures correct ordering of writes.

- Random vs. Sequential I/O:
When dealing with I/O, there are two important workloads to consider: random and sequential. Random involves reading small amounts of data from random locations on disk, while
sequential involves reading large number of sectors consecutively from the disk. To understand the different in performance between the two worloads, first it is important to
know about the different types of disk on the market. The first is "high performance", where engineerings tried to make the disks spin as fast as possible, thus to have low
seek times, lower rotational delays, and to transfer data quickly. The other type of disk is the "capacity" disk, which has slower drives, but allows for more bits to be packed
into a smaller space, thus giving more capacity. Now, we can analyze the two different type of workloads. The performance of the disk operations will always be faster when doing
sequential operations on the disk vs. doing random read/write operations. The reason why is obvious: random ops involves having to move around from track to track, which would
incur a large amount of overhead due to seek time, and then rotational delays as well. If doing sequential reads, then there is no seek time, and only rotational delay (if you 
missed the start of the sector that you want to read). In addition, operations involving larger transfers of data perform better than those with smaller transfers for the same
reasons. Large transfers can amortize the overheads of seek and rotational delays by reading or writing more memory at once. In addition, by doing this, the caches can be used
more effectively, and reduce the time to go to disk completely (in some cases, if the data is in the cache). 

- Buffered I/O:
Performing larger transfers of data are more efficient and have better disk performance than doing smaller transfers. The reason (as already discussed) is that disks have high
seek and rotation overheads, so we want to reduce these as much as possible. By reading or writing more data at once, the overheads can be amortized. In addition, each transfer
has per-operation overhead that come in the form of setting up the operation, the device executing the new operation, and then dealing with the completion interrupts. Thus,
if we can cut down on the number of transfers, then these overheads would be reduced.

However, despite fewer/larger transfers being more efficient, they aren't as convenient for most applications. Most applications, have natural record sizes that tend to be 
relatively small. But, we still want to make use of reducing overheads, and thus, still want to use large transfers. To deal with this issue, the OS can make use of buffered I/O,
which is the process of using the cache. For reads, the cache stores the recently used blocks, and thus upon a certain read (say a sector), perhaps read in the entire track, and
store it in the cache. Now, if the program exhibits good locality, then it will likely access other sectors on the track, and thus for future reads, can just access the cache and
not go to disk. For writes, a similar process can be utilized (which is the buffering of the writes). When the OS issues a write to the disk, instead of writing it immediately
to the disk (which would lead to more transfers), instead buffer it in the cache, and accumulate these writes, and then flush them out as the cache fills up. By doing this, instead
of doing many small writes (more transfers, more overhead), you can do one large write to the disk. Now, for large writes, this may not be stored in the cache, and you can just
write to disk. However, for the small writes, it would be better to accumulate them in the cache buffer, and then transfer them all at once upon memory filling up in the cache. 
By buffering the writes, you could potentially have a lot of writes to the same cylinder, and thus these can be done at the same time (with no seeking). Or, sometimes, you might
write the same value many times, but now the buffer, can just write the last write. Write-back caching can make the best use of this buffered I/O. 

Note that the OS is the one that has the cache, and not the disk (although the cache also has a cache). Therefore, upon doing a read of a single sector, the OS instead tells
the disk to do a read of the entire track perhaps, and stores this in the buffer. This process enables "read-ahead", which enables the OS to read and cache blocks that are not
yet requested by the process. For writes, the OS still makes use of a buffer (perhaps a write buffer), which accumulates the writes done by a process (or several processes), and
at a given time (perhaps when the cache nears being full), flush all writes out to disk (do multiple writes, but as a single write). Therefore, the number of transfers is reduced
due to accumulating (or "write aggregating") writes, but also perhaps if the process does many writes on the same data, then we can just do the last write. Therefore buffered I/O
is a way to make the best use of larger/fewer transfers to disk, but can also cut down on even going to disk (in reads perhaps from the same track, which is now in the cache). 

- Deep request queues:
By having more requests in the queue, you can keep the device as busy as possible. If you have a smart device, then you can have the disk do the scheduling on its own. To increase
throughput, you can have a long queue, and the device will be idle less. However, if you want to have better latency, you have a short queue, as larger queues may lead to 
scheduling of given operations (thus arriving earlier in the queue) to be processed later on. Thus, you will have good throughput, but perhaps worse latency.

To get a deep queue, you can have many processes that are making requests. Or perhaps, you are doing asynchronous I/O, or having threads doing I/O, and thus a process can be
issuing more requests.

- Disk scheduling:
As previously discussed, we want to keep the device idle as little as possible, and keep it as busy as possible. Therefore, we can issue it a ton of requests, which it stores in
a queue, and gets to them one by one. However, which request in the queue should it process? This question must be answered in a performance sense. For example, if the requests
that come in each have large seek times in between them, then we want to avoid this. We want to reduce the seek time and rotational delays, and thus, we want to schedule operations
to be done in a performance-minded order. Thus, the disk scheduler comes into play. Unlike with process scheduling, where the job time was unknown, disk scheduling knows a little
something about the time it will take to complete the request. This knowledge comes in the form of knowing the seek time and the rotational delay time that it will take to get
to the next request from the current location. Thus, the scheduler can pick the request that will take the least time to service first. 

One approach is "Shortest-seek-time-first (SSTF)", in which the scheduler orders the queue in terms of which track is nearest at the given time, and thus which will be the quickest
to execute. However, tracks towards the outside of the disk, have more tracks, and thus, the nearest track isn't always the nearest block to access. Therefore, the:

"Nearest-block-first (NBF)" scheduler can be used => which schedules the request with the nearest block address next. However both SSTF and NBF have a problem, which all schedulers
most think about which is "starvation" (i.e. processes or requests that won't be serviced for an unbounded amount of time). Some schedulers are ok with starving certain processes
or requests (i.e. priority-schedulers perhaps), but others may want to avoid starvation. Enter:

"Elevator (aka SCAN or C-Scan) Scheduling" => in which the disk head is moved back and forth across the disk and services requests in order across the tracks. Each sweep may
process a block on a given track, and if another block on the same track is received, then skip it, and get it on the next sweep. Thus, ensuring that every track will get processed
at some point (i.e. no unbounded wait time, no starvation). Other scheduling types:

"Shortest Positioning Time First (SPTF)" => takes into account the rotational delay issues, and thus if the seek time is much faster than rotational delay, then use SPTF, but if
the seek time is the bottleneck, then using SSTF or NBF or Elevator (SCAN or C-SCAN) may be the more effective. 

=========
- Data striping:
The idea of striping is that there is a striper that takes in three different writes (i.e. A, B, and C), and then the striper does 1 write to three different disks (A to 1, B to 2,
and C to 3). This increases the overall throughput of writes, which now does writes 3 times as fast. For reads, you can do reads from 3 disks perhaps, and send them all to the
initiator. For example, when streaming from Netflix, you can stripe a movie across multiple disks, and then read in from the different disks, instead of just buffering from a
single disk. If you need more bandwidth, then you can use striping to get more. 

However, with striping the striper coudl be a potential bottleneck as it must be able to handle the throughput of all the different targets. The striping agent could be the 
bottleneck. To summarize, data striping is the process of spreading requests across multiple targets. This allows for throughput to be increased, and fewer operations to occur.
For example, if you wanted to write three different pieces of information, you could do them all at once, by giving the multi-request to the striping agent, which writes each
of the three requests to a different target. This increases throughput, can reduce the number of operations, and exploit more parallelism.

- Data mirroring:
Mirroring is done for reliability (making more copies). You write the same piece of information to multiple targets, but when reading, you can treat it like striping. Therefore,
when doing mirror writes, you make use of redundacy and have multiple copies of the same information in different disks (in case one disk were to fail). This spreading of data
to different targets does have its cons. First, it increases the amount of write traffic, as now a single write has become multiple writes (for multiple targets). In addition,
mirroring requires more storage than before (due to having copies on multiple machines). When doing reads, you can read just like in striping, as now each target has all the 
information. So, if you wanted to read in A, B, and C, you can read each of them from a different disk.

To summarize, mirroring is the process of creating multiple copies of data (being redundant) to have more reliability. This ensures that if a disk were to fail, then there would
be another copy of the data. Writes involve writing the same thing to multiple copies. Reads can make use of the striping reads, as now the targets contain all the data you sent, 
and thus, you can read a different piece of information from the different targets. However, it uses more storage, and leads to more write-traffic.

- Parity/erasure coding:
The idea is that you start off with a bunch of blocks, but when writing them, you write some function of the blocks to each of a bunch of targets. With erasure coding, it is a 
complex function that allows you to recover information written. This process is used for efficiency (less storage). Also called N out of M encoding, in which you accumulate N
writes from the source, and compute M versions of that collection. You then send a version to each of the M targets. 

To summarize the three types of read/writes used for RAID systems (i.e. striping, mirroring, and parity/erasure coding):
Striping is used to increase throughput and bandwidth. It is the process of writing out a certain amount of data, but only writes some of the data to each of the multiple targets.
There is a striping agent which takes the request (i.e. A, B, and C), and then perhaps writes A to disk 1, B to disk 2, and C to disk 3. Therefore, the performance boosts can be
seen, as three writes could be treated as a single write. This would lead to increased throughput. In addition, when doing reads, you can read multiple items at the same time (i.e.
read A from disk 1, B from disk 2, and C from disk 3). This leads to fewer operations per second, and increased throughput, however it does have its downside. First, the striping
agent must be able to process multiple requests at the same time, or thus serve as a bottleneck. Also, it expects that the source is able to make parallel requests. If the source
can't do parallel requests, then it has no use for striping.

The other form is mirroring which is used for reliability and redundancy. It is the process of taking some amount of data and writing the same data to multiple targets. By doing
this, multiple copies of the data exist in different places, and thus if one disk were to fail, and the data lost, the data would still be present in a different location. In terms
of reads, mirroring can make use of the same benefit of stiping reads, in that it can read multiple items from different disks (as they all have all the information). Mirroring 
does have its negatives that come in the form of having to have increased storage, and adding write traffic on the source. Now, instead of 1 write, there are 3 writes.

Lastly, parity/erasure coding is used for efficiency, and is the process of accumulating some N writes from the source, and computing M versions of that collection. Then each of
the M versions is written to a different target. Problems with this form are due to a greater source computational workload (i.e. generating the function), and also the act
of deferring writes (so that you can accumulate parity blocks) could lead to losing such writes.

- RAID (Redundant Array of Inexpensive Disks):
RAID is a technique to use multiple disks to serve as one to create a faster, bigger, and more reliable disk system. It can be faster by utilizing have different data on each
of the disks. It can be bigger by having more disks. It can be more reliable by having redundant copies present on the disks, or by having parity/erasure coding which allows for
the recovery of certain lost data. To the user of the RAID, the interface is exactly the same, as it appears as a single disk (transparency). To the file system using the RAID, it
just appears as a big disk. When the file system issues a "logical I/O request", the RAID must determine which disks it needs to use, and then issues "physical I/O responses". 

Different types of RAIDs (RAID 0: striping, RAID 1: mirroring, RAID 4/5: parity-based redundancy):

RAID 0: striping:
There is no redundancy involved in raid 0, and thus it isn't actually raid. However, it serves as an upper-bound on performance, so it is analyzed. It consists of using striping,
which stripes data across multiple disks. Therefore, if a file system wants to write A, B, C, and D to the RAID disk, the striping agent may write A to disk 1, B to disk 2, C to
disk 3, and D to disk 4. Striping is used to extract as much parallelism from the system as possible, and can process parallel requests from the file system, and then have the 
striping agent store them to different targets. In addition, reading from the disk should be faster as well, as the targets all have different data, so if parallel read requests 
are made, they may be able to be serviced at the same time. 

Chunk size is of importance when studying performance of the RAID system. If the chunk size is small, then many files can be striped across many disks. This would allow for
increased parallelism of reads and writes to a single file; however, the positioning time to access the blocks across multiple disks increases. If using a big chunk size, then
the positioning time could be reduced, if the entire file were to exist on a single disk, but it could reduce the parallelism, as you can't do as much reads/writes from a given
file. 

RAID 1: Mirroring:
RAID 0 made use of the striping idea, but didn't exploit any reliability/redundancy. Raid 1 does make the disk more reliable by having multiple copies on multiple disks (i.e.
the mirroring of data to multiple disks). Therefore, if a disk were to fail, then there should be another copy on another disk. Raid 1 can make use of striping and mirroring, so
that it can have some improved performance, while boosting the reliability of the disk system. Therefore, while reads can read from whichever copy it wants (the performance could
be increased); however, when doing writes (the system must update all of the copies). Therefore, the performance of writes decreases due to the multiple writes that have to take
place. However, note that the writes can be done in parallel. In addition, raid mirroring reduces the capacity of the system to take on more data, as the disk space is cut in half
if having to do mirroring. Note that raid 1: mirroring also has a consistency problem due to the fact that it has to write the same data to multiple disks. If the data was 
successfully written to one disk, but before the other could be written, a system failure occurred, then the "identical" raid disks would be inconsistent. We need to ensure that
the writes are atomic, which can be done by using write-ahead logging. This is the process of writing to a log, what you are about to do (i.e. update two disks with new info) 
before actually doing the writes. Thus when a failure occurs, you can enact the recovery procedure which replays the transactions in the log to make sure that the disks are
in fact consistent. 

Raid 4: Parity
Redundancy to the disk can be added in the form of parity, which unlike mirroring can save space and boost capacity. However, parity based RAID systems due have reduced performance
as now each read or write must act on the disk containing the parity blocks. The idea is that for each stripe of data, there is a parity block that stores the redundant info. By
using the XOR function, the parity is able to be generated which ensures that if a single block of the stripe is lost, the lost stripe can be recovered using the other blocks
and the parity block. For each stripe, XOR all of the bits, which returns a 0 if there are an even number of 1's, and a 1 if there are an odd number of 1's. Therefore, if a block
of a given stripe was lost, then using the parity information, it can be recovered and reconstructed. Parity based RAIDs can make use of full-stripe writes, in which if the system
is writing data to blocks that are on each of the stripes of a full-stripe, then they can be optimized to be written all at once (and calculating the parity block). However, raid-4
suffers from the "small write problem", in which the system has to first read in the data at the blocks that need to be written and the parity block before writing them (as the
new parity value needs to be computed). Thus, there are one read and two writes that need to take place for a single write. For example, assume there are 4 disks of the RAID
system that store data, and 1 disk storing the parity information. Now assume you want to write to a single block in disk 1. Before doing this, you must read the parity block, and
then you can write the data to the block, but also must write the new parity value (if it changes due to the new information). 

Raid 5: rotated parity:
Rotates the parity block across the disks, to combat the small-write problem. 

- Parallel I/O:
Why would I want to use parallel I/O?:
I might want to use parallel I/O if I want to do multiple actions at the same time (i.e for Project 4B: reading temperature sensors, while writing reports to stdout, while reading
commands from stdin). Or perhaps, I am a server that needs to accept numerous parallel requests from many clients. 

How to enable parallel operations?:
1. Threads
One such solution is the use of threads. Where one thread can satisfy one request. By using threads, one thread blocking doesn't affect other threads from executing. If having
many operations that do very different operations, it makes sense to use threads to each handle a different operation. 

2. Non-blocking I/O
You have something that would normally block, but it is actually blocking to wait for something that isn't there yet. So, you poll it for example, to wait for the data to be there
and if it is there, then you read it. So you could set a file to be non-blocking, in which you would get an error if the file would block. It doesn't actually give an error, but
it will say something like "the file would have blocked", and then return, allowing you to poll other data perhaps, or have a while loop polling the same file. Thus, non-blocking
I/O works very well if you have one or two things that doesn't happen very often, but you just want to check to see when they do happen (and there is nothing wrong with checking). 

3. Multi-channel poll/select:
There are multiple possible input sources (and you don't want to poll every source asking if they are ready). Instead, you create a list of stuff that you are interested in, and
tell the OS to wake me up when each of the things happens. When any of the things happens, the OS wakes me up, and tells me which one happened. 

4. Worker threads:
Create a thread to serve each new request. Thread creation is expensive, and instead of doing thread creation/destruction, you do thread waking up/blocking, which recycles through
the working threads. When a thread finishes its request, it blocks, and it is awakened when a new operation needs servicing. This is a common model for web servers to use to 
handle parallel operations using threads. 

Non-blocking I/O is very simple to implement and is good when there are events we are waiting for that happen very infrequently. Poll/select is good when there is a large number
of sources, but they also don't happen very often. Parallel working threads are good for complex operations that may happen very frequently, in which the threads service them, go
to sleep when done, and wake up to process new request. None of these options are good for massive parallelism. 

- Asynchronous I/O:
A much more large-scale operation, in which I am servicing requests from a large number of sources at all times. Many parallel clients with many parallel requests, in which I have
a deep queue, and use interrupt completions. It is the process of mediating a lot of requests, where we want to queue many parallel operations. 

What can we do?:
Scheduling Asynchronous I/O:
For every transfer that I want to do, I fill out a bunch of things (which file descriptor, what offset, request priority, byte count...), and then queue it for the file/device, and
then there is a device or file that processes the queue. So, the system queues them, and then dishes out the ops to others who will send back a response

Asynchronous Completion:
You can poll the status of any operation, or you can block, and say wake me up when any of the operations completes, at which time I wake up and process them. Or with fsync, you
can say, "hey bro, i need that shit now". 

Completion Notifications:
When they complete, I can specify what I want (i.e. have a notify signal, or a signal number, or even specify a signal handler to deal with the completion). 

Signals for Event Notifications:
Signals were originally designed for exceptions that occurred infrequently, and most resulted in failure. However, they were being used now for event notifications, but signals
don't queue, and we don't want to lose certain events happening. So, a new event handling process was created, which modeled interrupts, and is known as sigaction(2). Once the
function is invoked, it can block other signals from being processed (they are queued), and it is a more precise signal handling mechanism, and then reenables the queued signals
upon being returned. 

==============================================================================================================================================================================
Topic 13: File Semantics and Representation:
--------------------------------------------

- Introduction:
Thus far we have seen two key OS abstractions: the process, which is a virtualization of the CPU, and the address space, which is a virtualization of memory. They have allowed us
to run many processes on the same machine (perhaps at the same time), and in their own private address spaces. Another abstraction that needs to be added is the idea of persistent
storage. Two types of persistent storage devices have been discussed already: hard disk drives (in the form of single disks, or RAID systems), and solid state storage disks (SSDs).
Persistent storage devices store data permanently, and thus are a required necesssity of any computer system as people want to have data be retained even after powering off the
system. Thus far, we have computer systems that have volatile memory, that allows for memory to be saved as the computer system is running, but upon powering off, all such data
will be lost. 

- File system semantics (i.e. File Names/Introduction, Interface):

- File Names/Introduction:
We now discuss the interfaces involved with the UNIX file system. As with all interfaces, there needs to be some abstractions, and these are "virtualizations of storage". Thus, we
have seen three types of virtualization (the CPU, memory, and now storage). There are two types of storage abstractions: the file (which is just a linear array of bytes that are
read or written), and the directory (which is a form of metadata that stores inode, filename pairs). Each file consists of a low-level name, which is called the "inode number", as
well as a user-defined name given by the user. The OS need not have to know much about the files, but rather the file system just makes sure that the file data is stored
persistently. 

A directory is also a type of a file, and thus has an inode number, and a user-defined name, but it doesn't contain data in the same sense. Instead, directories
contain a list of (user name, inode name) pairs known as directory entries. Directories can be placed within other directories, and thus a "directory hierarchy" is created, which
is the outline of a file system. The directory hierarchy begins with the "root directory, '/'", and then just concatenates on other directories and files to the pathname. The 
path from the root to any file is called the "absolute path", while the path from a given directory to a file is called the "relative path". Each new directory/file in the pathname
is separated by a single slash. 

The type of a file is specified by an extension, which takes place after a '.' and then is some format (i.e. .c, .txt, .doc, .jpg, etc). Pros of the UNIX filesystem, is that it 
provides a convenient naming scheme for all files. Names are very important in systems, and thus the UNIX system allows for a unified way to access files on disk, USB sticks, CD,
etc. 

- File System Interface:
The file system interface consists of creating, accessing, deleting, renaming files.

Creating files:
Files are created using the open() system call. The open system call consists of various parameters, with the first being the name of the file to open (or create), the second
being a list of options to pass, so the program can do certain things with the file, and the third being a list of permissions to grant owner, group, other for the file. If the
user specifies the use of the O_CREAT flag, the program will create a new file with that name of the first parameter. O_WRONLY => write only, or O_RDONLY => read only, and O_TRUNC 
means that if the file already existss, then clear it to have a size of zero bits, and remove existing content. For permissions, you can have stuff like S_IRUSR => read only for
the user of S_IWUSR => user can write only, etc. There is also the creat() system call, which is like the open() system call, except creat() can only create files, while open()
can do all sorts of things. The most important aspect of the open() system call is what it returns => which is a file descriptor. A "file descriptor" is just an integer, that is
private per process, and is used by UNIX systems to access files. By specifying the file descriptor, a user can read/write from a file. 

File descriptors are a "capability" => which grants the owner of the file descriptor certain abilities to perform certain operations. Capabilities will be discussed more in future
chapters, but this one is particularly important as it grants the process (given the right permissions) the power to operate on files. Note that there are three reserved file
descriptors (i.e. 0 => stdin, 1 => stdout, and 2 => stderr). These are reserved for every process, and thus the first available file descriptor is always 3. 

Once we can access files, we can now read and write to them, and this is done using the read() and write() system calls. They each have three parameters, with the first being
the file descriptor (i.e. the file to read/write to/from), the second being a buffer containing the bytes to write to the file, or a buffer that will store the information read
from the file, and the last being the size of bytes to read/write to/from the file. Using the "strace" command, a user of the command-line can trace the list of system calls 
involved with a given command. For example, with the "cat" command, it consists of the open(), read(), write(), and close() system calls. There are two types of read/write I/O
that can take place (as discussed in the last section): sequential and random (even though it isn't actually random). Sequential I/O involves reading contiguous bytes of memory
from a file, or writing to contiguous bytes of a file. Random I/O involves reading/writing from specific offsets within a file. This is accomplished using the lseek() system call,
which takes three parameters: the first is the file desricptor, the second is the offset (which positions the file offset to a particular location within the file), and the third
is called "whence", which determines how the seek is performed (i.e. offset from start of file, offset from current location, or offset to the last byte of the file). 

As discussed in the last section, there are various forms of writes that can take place to memory, or to peripheral devices. Using buffered I/O, caching, writes can either be 
performed at the exact time that the command arrived, or the writes can wait, and be executed at a later time. The process of writing immediately to disk, upon writing to the
cache is known as "write through". The process of writing to the cache, and then waiting until a later time to write to disk is known as "write-back". Write through comes with
the benefit that the write actually occurred, and is acknowledge immediately. However, it isn't the most efficient performance-wise as writing to disk is a very slow process, and
having to perform the write to disk every time a write comes in (no matter if the write is small or large) is a wasteful process. This can be beneficial, and in fact necessary, if
working with systems (e.g. database systems), where performance is sacrificed for the expense of accuracy of data. Write-back reduces the overhead of writing to disk everytime and
buffers the writes in the cache. After writing to cache, it will notify the process that the write was successful, when in fact the write hadn't happened yet. Therefore, it can 
buffer (or wait) to write to disk in a process known as "write aggregation" until a certain time. The reason to wait is that writing to disk is a costly procedure, and it would
be more performance-efficient to do a large transfer to disk, and have to go to disk fewer times, then do many small transfers to disk, frequently (as seeks and rotational delays
incur serious overhead). In addition, waiting is beneficial, as perhaps some process writes the same data many times, and thus we can just write the last write done. Or perhaps,
the process may have done a sequence of writes to many places, but one such place had many writes to the same cylinder, and thus we can do these writes together. The disadvantage
of write-back is that it may not be true. What I mean by this is that it tells the process that the write was a success before actually writing to disk (so the data is not yet
persistent). Then at some later time, it attends to write to disk, but what if a power failure were to occur before any writes or in the middle of the writes? This would result
in the disk being in an inconsistent state. In the face of this failure, there are several procedures to deal with it, with one being the process of keeping a write log. The idea
here is to write to a log stating the operations that the OS was going to do to the disk (i.e. write to here, and then write to here). Thus, if a power outage occurred, a recovery
procedure could take effect, in which the OS runs through the log, and makes sure that all operations actually went through. If some didn't, then these can be properly serviced.

When writing immediately (via write-through caching), the fsync() system call is used. It tells the file system to write the data immediately to the file system, instead of 
buffering the writes for a later time. Database management systems need writes to be forced to disk from time to time to ensure correct recovery protocol, and thus fsync() is
used. When a process calls fsync() for a particular file descriptor, the file system forces all "dirty" data to the disk, which writes all the cached data to the disk at this
given instance. 

Renaming Files:
Renaming files or directories (which are also types of files) are achieved with the mv UNIX command. The mv command can moves files to different locations, or different 
directories, but it can also rename files. It is able to rename files using the rename() system call. Renaming files is a process that needs to be done atomically, which means
that it is an all-or-nothing operation. It can't be the case that a file is half renamed, but then a power outage occurs, and the other half isn't completed. 

Getting Information About Files:
A filesystem not only stores and retains files, but also keeps information about each file, which is known as its "metadata". The metadata for a file can be retrieved using
the stat() or fstat() system calls. These calls take a file descriptor as a parameter, and then fill a "stat" structure with the file's metadata. The metadata consists of a file's
inode number, protection (mode), permissions, creation time, modification time, last access time, number of hard links, user id, group id, size, block count, etc. Now, using the
stat() system call allows a program to retrieve the metadata of a file, while the file system stores this metadata in the "inode" of a file. The inode contains the metadata, and
a file is referenced by its inode number, or username.

Removing Files:
Files are removed using the rm or rmdir UNIX commands. However, the system calls that are used are unlink(), which accepts one parameter, which is the file descriptor. The reason
that the system call is called unlink() is because a file in the UNIX file system isn't removed from the system if it still has a number of hard links pointing to it. Once all
the hard links are removed from the given file (i.e. the link count is 0), then the file is actually removed from the file system

Making Directories:
Directories are made using the mkdir() system call, which takes in two parameters, with the first being the name of the directory, and the second the mode of the directory. 

Reading directories:
Directories are read using the ls UNIX command, which prints to the screen the files (or other directories) within the directory, and has several options that can print out other
information about the directory entries. 

- Hard link vs. Symbolic links:

Hard link:
There is a system call known as link() which essentially creates another way to refer to the same file. The command-line program ln is used to create these "hard links". Therefore,
if you have a file called file1, and you link another file to it called file2, then when accessing or referring to either file name, you are referring to the same file. By adding
a hard link to a file, you increase its link count. Therefore, you have to filenames (i.e. user names referring to the file) that refer to the same inode number. 

So, to sum it up, a hard link to a file creates another username that refers to a given inode number. When adding hard links to a file, the link count is increased, and only upon
having a link count of 0 will the file be removed from the file system. When you delete a username filename (i.e. rm file2), it reduces the reference count by 1, and unlinks the
filename that was just deleted from the inode number. Note that hard links can only exist within a single system, while symbolic links can exist across systems. This is because
the inode number is system-specific, and is a way to refer to files that are on the system. Other systems will most likely use the same inode numbers, but they are referring to
their own files. Thus, as the hard link is a link to an inode number, it can only exist on a single system.

Symbolic Link:
Symbolic links are created using the ln command, but with the -s option. They are also referred to as "soft links", and involve the linking of a filename to another file on the
surface. However, soft links are very different than hard links, and in fact are a type of file themselves. As symbolic links are a special type of file, they don't actually 
contain the data of a file (like a hard link), but instead they just contain the pathname of the current location of the symlink to the file it is linked to. Therefore, the size
of the symbolic link file is dependent on the length of the path to the linked file. A longer pathname would result in a larger symbolic link file. As soft links just contain
the pathname to another file, they have the possibility of generating "dangling references." This is the process of deleting the file that the soft link points to, and in doing
this deleting the pathname of the link. Also, just renaming the linked file, or moving it to another location affects the soft link. By doing any one of these options, the soft
link now points to a path that doesn't exist, and thus is not usable anymore. 

- The mount operation (making and mounting a file system):

Making a file system:
A file system is created using the mkfs command, which takes as a parameter a device (which could be some disk partition), a file system type (e.g. ext3), and it writes an
empty file, starting with the root directory, onto the disk partition. With this example, the mkfs command would have created an empty ext3 file system on the given disk partition.

Mounting the file system:
Now that the file system is created, it needs to be made accessible within the file-system tree. This is achieved using the mount command, which takes an existing directory as a 
target "mount point" and pastes the new file system onto the directory tree at that point. For example, assume that we have an unmounted ext3 file system on the /dev/sda1 device
partition, which contains a root directory and two sub-directories a and b. Say we wish to mount it at /home/users within the file system. Thus, running the following command:
$ mount -t ext3 /dev/sda1 /home/users => the command will mount the new file system at the given location. By running the mount command with no parameters, it will show you what
is mounted to the system. 

- File types and attributes:
Many OSes try to represent all data sources as files. Files are simply an array of bytes that can be read/written (in most cases). However these files can actually be a wide
variety of things from actual files, to directories, to load modules, to videos, etc. While these files all consist of an array of bytes, they aren't all meant to be processed
as byte streams, but require the use of different APIs. 

Ordinary Files:
Text file => byte stream that when processed breaks it up into lines (via the \n or \r\n characters)
Archive (i.e. zip or tar) => single file containing many other files. It is an alternating sequence of headers and data blobs
Load module => similar to an archive in that it has alternating sections, but consits of code, symbol table, text sections
MPEG stream => sequence of audio, video, frames that require a unique processing of its bytes

Data Types and Associated Applications:
As files are only meaningful if they have the right program interpreting them, it is important to find the right program for them. This can be achieved by the user specifying
the command to process the data (i.e. gcc blah.c) or (emacs hello.txt). Files can be "classed" by specifying some extension (i.e. .c, .png, .txt). Another approach is to have
each file begin with a "magic number" which specifies the type of file. 

File structure and operations:
All files have a structure, and there are certain operations to perform on a specific file. For example, can't perform the read() or write() system calls on a video stream. 
Depending on the different type of file, there are specific commands to access the file type. 

Directories: 
These are files that don't contain blobs of client data, but instead are namespaces => the association of names with blobs of data. The namespace includes files that are owned
by numerous users. 

IPC Ports: 
Pipse are channels through which data is passed; however, the write() and read() system calls can be formed on them (via file descriptors)

I/O Devices:
I/O devices in UNIX are referenced using the same system calls as other files (to make the interface consistent) and use read() and write(). There are other I/O devices (e.g. 3D
rendering engine) that will use different operations and commands to access. 

File attibutes:
In addition to files containing data, they also have metadata => which is the data that describes the files. In UNIX, files all have a standard set of attributes that are stored
in a "stat" structure and can be viewed via the stat() command. These attributes consist of device owning it, inode number, file type, ownership, permissions, mode, access time,
modification time, creation time, size, block count, etc. Files can also have "extended attributes" such as:
	     - The algorithm used to encrypt the file
	     - The certificate of the key used to encrypt the file
	     - The check-sum of the file (which is used in error-handling)
	     - Access control lists
To represent the extended attributes of a file, a common approach is to use "shadow files", which are paired with the actual file, and they contain the additional attributes

- Consistency model (i.e. read-after-write, read-after-close, open-after-close):
When do new readers see results of a write? I put data in, I can get data back out again. But this is imprecise. When multiple values are written, when do I see each value. 

Read-after-write:
I write to a file, and after having written, I do a read, I will see the data I wrote. Even if not the same process, the other process doing the read will see the data another
process wrote. This is called "POSIX consistency", and is what you want in a database. We have lots of processes running at the same time, but we always want to ensure that when
a given process makes some change to a file, that after completing the write, another process can see the changes

Read-after-close:
I do a bunch of writes, and eventually I do a close. The close implies that I am through doing writes. Instead of doing a bunch of small writes, buffer up the writes, and then
do one large write to the disk. Once I do the close, you have to do the write, so the close => becomes the commit operation. If a process does a read after the file is closed, then
it should see the data just written to it.

Difference between read-after-write and read-after-close:
Read-after-write consistency is easy to be achieved even if the stuff hasn't been written to disk yet. This is because, the cache stores the data that has been written even if it
hasn't been written to the disk yet. Therefore, a bunch of processes can see the writes inside the cache. Read-after-close is a little different because if writes have been 
buffered in cache and not yet written to disk, and then failure occurs, and the cache is lost, the processes may get old information. 

In summary, read-after-write is the consistency model that states that if a process does a write to a file, another process upon doing a read can see the changes made. 
Read-after-close is the consistency model that states that if a process does writes to a file, and then closes it, other processes will see the changes after the close. 
Read-after-write consistency goes hand-in-hand with write-back caching in the sense that all of the processes can see the changes being written to the cache. They all have access
to the cache, and even if the data isn't written to disk, the cache contains the latest version of the writes. Therefore, if any of the processes were to do a read, they would see
the latest changes made (i.e. read-after-write). Now, using the write-back caching approach, all writes aren't actually written to disk until the writes are flushed. Thus, if a
crash occurred, the write data would be lost, and thus upon reboot, you might get the old data. Therefore, you will have read-after-write consistency, but after a crash, you may 
only get read-after-close consistency (i.e. upon reading you will only get the write information that was last written to storage). Because we are trying to utilize less writes
to disk (i.e. write-back), we may be talking about read-after-close consistency.

Open-after-close:
Let's say that two processes open the same file. And I am reading records, and you are writing records. And thus, we are violating all-or-nothing atomicity associated with a file.
The idea of open-after-close is that upon opening a file, I get a consistent snapshot of the file at the time of opening it. Thus, if I am reading the file, and another process
is making changes to the file, while I am still reading it, it doesn't affect anything that I will read. The reads I am doing are of a consistent snapshot when I opened the file.

Difference between read-after-close and open-after-close:
Open-after-close states that if I opened before you do the write, that I won't see the write. 

Explicitly versioned files:
Let's say that I run the read on the file at time a and then at time b, and it produces two different outputs. Now, this might be upsetting to a process as it is getting different
information. Therefore, some processes may want to open a specific version of a process that will result in a consistent snapshot of output. 

- Database semantics:
Extending ourselves beyond the byte streams (a file is just a bunch of bytes), there are other types of files. For read-after-write consistency, we were talking about POSIX
consistency, but for databases, we want ACID semantics (i.e. we want every op to be atomic, we want every user to see consistency at all times, we want every op to be isolated, and
we want every result to be durable). A database is not a byte stream, but is a bunch of records that has complex indexes, and multi-object ops that are very different from the 
ordinary file byte streams. 

- Object semantics:
They are simplified file systems, cloud storage, which states that objects are immutable (i.e. you can't change a file). You can't change a byte in a file, but you can replace it
with another file. Therefore, there are different, well-defined versions of objects (either version 1 or version 2). Amazon brought back the use of objects with their s3. The 
semantics of objects is that it is really cheap, you don't have to manage the storage, where the objects are large, immutable, single-objects in a flat namespace. It is a totally
different than POSIX model with different objects and different commands

- Key-value stores semantics:
At the same time Amazon was developing S3, they decided that they wanted the world's largest database, but didn't want to pay Oracle for this database. So they created DynamoDB, 
which is a distributed Key-Value store, which didn't need POSIX or ACID consistency. With ACID database, you can always get the consistency, atomic updates. In key-value store, 
if there are 4 different records, the inconsistencies are exposed to the user when reading at a given time, but it makes use of "eventual consistency", which states that eventually
at some given time in the future, everyone who will read a given key-value store will see the same changes that were made. Key-value store states that even if millions of users
are making changes to the same record, at some point, the values will converge into a single record.

Can provide an infinitely extensible (i.e. trillions of records) that are incredibly fast (designed for frequent small transfers) and have an eventual consistency model. 

- File Names and Namespace:
The file system knows files by their inode numbers. If you go inside of any file system, there will be some 64-bit number that is the true identification of a file (i.e. inode). 
These are the real names that files have, but they aren't the human-readable file names (given by the user). Therefore, the file system needs to implement some "namespace" that
allows for the association of a file with a filename (or multiple filenames). 

There are many ways to structure the namespace:
Can make use of separators (i.e. slashes), the basename (the user filename given), and finally some suffix extension (i.e. .c, .txt, etc.). Suffixes allow for specific file systems
to specify the given file type.

Flat namespace:
Before people had directories, there was the notion of "a" namespace. All file names must be unique within that context (that namespace). We both can have a file called "foo" as
long as we are in our own namespaces. So for flat namespaces, the files all have exactly one true name. The alternative to flat namespaces is hierarchical namespaces.

Hierarchical namespace: 
We always have a local naming context that is well defined. Within a given directory, you can only have one filename. However, you can have files with the same name in the same
namespace if they are in different directories. For example, I can have a directory with two directories nested within it that are all named "foo". 

- Goals of namespace representation:
As discussed above, the namespace is a way of naming files in a file system. There are two types of namespaces: flat namespace and hierarchical namespace. In flat namespace, there
can be only one name for a given file in the entire file system. In hierarchical namespace, there can be multiple files with the same name as long as they are in different
directories. Thus, the goals of namespaces are to have files that are easily recognizable to the user of the file system, allows them to come up with a name that they want, ...

- Goals of file systems:
File systems have the main goal of => if you put data in, you can get it back out without getting corrupted (integrity) and privacy (only the user of the file system can see it). 
In addition, file systems want to be able to find files associated with their names, and we want to able to read/data from any offset in any file

=======
File System Implementation:

- Very Simple File System (vsfs) => which is a simplified version of the UNIX file system:

When thinking about a file system, there are two main aspects to understand: the data structures to implement the file system, and the access methods to act on the file system.
The data structures refer to the on-disk structures that the file system uses to organize its data and metadata (i.e. arrays of blocks, bitmaps, indirect pointer blocks, etc.). The
access methods refer to the calls that can be made to read/write files, or open/close files such as open(), read(), write(), close(), etc. 

Overall organization:
The disk of vsfs is divided up into blocks => which are just certain sized chunks that store some form of data (assume 4 KB block size). Thus this file system can be viewed as
simply a series of blocks all of the same size. The type of data that is stored in these blocks (well the majority of the blocks) is user data. Thus, for this file system, and
for any file system, there needs to be a location on disk that stores user data => which is known as the "data region." Moreover, the file system needs to keep track of the 
metadata of all the files, which is stored in a structure known as the "inode." All of the inodes are stored in the "inode table", which is simply an array of inodes. 

So far, the file system is simply an array of 4 KB blocks that contain data. A large portion of the blocks is the data region, where each block stores some user data, and a much
smaller portion of the file system is the inode table, which is just an array of inodes (contained in blocks) that holds the metadata of the files. 

Inodes are typically 256 bytes in size, and the inode table occupies a relatively small region of the file system. Next, there needs to be a way to keep track of which blocks
are either free or allocated, and the structures to keep track of this information are known as "allocation structures." The method for allocation-tracking used in the vsfs, as 
well as the UNIX file system, is through the use of a "bitmap". There are two bitmaps: one for the data region ("data bitmap") and one for the inode table ("inode bitmap"). The
data bitmap for this file system occupies a single block, and each bit within the block refers to a datablock (hence the term bitmap). For example, lets say the first byte of the
data bitmap are 1's. This means that the first 8 data blocks of the data region are allocated (note: a 1 means allocated, 0 means free). The inode bitmap serves the same purpose. 
Lastly, at the beginning of the file system (and if there are many groups of the file system, perhaps at the beginning of each group) is the "superblock." The superblock contains
information about the particular file system => number of blocks, number of inodes, block size, inode size, number of groups, location of the inode table, and a "magic number" to
identify the file system type (in this case, vsfs). Therefore, when mounting a file system, the OS will read the superblock first, to initialize the various parameters, and then
attach the volume to the FS-tree. 

To summarize, the Very Simple File System (vsfs) is simply a disk that consists of an array of 4 KB blocks. These blocks contain various information => user data, metadata, 
allocation-lists, file system information. The first block of the file system is the "superblock" which contains => number of blocks, number of inodes, block size, inode size,
location of the inode table, type of file system, etc. The file system also contains the inode table, which is an array of blocks that contain the inodes. The inodes contain the
metadata of the files. The file system consists of a data region, which contains the user data (i.e. the data stored in the files). Lastly, the file system consists of two bitmaps:
the free inode bitmap and the free block bitmap. Each contains a bunch of bits, in which each bit refers to a data or inode block that is either (1) allocated or (0) free. 

- The Index Node (INODE):
The inode is a stucture that holds the metadata for a given file (i.e. id, inode number, mode, permissions, times). Each inode is referred to by its "inumber" or inode number. 
Given an i-number, you should be able to directly calculate where on the disk the corresponding inode is located. Remember that disks are not byte addressable, but are actually
addressed by their sectors (in their tracks). Thus to find a given block, the file system issues reads to sectors to find desired blocks. Each inode contains basically everything
you need to know about the file (i.e. type, size, number of blocks, direct/indirect block locations, link count, ids, etc.). 

The inode refers to where the corresponding data blocks are via the direct/indirect blocks. The direct block consists of 12 "direct pointers" (disk addresses), which refers to the
first 12 data blocks of the given file. In addition, there are three other blocks: singly indirect block, doubly indirect block, and triply indirect block (perhaps more). The
singly indirect block, contains "singly indirect pointers", which points to a certain number containing direct pointers, which then point to more data blocks. The doubly indirect
block contains "doubly indirect pointers", in which each points to a certain number of "singly indirect pointers", which then point to "direct pointers", which point to data
blocks. The triply indirect block does the same thing bruh. The use of blocks containing pointers to data blocks is known as a "multi-level index" approach. Many file systems,
including Linux ext2 and ext3, and the original UNIX file system use the multi-level index approach. However, other file systems such as Linux ext4, use the "extents" approach.
The use of the multi-level index leads to an imbalanced tree of a file system. This imbalanced tree is okay, as most file systems consist of most files being small. 

Extents:
Instead of using simple pointers, extents are disk pointers plus a length (in blocks). Thus, instead of requiring pointers for each block of the file, all one needs is a pointer
and a length to sepcify the location within the file. Extent-based file systems use multiple extents. 

Pointers vs extents:
Pointer-based => more flexible, but use a large amount of metadata per file (i.e. the extra indirect blocks). They are flexible because these file systems allow for the data 
blocks of the files to be all over the place in the file system, and not all in a contiguous location.
Extent-based => less flexible, but more compact => they work well when there is enough free space on disk and files can be laid out contiguously. If the data blocks for a file
can be laid out all together, then the extent based approach is more compact (and uses less metadata) as all you need is a single pointer. 

Linked-list approach:
Another approach to designing inodes is using a linked list, in which each inode consists of a single pointer that points to the first block of the file. It is different from
the extent approach, as the data blocks need not be contiguous, but rather can be scattered like in the multi-level index approach. Thus, each data block has a next pointer to
the next block in the file. For larger files, the inode might contain a pointer to the last data block of the file. However, the linked list approach performs poorly when reading
the last data block of a file, or doing random access. Thus, an in-memory table of link information can be used, instead of having each data block have a next pointer. Therefore,
when doing random access, one can first scan the table for the address of the desired data block, and then jump there directly. 

The linked-list data block approach is what the File Allocation Table (FAT) file system uses. 

Directory Organization:
In vsfs, and in the UNIX file systems, directories are simply just a list of (entry name, inode number) pairs. Therefore, a directory simply is a large list of directory entries,
each of which contains the inode number, the record length, the filename string length, and the filename string. The record length consists of the bytes for the filename string,
plus leftover bytes (which are the number of bytes needed to properly align the directory entry with the alignment requirements, perhaps 4 bytes). Note, that each directory has
two extra (invisible entries) => the '.' directory (i.e. the current directory), and the '..' directory (i.e. the parent directory). Moreover, when a directory entry (file) in a
directory is deleted, set the inode number of the directory entry to 0 (indicating the entry isn't in use), and then this entry can be filled in by a new file perhaps. 

Free space management:
All file systems must keep track of which inodes and data blocks are free, and which are allocated. Thus, "free space management" is important for all file systems. In vsfs,
two simple bitmaps are used. However, there are many ways to manage the free space instead of bitmaps => free lists (where a single pointer in the superblock pointed to the first
free block, and inside that block the next free pointer was kept, and so on forming a free block list), and also B-trees (to represent which chunks are free). 

When creating a file in the bitmap approach, the file system must allocate an inode for the file. The file system thus searches the inode bitmap for a free inode, allocates it to
the file, and marks the inode in the bitmap as used (i.e. 1), and then finds free data blocks, gives them to the file, and then updates the free datablock bitmap. With Linux file
systems ext2 and ext3, upon creating a new file, they search for a sequence of blocks (e.g. 8) that are free and contiguous, and thus guarantees that a portion of the file is
contiguous on the disk. This improves performance as when reading from a file (if the sequence of free blocks was available), a sequential read/write could be achieved. This is a
"pre-allocation" policy that is commonly used by file systems to boost performance. 

Access Paths: Reading and Writing:

Reading a file from disk (i.e. open file, read it, then close it of /foo/bar):
First, you issue the open() system call, at which time, the file system uses the full pathname that the user provided, to "traverse" the pathname to find the desired inode, using
the inode, the file system looks up basic information about the file (i.e. where its data blocks are, access rights, permissions, etc.). Note that all traversals must begin at
the root directory of the file system (which has a well-known inode number, usually i-number: 2). Using the inode number of the root directory, the file system searches the 
directory entries of the root directory until it finds the inode number of foo. Using this inode number, the file system jumps to the foo inode, and reads the directory entries
of foo, until it finds the inode number of bar. The file system then reads in bar's inode into memory, which then searches the inode (for the permissions) and then allocates a 
file descriptor tor the process, which stores it in it's per-process open-file table, and returns control to the user. The program then issues the read, in which the file system
performs the read, returns the required data, and then the process calls the close() system call. 

Writing to disk:
First the file is opened (in the same process), and then the process issues the write() system call. However, unlike with reading, writing to a file may result in blocks being
allocated. When writing to a new file, the data needs to not only be written to disk, but the data blocks need to be allocated, and the data block bitmap, and inode bitmap, and 
inode need to be updated as well. Thus, for a single write, there are many I/O operations that need to be done, in addition to the directory traversal to open the file. Therefore,
there needs to be some performance improvements to the file operations, which comes in the form of caching and buffering.

Caching and Buffering:
Reads/writes to disk can be very expensive (slow as shit). Thus, most file systems use caching (like everything else) to speed up performance. Without caching, opening a file
results in a lot of reads (one to the root directory's inode and then the directory entries, and more). Early file systems used "fixed-size caches" to hold popular blocks, and then
specific cache algorithms to decide block eviction (i.e. LRU). This form of "static partitioning" of memory can be very wasteful. Thus, modern systems use "dynamic partitioning",
in which they use a "unified page cache" => which virtual memory pages and file system pages are stored in the same cache. Thus, depending on which needs more memory at a given
time, page stealing, or rather page eviction can be used. Thus, with caching, the first open would generate a lot of I/O traffic, but then subsequent opens of the same file would
result in more cache hits. With writes, as opposed to reads, the process is different, as writes need to go to disk to be persistent. As seen, writes to disk are very slow, and 
thus (as discussed before) write buffering is used to delay writes, and then do a large number of writes at the same time (i.e. "write aggregation" or "batch writing"). By doing
this write-back caching method, performance is boosted in the normal sense, but also some writes can be avoided in general if a file were to make multiple writes to the same data,
we can just take the last write. However, as noted before, write-back leads to issues if crashes occur, as the writes were never committed to disk yet. With databases, they don't
care about this performance boost, and want assurance the write actually occurred, which is known as "write-through" caching or "direct I/O" and using the fsync() system call. 

Durability/Performance Tradeoff:
All storage systems have this tradeoff of having high performance or having high durability/consistency/reliability. If using write-back, you can have higher performance at the
cost of lower reliability. If using write-through, the performance is much slower, but it is more guaranteed that the data was actually committed to disk. 

- BSD (Berkeley Fast File System) or FFS Organization:
- BSD bit-map free lists:
- BSD directory entries:
First of all, Ken Thompson created the UNIX file system, when he created the UNIX OS. The data structures simply consisted of the super block (which contained the size of the
file system, number of inodes, pointer to the head of the free list of blocks (i.e. using linked-list free list), etc), the inode region, and the data block region. His file
system was incredibly simple, and supported the basic abstractions (i.e. file and directory hierarchy). However, the first UNIX file system was incredibly poor in performance. The
file system got to a point where it was only delivering 2% of disk bandwidth. The main issue was that the file system treated the disk like it was random-access memory => data
was spread all over the place without keeping in mind (that unlike memory of a computer system), the disk is a disk (which contains the mechanical parts that incur seek and 
rotational delays). The data blocks of a file were often far away from the inode, which resulted in large seeks upon first reading the inode, and then data blocks. In addition,
the file system would get "fragmented" very quickly as the free space was not carefully managed. Thus, the data blocks of a file were also spread out across the memory, and reading
or writing would result in seek times and positional delays as the next block to be read was most often on another section, track, or even worse another surface. Lastly, the 
original block size was too small (512 bytes), and thus transferring data from disk was very inefficient. Smaller blocks are good for minimizing "internal fragmentation", but are
bad for transferring as each block may require positioning overhead to reach it.

In summary, the original UNIX file system was good in the sense that it was simple and made use of the directory hierarchy. However, it was very inefficient and poorly performed as
the disk was treated as random-access memory (inodes were far away from data blocks), which led to lots of overehead (due to seeks and positional delays), the free space was very
poorly managed (i.e. external fragmentation), and the block size was too small (which led to more positioning and more transfers). 

Fast File System (FFS):
A team at Berkeley developed the FFS, which was designed its data structures and allocation policies to be "disk aware" and thus boost performance. They chose to keep the
same interface to the file system (i.e. open(), read(), write(), close(), ... system calls), which thus made it so that people of the old file system would be willing to adopt
their new file system as they didn't have to change anything to use it (as the interface was still the same). 

Cylinder Group:
First, FFS divides the disk into a number of "cylinder groups", in which a "cylinder" => is a set of tracks that are on different surfaces but are all the same distance from the
center of the drive. Thus, the entire disk can be viewed as a collection of cylinder groups. Note that modern disk drives don't tell the clients information about their disk
geometry, and thus Linux ext2 and ext3 for example, don't use cylinder groups, but rather "block groups" => which are consecutive portions of the disk's address space. The block
groups act in the same way as the cylinder groups, and thus boost performance significantly as the groups result in having low seek times when accessing files within the same
group. Each group consists of the superblock, the free inode and data block bitmaps, the inode table, and the data region. 

To summarize, FFS (and then later ext2 and ext3) made use of cylinder (or block groups), which consist of data blocks that would result in the lowest amount of seek time when
accessing files within the same group, as they are all in similar locations on the disk surface. By doing this, when users access files within a given directory for example, the
files are all in the same group, which is on the same cylinder on the disk, and reduces the need for seeks and positional delays. This opposes the original UNIX file system, which
could have data blocks of the same file, or files of the same directory scattered all over the disk. 

FFS Policies: How to allocate files and directories:
The placement of directories has a specific policy => find a cylinder group with a low number of allocated directories and a high number of ree inodes, and place the directory data
and inode in that group. This was used to balance the directories across groups, and because the file is a directory, it is likely that it will add files to it (hence the large
number of free inodes). 
The placement of files policy was two-fold => 1. when allocating data blocks for a file, put them in the same group as the inode (to reduce seek times), and 2. place all files that
are in the same directory in the same cylinder group of the directory. 

These placement policies were largely successful as usually, a user will access files mainly within two levels of the current directory. Thus, placing them all together in the
same group would reduce the seek time. 

Large-file Exception:
There is one important exception to the file placement policy => large files. Large files would entirely fill the block group it is first placed within (and perhaps other groups as
well). Filling a block group, thus would prevent other "related" files (of the same directory perhaps), from being in the same cylinder group (which would lead to increased seek
times, and hurt performance). Thus, FFS generated a large-file policy => for a large file, allocate a certain number of blocks (e.g. 12) for the large file in the block group
of its directory, and then place the next "large" chunk of the file (e.g. blocks pointed to by the singly indirect block) in another block group, and so on. This dividing up
of the large file over several groups would obviously hurt performance when reading/writing this file. However, this problem can be solved (or rather amortized) by choosing a 
good chunk size. If the chunk size is large enough, then the file system would spend more time transferring the data from disk (which is what is desired), and less time seeking
between the chunks of the block. 

Other FFS Innovations:
Designers of FFS were worried about accommodating small files (i.e. 2KB in size) when the blocks were 4 KB in size. While the block size was good for transferring data, it would
be bad for space efficiency, and lead to "internal fragmentation", and if every file were small, then roughly half the disk would be wasted. To solve this proble, FFS designers
created "sub-blocks" which were 512-byte little blocks that would be used by small files. Thus, the entire 4KB block would not be wasted. As the file grew, then other sub-blocks
would be used by the file, until 4KB of data was used, at which time, the file system would copy the sub-blocks into the actual 4KB block, and free the sub-blocks. This process
would be inefficient => copying the sub-blocks to the actual block. However, FFS generally avoided this approach by modifying the libc library => the library would buffer writes
and then issue them in 4KB chunks, thus avoiding the use of the sub-blocks. 

In addition, FFS introduced the technique of "parametrization" => in which, to combat missing blocks if a file were placed on consecutive sectors, FFS used a different layout, in
which data blocks were not chronological, but were "skewed" (i.e. 0, 6, 1, 7, 2, 8, 3...). This can be inefficient and reduce peak bandwidth, but by using a "track buffer", the 
buffer can read in a full track upon doing a read to it, and then avoid going to disk for everything on that track for future requests.

In addition, FFS was one of the first file systems to allow for "long filenames" which gave more freedom to the user

In addition, FFS created the optino to rename a file

Lastly, FFS created the "symbolic link" => which allow the user create aliases to any other file/directory on the system (and not just the file system volume)

Last note: The boot block:
The boot block is at the start of the file system, which just starts up by loading the super block, etc. 

- Disk Operating Systems (DOS) File Allocation Table (FAT) file system organization:
DOS started off as writing Basic I/O Subsystem (BIOS) for the Basic interpreter. People were storing Kilobytes of data in their DOS filesystems. DOS was created for basic purposes.
They wanted it to be simple, and thus it is very different than the UNIX file system.

Volume Structure:
It begins with the boot block, and next is the BIOS parameter block (BPB) which is quite analogous to the super block. It specifies the cluster size, and the FAT length. The 
clusters are analagous to the data blocks of the UNIX file system. They begin immediately after the end of the FAT, and the root directory begins in the first data structure. The
file allocation table is very different than inodes. 

Clusters in a DOS FAT File:
We divide the file system into clusters (that can be 512 bytes or 8K bytes). If they are smaller, then less internal fragmentation, if larger, then more efficient I/O due to less
seeking and positional delays, and more time transferring data. A directory entry is unlike the UNIX directory entry (which associates a name with an inode number). In DOS, a 
directory entry is a file descriptor. The directory entry itself is the file descriptor. It has the name of the file, then length of the file, and the address of the first cluster.
This is all directory entries need in DOS because the data is stored as a linked list (as opposed to the multi-level indexing with indirect blocks like UNIX). Thus, to access the
data of a directory entry, all you need is the address of the first cluster, which will have the next pointers. Given the cluster number, and the bytes per cluster, you can find
where each cluster is. So for example, lets say a directory entry states that the file "myfile.txt" is 1500 bytes long, and the first cluster is at address 3. Each cluster size
is 512 bytes, and you know where the first cluster address is, so you offset to the start of cluster 3. You read in the first 512 bytes, and then you want the next cluster. So, you
consult the File Allocation Table, in which each FAT entry corresponds to a cluster, and contains the number of the next cluster. A -1 signfies an end of file, and a 0 indicates
a free cluster. In this example, the third entry of the FAT table holds address 4, so you go to cluster 4, and read the data from there. Now, you look in the FAT, at offset 4, and
see that it contains address 5 (i.e. cluster 5). So, you go to cluster 5, and read in those bytes, and you notice that it is the end of file. However, you can look in the FAT at
offset 5, and see it contains a -1 indicating end of file. 

Notice that DOS implements a linked list, but instead of each data block holding the next pointer, the next pointers are stored in the File Allocation Table. To make the process
of doing disk I/O more efficient, instead of having to jump to the FAT table each time (which involves seek delays), is to read the entire FAT into memory (which is very small), 
thus saving the time to do extra disk searching. 

When talking about FAT-12, or FAT-16, FAT-32, ... => we are talking about different FAT sizes (i.e. the number of clusters they store). FAT-12 can have 4K clusters, FAT-16 can have
32K clusters, ..., a FAT-32 can have 4B clusters

FAT Table:
Note that the FAT table serves as both a free list (i.e. if 0 entry, then a cluster is free), and keeps tracks of which blocks have been allocated to which file, and the addresses
of currently allocated clusters. A file's directory entry contains a pointer to the first cluster of that file. Next, using the first cluster number as an offset into the FAT,
you can get the next cluster in the file. For example, if the directory entry contains address 3 (which is the first cluster of the file). Going to the 3rd offset of the FAT, the
entry is the next cluster. 

FAT Performance/Capabilities:
- Finding a particular block:
  - If doing sequential I/O => it is quite efficient, as you can get the first cluster from the directory entry, and then follow the chain of pointers through FAT
  - If doing random I/O => you have to follow that many pointers to chain through the read/write. For example to know the cluster of the third block of a file, you need to know
the cluster of the second block (as these are used as offsets into the FAT). Therefore, for random I/O, you would have to trek along the FAT until the specific cluster was found.

- Entire FAT is kept in memory:
  - No disk i/o required to find a cluster
  - For very large files, the search can still be long, as you have to follow that many pointers
- Not meant for "sparse" files
- Width of FAT determines max file system size (as the FAT determines the number of clusters present)

FAT Garbage Collection:
Older versions of FAT file systems dind't bother to free blocks when a file was deleted. Instead they crossed out the first byte of the file name in the directoy entry (via the 
reserved 0xE5 value). This reduced the amount of I/O needed for file deletion; however, it resulted in regularly running out of space. Instead, they used garbage collection.

- Free Space Management Goals:
We want to be able to allocated/deallocate blocks as quickly as possible. We want to reduce the amount of I/O as much as possible. In addition, it would be nice to allocate
new space that is in the same cylinder as the file (less seek time), and also to free up space that could make a next file allocation contiguous for some of its data blocks. This
would enable sequential reads/writes to take place (less seek time). These ends can be achieved by smart choosing of blocks, and also using coalescing and de-fragmentation to
perhaps make some contiguous blocks to be next to each other.

Bit Map Free Lists:
If using fixed size blocks, then using bit maps for the free lists is the best way to go. Thus, this process makes coalescing very easy to achieve. So, if we want to free up
contiguous blocks all we have to do is consult the bitmap, if there are two blocks that are free next to each other, then there will be consecutive 0's (i.e free blocks). If we
want to find a free block, then we can just look in the bitmap. Bit maps are very small as there is just one bit for block, and they don't take up too much space in the file 
system. BSD (UNIX) utilizes the free bit map. Each cylinder group has its own bitmaps and their own inod etable and data regions. Thus, when doing I/O you can consult the free
bit map when determining where to create a file (if it doesn't already exist). This process begins by reading the root inode, then the data of the root, which will give the inode
of the first name in the path to search. Therefore, a write to a new file takes 3 writes (one to write to the inode bitmap, one to write to the data block just allocated, and then
one to write to the inode to update it). 

FAT Free space:
Instead of having the bitmap to store the free list, the FAT serves as the free list as well as holds the addresses of the next clusters. If the FAT has an address of 0, then
the cluster is free, if -1, then end of file. Thus FAT serves dual-purpose, so when allocating space, just run through FAT to find an entry of 0. If we want to make contiguous
clusters for a file in use, then start at the last cluster of the file, and then start there when searching the FAT, so that you can find a close entry near the current file. 

- Namespace representation:
Directories are a special type of a file if using a hierarchical namespace. They are special because only the OS is to allowed to update the directories as botching a filename
makes it so that you can't find a file. Therefore, writing to a directory is a privileged operation. Directories contain multiple directory entries which describe one file and its
name. User applications are allowed to read directories to get information about each file. 

DOS directories:
The inode and the directory entry are the same thing. The name is part of the inode. DOS directories are basically the same thing as UNIX directories, except you can't have 
multiple names for a given directory as there are no links. 

- Goals of UNIX file system mount:
The goal is to make many file systems appear to be one giant one. Users shouldn't have to be appear of the boundaries of the file systems. Using the hierarchical nature of the
file system, you mount filesystems to given locations within the file system. 

- File Access Layers of Abstraction:
A disk can be partitioned into having multiple logical disks that each can contain a different fire system. By partitioning a single disk, then multiple file systems can be put on
it. It is risky to have multiple file systems spanning multiple disks (because if a disk fails, then you could lose parts of many files). It is better to lose all of some files
and none of the others. The abstractions are necessary to provide the interface to the user that everything works the same (despite different file systems having different device
drivers perhaps, and all having different implementations). You access the files all using the same system calls and file operations, and there exists a "virtual file system
intergration layer" => which generalizes all file systems, which permits the OS to treat all file systems as the same. Thus, the implementation is hidden from the higher level
clients, while the integration layer implements them all using their own file system-specific lingo. 

- Dynamically Loadable File Systems:
By using the federation layer to generalize the file systems, you can make use of plug-ins to add on new file systems. Each file system is implemented by a plug-in module, and thus
a dynamic implementation of file systems can be used. 

- FUSE (User Mode File Systems):
FUSE is a software interface for UNIX-like OSes which allows for non-privileged users to create their own file systems without using kernel code. FUSE is useful for writing virtual
file systems, which don't actually store data themselves

================================================================================================================================================================================
Topic 14: File Systems Performance and Robustness:

- Crash detection and repair:
Unlike with most most data strucutres, file system data structures must persist (despite turning off power, power loss, system crashes etc.). A major challenge faced by a file
system is how to update persistent data structures despite the presence of a power loss or system crash. For example, what happens if, right in the middle of a write to disk, the 
power turns off? These are problems known as "crash-consistency problems." If in the middle of a write to disk perhaps, and you only performed half of the write, if the system
crashes, then upon reboot, the disk would be left in an "inconsistent state". We require that disk operations be atomic, thus if power goes out, the disk would be in an 
all-or-nothing state, where either all of the write succeeded, or it all failed. There are two approaches to this problem: 1. using a file system checker (fsck), which is outdated,
and 2. use of "journaling" (or "write-ahead logging") which adds some overhead, but is the quickest method to recover from crashes/power losses. 

Example:
Assume that there is a file that already exists, and we wish to add some data to it. This involves updating the data bitmap (i.e. allocating a free block for the write), updating
the inode (i.e. changing the size of the file, and the blocks for the given file), and the new data block (i.e. writing to the just-allocated data block). This is a single write,
but as seen there are multiple parts to it. Thus, if a power failure occurred during the operation, the disk would be left in an inconsistent state, which could be a bad thing.
If just the data block was written to, and the block bitmap and inode not yet updated, then this is fine; it appears as if the write never occurred. If just the block bitmap
were updated, then this results in a "space leak", as the file system says this block is allocated, when in fact it is empty. If just the inode was updated, then it would appear
as if there is some data at the given block, and thus "garbage" data would be read from the empty block, which is a problem. There could also be a combination of two events 
occurring and not the third, but they mostly result in situations that we don't want to be in anyway. 

Note that usually writes are buffered in the cache, so that we can exploit write-back procedures, write-aggregation, and write larger transfers to disk to improve performance.
Thus, a bunch of writes could potentially be lost due to a crash, and we need a secure and fast way to recover from this, and get the file system back up to date. Therefore, we
want these writes to disk to be "atomic", so that we can avoid the disk system being in an inconsistent state which could lead to "space leaks", "garbage values" in blocks, etc.
However, you can only write to a disk one operation at a time, so we need some other solution, as we can't just make the action atomic.

File System Checker (fsck):
Early file systems dealt with crash consistency problems using fsck. This approach can't fix the problem with garbage values; it can only make sure that the metadata of the file
system is consistent with the data region. Before mounting the file system each time, fsck is run, which checks to make sure that the on-disk file system data structures are in
a consistent state. This is what it does for each of the data structures:
- Superblock => checks to make sure the elements all match (i.e. the total number of blocks is greater than the number currently allocated)
- Free blocks => Scans through the bitmaps to build a picture of the current file system state and then checks it with the inodes (going with the inode if there is an 
inconsistency)
- Inode state => checks each inode for corruption (i.e. checks file types etc.). 
- Inode links => checks the link count, and makes sure that there are the right number of files linked to it
- Dumb checks like that

In summary, fsck basically runs through the whole file system checking for inconistencies, which is very slow (if the file system is large). And doesn't really do an appropriate
check of the file system condition. It is highly irrational to do such a check for most cases. For example, as in the example above if only a single write occurred (and was
interrupted), fsck scans through the entire file system for a single write operation. Waste of time, who ever created fsck is an idiot, and hence why people call it "f suck."

Journaling (or write-ahead logging):
The most popular idea for crash recovery, and is the recovery protocol used by ext3 and ext4 (among other popular file systems). The idea is that as updates are being made to disk,
before actually committing the changes to disk, you write down a little note (somewhere else in disk) describing the action that you are about to do. Hence, it is writing a log
ahead of time about what you are about to do, thus if the system crashes, you can check through the log, and make sure that the changes were made, or if they are half-done, then
you can complete the changes; putting the system in a consistent state. If a crash were to occur, while writing to the journal, then you can scrap the operation you were about to
do. As you are writing to disk each time before writing to the actual location on disk, this obviously incurs some form of overhead as now two writes are being done instead of 
just one. However, this overehead could be fine if you are willing to suffer a little slower performance for dealing with crashes properly. On ext3 for example, right after the
super block, there is another volume (or section) of the file system that is made for the journal (or write-ahead log). 

Data Journaling:
Each transaction (i.e. the log before committing the change) consits of three things: 1. the "transaction begin" which tells about the update, and contains a "transaction 
identifier (TID)", 2. the exact contents of the blocks that are abou to be written (known as "physical logging", or the system could use "logical logging", which doesn't actually
contain the block data, but a summary of what is in the blocks), and 3. the transaction end, which marks the end of the transaction and also contains the TID. Once the transaction
has been fully committed to the disk journal, then the actual commit can be written to disk, which is known as "checkpointing." 

However, with just these two parts: "journal write" and "checkpointing", there is still a problem. What if the system were to crash when writing to the journal? An initial 
approach was to write each part of the journal (i.e. the transaction begin, then data, then transaction end) one at a time. However, this is too slow. Another idea was to buffer
the whole write of the journal at once; however, due to scheduling, it could occur where the writes don't occur in order, and perhaps a power failure occurs, and the transaction
end is written, but not all of the data portion. Thus, it would look as if it were a valid transaction, when in fact some of the data contained garbage. To combat this solution,
the journal write was broken up into two pieces: 1. the transaction begin, and the data blocks, and 2. the transaction end. Thus, if the power failure occurred during the 
first part, then you can just scrap the transaction as it never completed. Now, the reason that this works is due to the file system guarantee that any 512-byte write will happen
atomically. Therefore, the transaction end should be made a 512-byte segment, so after writing the first part of the journal, the second part is guaranteed to succeed. 

Recovery:
Now when a crash occurs, we can service it. If the crash occurred during the journal write, then we can just skip this pending update. If the crash happened after the journal write
completed, but before or during the checkpoint phase, then the recovery of the file system can occur => which replays the transactions in order, and makes sure that they were
actually committed to disk. If they didn't complete, then they can be committed using the journal. This is known as "redo logging", and ensures that the disk data structures are
consistent. 

Batching Log Updates:
The basic journaling protocol incurs a lot of overhead, as we have to write each time to the journal before actually committing them to disk. Thus, we can make use of the cache
to buffer the journal updates, and then go to write them out at a certain time (using write-back caching) instead of doing a write each time it comes in. 

Making the log finite:
The log is a finite size, so what happens if the log fills up. Well if it fills up, then no longer commits to disk can be made, which is stupid. Thus, most journaling file systems
created a "circular log", which involves reusing the log entries. This is achieved by having an initial "superblock" entry within the log, which keeps track of which transactions
have been checkpointed and which haven't yet. Thus it would reduce the recover time, but also would enable the log to be reused. Thus at some point after performing some 
checkpoints, the superblock should be consulted to free up some log entries.

Thus, data journaling consists of four parts:
1. "Journal write" => Write the transaction begin, and the contents of the update to the log
2. "Journal commit" => Write the transaction end to the log, which verifies that the transaction has been completed to the journal
3. "Checkpoint" => Physically make the changes to the file system
4. "Free" => After a checkpoint has completed, mark the information in the superblock, and then at some specified time, check the superblock, and free transaction entries that
have already completed. 

This is a fine recovery protocl; however, it incurs a lot of extra overhead with writing twice to disk, despite crashes being a rare event. To solve, or better handle this 
journaling overhead, "metadata journaling" was created. 

- Metadata Journaling:
Although recovery is now fast and reliable, the normal operations of the file system have been greatly slowed down. For each write to disk, we have to do two writes, which leads
to larger overhead to due to seek and position delay time, as well as performing the I/O. A simpler form of journaling called "ordered journaling" (or just "metadata journaling")
involves writing the actual data to the file system, then doing a journal transaction, then checkpointing the metadata, and then doing some transaction frees. 

1. Data write => write data to the file system
2. Journal metadata write => write the transaction begin and metadata to log
3. Journal commit => write transaction end block
4. Checkpoint metadata => write the metadata information to the file system (i.e. bitmap and inode updates)
5. Free => Free transactions after they complete using the journal superblock

Another approach to crash consistency problems is to use "copy-on-write" which is what "log structured file systems use (LFS)", which will be discussed next

- Log-structured File Systems (LFS):
A team at Berkeley devised a new file system with the following motivations in mind:
  - System memories are growing => as memory gets bigger, more data can be cached in memory. And as more data is cached, disk traffic increasingly consists of writes, so file
    system performance is largely determined by write performance
  - Large gap between random I/O performance and sequential I/O performance => due to seeks and rotations, random I/O will obviously be slower than sequential
  - Existing file systems perform poorly on common workloads => for example, FFS needs to perform a lot of writes just to create a new file of size 1 block
  - File systems aren't RAID-aware => raid 4/5 have small-write problem, but existing file systems don't try to avoid this worst-case RAID writing behavior

Thus, they created LFS => in which writing to disk, first buffers all updates (including metadata) in an in-memory "segment", and when the segment is full, then all writes
to disk take place in a long, sequential transfer to an unused part of the disk. LFS never overwrites existing data, but "always" writes segments to free locations. As segments
are large, the disk is used efficiently, as large transfers are cheaper, and they make use of sequential I/O. 

Writing to disk sequentially:
For example, when writing a single data block to a file (in the data region), this write has to take place, but also the inode (metadata) has to be updated as well, to now
point to the given data, change some inode information, etc. Now, with LFS, instead of writing these things to different locations on disk (which comes with seek and positional
delays), the inode and data block are next to each other on the disk. Therefore, LFS has the ba sic idea of writing all updates (data blocks, inodes, etc) to the disk
sequentially.

Writing sequentially and effectively:
However, just writing sequentially is not enough to boost performance, because in between the first and second write, it is possible that the desired section to write has passed
under the disk head, and thus a full rotation must take place. To deal with this issue, LFS uses "write buffering", where before writing to disk, it keeps track of all updates
in memory, and when enoug updates are in memory, all the writes go to disk at once. The large chunk of updates LFS writes at one time is referred to as a "segment". As long as
the segment is large enough, writes will be sequential and efficient. 

How much to buffer?:
If the segment is too small, then we will end up doing more frequent, smaller writes to disk. Even though the writes will be sequential, this will still incur too high of an
overhead. Thus the segment size needs to be large enough to amortize the cost of the seek and positioning overheads. 

Problem with LFS: Finding Inodes:
In the typical UNIX file system, you can find inodes by using the inode table, and using the inode number to find (by offsetting into the correct location of the inode table). 
With FFS, the process is still simple, but now you add on the group number calculation as each group has an inode table. With LFS, the process is more tricky, as inodes are
scattered throughout the disk. In addition, no data blocks are ever over-written, and thus there can be multiple copies (older versions) of a given inode. 

Solution through indirection: the inode map:
To find inodes in LFS, a "level of indirection" was developed between the inode numbers and the inodes using an "inode map". The imap is a structure that takes an inode number
as input and produces the disk address of the most recent version of the inode. Any time an inode is written, the imap is updated with its new location. The imap needs to 
persistent and thus needs to be present on disk; however, if it is at a fixed location, then this would require two writes each time (which is the case we were trying to avoid by
creating a new file system). The solution is to keep parts of the imap right next to where we are writing the new information sequentially. 

Completing the solution: checkpoint region:
However, how do find the piece of the inode map now as it is also scattered throughout disk? There needs to be some fixed location on disk to begin a lookup, and this place is 
known as the "checkpoint region (CR)." The checkpoint region contains pointers to addresses of the latest pieces of the inode map, and thus the inode map pieces can be found
by reading the CR. Note that the checkpoint region is only updated periodically (every 30 seconds or so), so the two writes doesn't affect performance as it doesn't occur very
often. 

Therefore, write traffic is reduced by writing only large segments to disk, and multiple writes (i.e. updating inode and data region in different locations) is removed, as only
the CR needs to be updated at infrequent times. The writes occur sequentially, and writes never overwrite existing blocks, so writes are utilizing file system write bandwidth. 
Reads occur in the same way, but writes are now more efficient, despite being weird. Note that directories in the UNIX file system consist of name, inode number pairs. Therefore,
in a typical file system, upon updating an inode, then the directory entry would also be updated as it points to the inode information. However, with LFS, this problem is avoided
to solve the "recursive update problem" => which avoids updating the directory, and instead just updates the inode map with the new infomration. 

Garbage collection with LFS:
As LFS never overwrites any blocks in the file system, eventually the file system will fill up, if nothing is to be done. Thus, there needs to be some form of garbage collection
to free up blocks. With LFS never overwriting any blocks, this means that there will likely be multiple versions of data blocks, and inodes (i.e. the newest version, and a bunch
of old versions). LFS chooses to only keep the latest live version of a file, and thus in the background, LFS does period garbage collection of old stuff. The segment is a
mechanism used to make efficient large writes, but it is also integral in cleaning. LFS can't just go through and free single blocks as this would lead to "holes" which would
not allow for contiguous, sequential operations to take place. Thus, LFS cleans on a segment-by-segment basis. Which periodically reads in a number of old (partially-used) segments
and determines which blocks are live within the segments. It then writes out a new segment of only the live blocks to a new location, and frees the old block. However, there are
two problems with this garbage collection: 1. how does LFS know which blocks are "live" and 2. which segments should it pick to clean?

Determining block liveness:
To determine which blocks are live, LFS records some extra information in the segment in the head of the segment => the "segment summary block", which contains information such
as inode number, the file it belongs to, and an offset. Thus, to determine if a block is live, look in the segment summary to find the inode number and offset, and then look in
the imap to find where the inode actually exists, and then read the inode to see where the inode thinks the given data block is. If it points to the disk address that was recorded
in the segment summary, then the block is live, if it doesn't, then it can be concluded as being dead, and thus can be removed. 

Policy: which blocks to clean and when?:
To determine which blocks to clean, LFS tries to segregate hot and cold segments (i.e. content being frequently over-written vs. segments that may have a few dead blocks). Thus,
the cold segments should be cleaned sooner, and the hot segments cleaned later. 

Crash Recovery with LFS:
Achieves recovery via the checkpoint region and by writing to a log. The CR is updated atomically, and two CRs exist in the file system (at either end of the disk). Also, like
with the journaling file systems, the CR is updated in a similar fashion (i.e. with a timestamp, then the updated pointers, and then a new timestamp). If the timestamps don't match
then a crash occurred, and is handled with. 

To summarize, LFS introduced a new approach to updating disk that involves writing to unused portions of disk, and never overwriting files in place. It later uses garbage
collection to free up old versions of the disk file. This is called "copy-on-write" => in which LFS gathers all updates into an in-memory segment, and then writes them out
together and sequentially to make writes as efficient as possible. 

- Causes of file system damage:
- Detection and repair:
- Checksums
Modern disks will occassionally seem to be mostly working; however, they can't access one or more blocks. Thus, there are two types of single-block failures (as opposed to entire
disk failures): 1. "latent-sector errors (LSEs)" and "block corruption". 

Latent-sector errors (LSEs):
LSEs arise when a disk sector (or group of sectors) has been damaged in some way. For example, if a "head crash" occurs, which is the process in which the disk head accidentally
comes into the contact with the disk surface, it can damage the surface, and make bits unreadable. Or cosmic rays can flip bits, leading to incorrect contents. These in-disk
"error correcting codes ECC" exists to determine if on-disk bits are good, and in some cases, to fix them. 

Block corruption:
There are also caes where a disk block becomes "corrupt" in which it is undetectable by the disk. Or, the disk firmware may write a block to an incorrect location, which also
leads to block corruption. The disk ECC may indicate block contents are fine, when in fact there is some other problem => these are known as "silent faults" and give the disk
no indication that the problem occurred. 

Fail-partial disk failure model:
The new failure model consists of both silent and non-silent partial faults that lead to one or more problem blocks, and not the entire system failing. Disks may seem to be
working, but some blocks may be inaccessible (damaged due to LSEs), or may hold wrong content (due to corruption). Thus, when accessing the seemingly-working disk, it may return
an error (i.e. non-silent partial fault via ECC), or it may return incorrect data (i.e. silent-partial fault). 

Dealing with latent sector errors:
Latent sector errors are easy to handle, as they are easily detected. When the system tries to access the block, and the disk returns an error, then the system should use whichever
redundnacy policy it implemented to return the correct block to the system, but to alse recover the block perhaps if using a raid-4/5 system via parity. When a full disk failure
and an LSE occur in tandem, then there is no wayto recover from this. For example, lets say the RAID system is in the midst of reconstructing the damaged disk using the parity
group, but then an LSE occurs. Then there is no way to recover the disk fully as now there are two pieces missing, and the parity cannot be used to reconstruct the lost blocks. 
Some raid systems make use of multiple redundant parity disks to combat this issue.

Detecting corrupting: using a checksum:
Detecting silent failures due to corruption (i.e. silent failures) is a tougher problem. How can a disk tell if a block is bad? One option is the use of a "checksum", which is
simply the result of a function that takes a chunk of data as input and computes a function using that data, and produces a small (4 or 8 byte) summary of the contents of the data.
The goal of using a checksup is to enable detection of corrupted data by storing the checksum with the data, and then confirming upon later access the data's current checksum to
see if it matches the original, correct checksum. A checksum function has tradeoffs: strength and speed. A simple checksum is to XOR all the bits of the data block. Thus if we
are computing a checksup over a block of 16 bytes, and the checksum is 4 bytes, we will have 4 bytes of XOR. XOR is a reasonable checksum, but if two bits in the same position
were corrupted, tehen the checksum wouldn't be able to detect corruption. Other checksum is a "cyclic redundancy check (CRC)", which takes a data block, treats it as a large
binary number, and then divides a given value by it. The remainder of the division is the value of the CRC. Whichever checksum function is picked, it should avoid "collisions" in
which multiple data sets produce the same checksum. 

Thus we can use a checksum and store it with each data block, or perhaps at the start of a given sector, store all the checksums for the data blocks. Now, when reading any given
data block, the user should also read the checksum (i.e. the "stored checksum"), and then the client should compute the same checksum (i.e. the "computed checksum"). If they match,
then it can be inferred that no corruption occurred, if they don't then corruption most likely occurred. If the disk uses redundancy (i.e. a RAID file system), then just go get
the redundant copy and return it to the user, and then try to recover the lost block. 

- Other types of corruption: Misdirected writes and lost writes:

Misdirected writes:
This is the process by which data is written to disk correctly, but to the wrong location. This process can be detected by adding a "physical ID" to each checksum. Thus, if the
checksum now contains the disk and sector number, in addition to the checksum, the client can just verify these three things. Note that there is now a good deal of redundancy for
each data block. This is fine however, as REDUNDNACY IS THE KEY TO ERROR DETECTION AND RECOVERY. 

Lost writes:
This occurs when the device informs the upper layer the write completed, when in fact it never persisted to disk. This could occur due to write buffering, and then a crash occurs,
in which immediate reporting took place, and the upper layer was told the writes were successful, when they were just in the buffer. Thus, the file system may only be able to
guarantee "read-after-close" consistency, in which the only guarantee that upon reading data, the write that came before it persisted was upon the file being closed. 
Read-after-write is the process that a read occurred, after writing to a file (even if the process hasn't called close yet). This is possible due to the reading of data from
multiple processes of the updated data in the cache. If a crash occurs, then the cache data is lost, and then upon reading the same data, you will most likely get a much older
version. This crash thus can only guarantee read-after-close consistency. 

- Scrubbing:
When do checksums actually get checked? Some amount of checking occurs when data is accessed by applications, but most data is rarely accessed, and thus would remain unchecked. 
Unchecked data could be problematic for a reliable storage device. The solution is using "disk scrubbing" which periodically reads through every block of a system, and checks
whether the checksum is still valid. It is implemented as a part of a typical system schedule scan. 

Overheads of checksumming:
1. Space => checksums take up space on disk (especially if the extra data such as the block number and sector are present) => this is less of an issue
2. Time => computing checksums can lead to icnreased overhead especially if they are computed for every block accessed. In addition, more complex/more reliable checksums, would
take longer to compute. 

- Defragmentation:
Defragmentation is a technique used to eliminate "external fragmentation" and to create densely allocated resources. Garbage collection while not defragmentation is a forms
of cleaning up no longer used data, and is similar in its goals. Both are techniques for remedying space that has been rendered unusuable by unfortunate combinations of allocation
events. 

File I/O is more efficient if the operations are sequential/contiguous blocks. This is easy if there is free space that is well distributed in large chunks. With time, free space
becomes fragmented, but defragmentation or garbage collection can take place to free up space. Can speed up I/O by cleaning up free space. Can involve the use of garbage collection
or coalescing to recombine fragments to larger free pieces. Can use compaction to move fragmented existing data to a contiguous region somewhere else in memory. It should be fast
and efficient, and should not affect current execution.

- Block size and internal fragmentation:
Per operation overheads are high (i.e. DMA startup, head seek, positional delay, completion interrupt handling). Thus, larger transfer units are more efficient and amortize
the overheads. However, this requires fixed size allocation units. And larger fixed size chunks won't be used all of the time => which leads to "internal fragmentation". Thus, 
we need variable partition allocation. Large blocks are a performance win, but lead to internal fragmentation. However, variable partition is difficult as it also leads to external
fragmentation, which would lead to serious file system performance degradation. 

- Read caching:
Disk I/O takes a very long time. We can amortize the overheads by using deep queues (i.e. giving many requests to the device to keep it busy at all times), and make larger
transfers, and fewer transfers to improve efficiency. However, even larger transfers don't boost performance significantly. Instead, we need to reduece the number of disk I/O 
performed. This can be achieved by using an in-memory cache to cache popular blocks. By using "read-ahead", we cache more data than requested (i.e. want to read a sector, so
cache the whole track). In addition, all writes go through this cache, which ensures everything is up to date. 

- Read-ahead:
Requst blocks before they are requested and store them in the cache. Thus, we reduce the wait time of waiting for a request to complete, and reduce the amount of times needed to
go to disk. We can do read-ahead when the client requests data sequentially, or seems to be reading sequentially. The risks of using read-ahead are that it may waste disk access
time as it is reading blocks not requested, and that it may waste the buffer space of the cache with unneeded blocks. 

- Special purpose cache:
Caching blocks of regularly processed files makes a lot of sense. Caching indirect blocks, thus holds the addresses of a ton of blocks. Directory entries make up 1% of the entries
but can account for 99% of access. Thus, we may perhaps want to cache entire paths. 

- Out-smarting LRU:
It is hard to guess what a program will need. But we can make some logical assumptions => the program won't re-read load modules, DLLS, audo/video frames just played, files just
deleted, etc. Thus, we can drop these such files from the cache, and increase our chances of having an effective cache

================================================================================================================================================================================
Topic 15: Security, Protection, Authentication, Authorization:

- Challenges of securing the OS:
Why are OSes particularly important from a security perspective? => Because everything runs on top of an OS. If the software you are running on top of is insecure, what's above
it is also going to be insecure. In addition, our software relies on proper behavior of the underlying hardware, which the OS has full control over. The task of securing an OS
is not an easy one, as modern systems are very large and complex. The longer the code is, the more flaws and room for error exists, and thus for large programs (i.e. the OS), the
harder it is to make secure. Another challenge is that OSes are meant to support multiple processes simultaneously, while still segregating the processes from one another and not
allowing them to access privileged code and other operations (without OS supervision). Some processes are downloaded or running from the internet, which we would obviously not
trust as much as some other processes, and for this reason, not want to give full access to the system to this process. Other applications do their operations with the guarantee
that the OS will provide such and such security to the given operation. However, if the OS is unable to achieve the desired security, then the applications must do more work
to secure their systems, but as discussed earlier (if the OS isn't secure), and they are running on top of the OS, then this would just be wasted work. 

What is the OS protecting?:
At the high level: everything => the OS has complete control of all the hardware, and thus it can perform any operation one of its periperhals supports. Thus, processes are at
the mercy of the OS, and thus, the OS needs to be able to secure every part of itself, so that processes can run properly and securely. Security flaws in the OS can completely
compromise everything about the machine the system runs on. 

- Security Goals and Policies (Confidentiality, Integrity, Availability):
1. Confidentiality => If some piece of information is supposed to be hidden from others, then don't allow them to find it out 
   - Ex: Keeping your credit card nnumber confidential
2. Integrity => If some piece of information or component of a system is supposed to be in a particular state, then don't allow an adversary to change it. An important aspect
of integrity is authenticity => it is important to be sure that the information hasn't changed, but also that the information was created from a reliable source
   - Ex: You place an online order for pizza delivery of one pepperoni pizza, and you don't want a prankster to change the order to 1000 anchovy pizzas.
3. Availability => If some information or service is supposed to be available for your own or others' use, make sure an attacker can't prevent its use
   - Ex: If you are having a big sale, you don't want your competitiors to be able to block off the streets around your store

- Controlled sharing:
This the idea that builds atop the CIA principles for security and is the process of being able to share some secrets with some desired people, while keeping it private from 
others. For example, we may allow some people to change our enterprise's databaes, but not just anyone. Systems need to be made available to a particular set of preferred users,
and not others. 

- Non-repudiation:
This is the idea that when someone told us something, they can't later deny that they did so. Making something harder and more expensive to repudiate their actions, makes it easier
to make them more accountable 

- Principles of secure design:
1. Economoy of mechanism:
This is the idea that you should keep your system as small as possible. Simple systems have fewer bugs and are easier to understand
2. Fail safe defaults:
This is the idea that when thinking about a design or aspect of a computer system, default to security, and not insecurity. If policies can be set to determine the behavior of a
system, have the default for those policies be secure, and not less.
3. Complete mediation:
This means that you should check if an action to be performed meets the security policies every single time the action is taken.
4. Open design:
Assume that attackers know every detail of your design. If the system can achieve the security goals anyway, then your system is secure. 
5. Separation of Privilege:
Require separate parties or credentials to perform critical actions. 
	- Ex: Two-factor authentication: require password (what you know), and possession of hardware (what you have) 
6. Least privilege:
Give a user the minimum privileges required to perform the actions you wish to allow. The more privileges you give to a party, the greater the danger that they will abuse those
privileges, or that they will mess up, and an attacker may steal the privileges
7. Least common mechanism:
For different users or processes, use separate data structures or mechanisms to handle them
    - Ex: Each process gets its own page table in a virtual memory system
8. Acceptability:
If users won't use it, then the system is worthless. If the system is very hard to use (perhaps due to the increased security), then users will most likely not use it

- Access control mechanisms:
This is the process of the OS allowing certain processes to perform some operation or request some operation be performed for them by using some method to verify that the process
is allowed to do said operation. This is known as access control, and if the process has the right access or capability, then the OS will grant the process the action. For example,
if a process wants a system call to performed on its behalf, the OS can check the process identifier in the process control block to determine the identity of the process. If
the identified process is "authorized", then it can perform the requested action. If the process isn't authorized, then the OS can kill it perhaps. 

- Object, agent, principal:
Principal => security-meaningful entities that can request access to resources (e.g. human users, groups of users, complex software systems)
Agent => The process or active computing entity performing the request on behalf of the principal
Object => The request is for access to some particular resource known as the "object"
Credential => Any form of data created and managed by the OS that keeps track of such access decisions for future reference (e.g. process control block)

For example, some process (i.e. the principal) requests some system call say open() be performed on some file. The agent (i.e. the OS) sees the request from the process, and using
some access control methods, say looking at the PID of the process in the process control block to see if it has permission to do this operation verifies whether the process
can perform the operation. Say the process can do this system call, so the agent performs the open() for the process, and the process is given the file descriptor (i.e. the object)

The credentials of the principal are forms of identity that the user can use to gain access to a given object it desires. Without any credentials, why should a system authorize
the user to gain access to the private or protected object? Common types of identities are that the user is perhaps a human being, or in fact a group of users, or even a program.
Usually, once a proecss or individual has been authenticated, systems will amost always rely on that authentication decision for at least the lifetime of the process. Therefore,
granting authentication is a big deal, and mistakes need to be at a premium (or never happen at all). 

- How to authenticate users?:
Authorization => the process of verifying that you have access to a given object
Authentication => the process of verifying that you are who you say you are based on (what you know, what you have, or what you are)
When a person is accessing a terminal and is trying to log in, he/she needs to be authenticated. There are three ways to authenticate a human being:
1. Authentication based on what you know:
The most common type of this authentication is through the use of passwords. A password is a secret (i.e. what you know), and is a way to prove your identity to a computer system,
despite the fact that passwords can be cracked/hacked/stolen (which then someone is impersonating an identity to gain access). This form of authentication, the use of passwords as
something you know, is only strong assuming that the password is a secret. Assuming that the user is the only one to know the password, then the password is a secret. Note that
the system authenticating the user need not know what the password is (as this would be one too many people who knows the password). Instead, the computer system can store a "hash"
of the password, and not the password itself. This works because the system is just checking to make sure that the user knows the password, but the system doesn't actually care
what the password is. Thus, upon logging in, check the user password just entered with the hash stored on the system, and if they match, then the user is legit. By the nature of
hashes, they are "one-way", and you shouldn't be allowed to reverse the hashing algorithm (i.e. given the hash, produce the plain-text password). 

Storing the hash gives an added benefit to not having another person in on the password loop, and it comes in the form of if the hash is lost or stolen, the attacker still doesn't
know what the password is. Given the hash, you can't produce the password in normal methods (although the attacker could get lucky, or use some dictionary perhaps to crack the
hash). Thus, we want to ensure the attacker has no chance to find the password from the hash. The hashes that we are talking about here are "cryptographic hashes" => in which it is
infeasible to figure out the password using the hash, other than by passing guesses through the hashing algorithm. 

Password characteristics => we want the password to be long in length, and also be made up of a larger number of different characters. In doing this, it may be harder to remember,
but it makes it harder to guess (or brute-force or dictionary-attack) as there are more possible password option. Attackers (knowing that people need to remember their passwords)
know that random strings of characters usually won't be chosen for passwords, and that most people choose names or familiar words (as they are easier to remember). Thus, there
is an attack known as a "dictionary attack" => in which attackers have a specialized list of common words, anems, meaningful strings that people tend to use for passwords, and 
then using some automated system, make guesses at the password. If the system is smart, then there should be some lockout after a certain number of failed attempts, and the 
attacker shouldn't be able to make 15,000 guesses at an account's password without getting it right. 

Now, guessing seems infeasible using this form of the dictionary attack. But what if the attackr stole the password file (i.e. the hashes of the passwords)? Now, the hashes are
in fact cryptographic hashes, in which you shouldn't be able to reverse engineer the password from the hash. However, all cryptographic hashes for passwords used are from known
cryptographic hashing algorithms. Thus, the attacker now has his dictionary, and the cryptographic hash, and the hashing algorithm, so he is free to make as many guesses at the
password until he finds one matching the hash. To make this more efficient, the attacker will have a dictionary that is in hashed form, thus all he needs to do is check his hashes
with your password hash to see if they match. To combat this problem, computer systems introduced the "salt" => which is a large (32 or 6 bit) random number that is concatenated
to the password. It is also hashed, and stored with the password hash. By using the salt, the dictionary attack just increased in time by a great deal. Now, the attackers 
dictionary can't just include the password hashes, but must now form each password hash with every combination of a potential salt (32 or 64 bit number). If the salt is 32 bits
for example, then that's 2^32 different translations for each word in the dictionary. Therefore, the attacker (even if using automation) may be deterred from performing the attack
as it may take some ridiculous amount of time to perform. 

In summary, authentication by what you know (in the form of passwords) is a thing of the past (despite still being used widely today). They have been cracked, or hacked many times
in the past, and even with the hashes are not 100% secure from attacks. Thus, they at best can serve as part of a multi-factor authentication process (in which you provide the 
password, and some other form of authentication) to gain access to the certain object.

2. Authentication based on what you have:
This form of authentication is different because unlike going up to an ATM machine (in person), you are accessing some system that is not directly in front of you, but could be
across the world on some network server or system. Therefore, authenticating based on what you have takes on a different form. There are devices, or security tokens, called
"dongles" which plug into a USB port, and the system can verify that you have the proper device or not. In most cases, the system will make use of the person's capability to
transfer information from whatever we have that the system is trying to authenticate. For example, there are various smart token displays that can ask the user to "enter in the
information that you see on the screen into the keyboard." The OS doesn't get direct proof that the user has the device, but if someone with access to the device could know
the information he should type in, then that is good enough. These kinds of smart token displays need to be changed frequently, so that the information doesn't become static, and
a bot can just enter in the static verification each time. 

An obvious weakness of authentication based on what you have is, what if you don't have it? Well, in this case, you can't gain access to the system, but also, what if someone has
your device, then they can get it (given there is no password on the phone perhaps). However, if using two-factor authentication, then the attacker can still not access the system.

3. Authentication based on what you are:
Human beings are unique creatures with different physical characteristics, and thus "biometric authentication" is a form of authenticating, which can come in the form of a retinal
scan, fingerprint scan, facial scan, etc. This approach is very difficult to achieve, as there needs to be an authentication mechanism. However, it is very challenging, as there
could be cases where people could have similar enough features that the system might authenticate them. Or perhaps, the actual owner looks different or is wearing some form of
clothing blocking the face or something, then the computer might not even authenticate the actual user. For example, lets say the system does facial scans, and I created the 
account with a thick beard. Now, I have a shaved beard, and the system must still be able to authenticate that I am still the user. However, computer programs don't recognize
human features the way that people do, but just convert the data to a bunch of 0's and 1's. Another example of a challenge is that depending on the different lighting of a given
room, the scan might pick up a brighter facial scan or dimmer one perhaps, and thus, the bits obtained from the scan will not match what was initially recorded. 

False negative => Incorrectly decided not to authenticate
False positive => Incorrectly decided to authenticate
Crossover error rate => metric for describing accuracy of a biometric

With any form of biometrics (at this point), there will be a characteristic false positive and false negative rate. Both are bad, but we would like both to be low. However, they
usually are orthogonal, with one being low, resulting in the other being high. For example, if we are very stingy and only allow scans that are very close (say 0.001% off from
the original scan taken), then we will get a low false-positive, but will have a high false-negative (as we may incorrectly not authenticate the actual user), and vice versa. 

Lastly, when using biometrics to authenticate, another big issue is that most need some special hardware to do the authentication, which isn't present on most machines. 

- Linux login procedures:
Linux authenticates users based on passwords, and then ties that identity to an initial process associated with the newly logged in user. The process:
1. Special login process displays a prompt asking for the user to type in his identity. The user types in username, and then it is echoed to the terminal
2. Login process prompts for the password. The password is typed in, not echoed.
3. The login process looks up the name the user provided in the password file. if not found, then login rejected. If found, then the login process determines the user ID, the group
ID, the initial command shell that should be given to the user once login is finished, and the home directory that shell should be started in. 
4. The login process combines the salt for the user's password and the password provided by the user and performs a hash on the combination. If they match, then login proceeds.
5. Fork a process. Set the user and group of the forked process to the ID's retrieved earlier. Change the directory to the user's home directory and exec the shell process
associated with the user. 

- Access Control:
Taking the information (credentials) given by a user, the system must do something with it. It is a two-step process:
1. Figure out if the request fits within our security policy
2. If it does, perform the operation. If not, make sure it isn't done

The first step is "access control" => we determine which system resources/services can be accessed by which parties in which ways under which circumstances. The system runs some
algorithm to make this decision, by taking certain iputs and producing a binary output (i.e. yes or no) to grant access (or not). 
Subjects => the entity that wants to perform the access
Objects => The thing the subject wants to access (e.g. file or device)
Access => some particular mode of dealing with the object (e.g. reading/writing a file)
Thus, an access control decision is the process of determining if a subject is allowed to perform some particular mode of access to an object => "authorization", which can also
be defined as some process of verifying that a user can access an object in a certain way given certain criteria provided. 

When will access control decisions be made?:
The system must run the algorithm every time it makes such a decision to do so. The code to implement the algorithm is called the "reference monitor", and it obviously has be 
correct and efficient. If incorrect, then we make wrong access decisions, which is bad (perhaps fatal). It needs to be efficient because each time it is run, it incurs some
overhead. By the principle of "complete mediation" => we want to check security conditions every time someone asks for something. 

- Access Control Lists and Capabilities:
There are two approaches when trying to decide which structures and which methods should be used in granting access control:
1. Access control lists => are just lists (e.g. Doorman has a list, and is like "not on the list, step aside", or yes you are on the list, come in)
In OSes => if user X wants to read/write a file, then th eOS will look up a list of processes that can access the file on a file-specific ACL
2. Capabilities => like keys that allow one to access something (e.g. a ticket to go see a movie)
In OSes => if using capabilities, when a process belonging to user X wants to read/write a file, the process hands a capability specific to that file to the system

- Using ACLs for access control:
Each file has its own ACL, resulting in simpler, shorter lists, and quicker access control checks. For example, when a process of user X tries to execute an open() sytem call, the 
OS gets the process' PCB to determine who owns the process (i.e. user X). Then the system gets the ACL for the file that wants to be opened (which is like extended metadata), and 
look up user X on the list. If he is there, then grant access, if not, then no access. To further this example, say that user X is allowed to read, but not write the file. In this
case, the systemw ould deny access, if the process of user X wants to read and write, as it doesn't have full access to the mode it wants to access it. 

Note that the ACL is a type of metadata, and thus needs to be persistently stored, thus it is somewhere on disk. The ACL should be close to the file inode information, so that
we reduce the seek times. A good choice of its location would be in the inode, but other possible locations are the file's directory entry, or perahps the first data block of the
file. 

In addition, how long should the list be? If we do the obvious thing, then the list would be a list containing all known user IDs of the system and their access modes. This could
be 1000s of entries for a large system. Typically files belong to one user, and are often available only to that user and perhaps a small group. So we don't want to reserve space
for all users for an ACL for every file. In addition, if the list is large, then every time that access control is performed, the list needs to be searched (and if 1000 entries,
this could be a lot of overhead). In the original Bell Labs UNIX system, persistent storage was at a high premium, and as such, they couldn't afford to have large ACLs for each
file. They created an implementation that only required 9 bits. They figured that there are effectively 3 modes of access (read, write, and execute, for most files), and thus
most security policies only needed three entries on each access control list. The entries for their ACLs were partitioned into 3 groups: 1 for the owner (stored in the inode), one
for the group (stored in the inode), and one for everybody else (who isn't the owner or group). Their solution solved dual purposes: 1. solved the problem of taking up a lot of
storage for ACLs, and shortened the cost for each access check, and 2. the file's inode already needed to be accessed to do anything with a file, and thus embed the ACL in the 
inode. Thus, a list search need not be done, but just some bitwise logic on a few bytes in the inode. 

ACL analysis:
Pros:
- How to figure out who has access to a resource? => simply look at the ACL
- If you want to change the set of subjects who can access the object, all you need to do is update the ACL
- The ACL is typically kept with or near the file, so when getting the file's inode, you get the ACL
Cons:
- If using the UNIX approach, then list searching is removed, but the ACL is limited in what it can do
- If using a long list, then searches incur overhead
- If you want to figure out all the resources that a principal can access => you need to search every ACL of the system (which would involve searching every file perhaps)
- If in a distributed system => you need a common view of identity across all of the machines (e.g. if user remzi on cs.ucla.edu wants to access a file stored on cs.wisconsin.edu,
  is user remzi, actually user remzi on the Wisconsin server? Thus, a consistent namespace for all distributed machines might need to be made (which is very challenging)

- Capabilities for Access Control:
When performing the open() call, either your process would present the key to the OS to open the file, or the OS would find the capability for you. Either case, the OS checks
the capability does the mode of access that you wish to perform. Capabilities are just a bunch of bits, data. As capabilities grant access to privileged resources, they
are typically fairly long, and fairly complex. We want our capabilities to be "unforgeable" => we don't want a process to be able to generate a bit pattern on their own and make
it into a capability. In addition, we have a "revocation" protocol in place => that allows a process to have its access revoked. In ACL's this is very easy, as you just update the
list, but for capabilities, it can be either an easy or difficult process. We also want the capabilities to be out of reach from processes, and thus the OS controls and maintains
them somewhere in protected memory space. Processes can perform various operations on capabilities, but only if the OS is mediating the process. For example, if process A wishes
to give process B read/write access to a file using its capability, then A must make a system call requesting the OS to pass the capability to B. 

Thus, for capabilities as access control, the OS needs to maintain a protected capability list for each process. As the OS already maintains a per-process protected data structure,
you can just add a pointer to the capability list in the process' PCB. Therefore, when the process wants to perform some action on a file, the OS can just consult the capability
list. However, this capability list for each process would incur high overheads (e.g. a process could have 10s or 100s or 1000s of capabilities, one for each file it can access). 
Thus, the list needs to be kept in the OS protected storage, which could be a waste. Another option is for capabilities that are cryptographically protected to be left in the
user's hands. Cryptographic capabilities make the most sense in distributed systems. 

Capabilities Analysis:
Pros:
- Easy to determine which system reosurces a given principal can access => just look through his capability list
- If the capability is kept in memory, then it can be checked easily
Cons:
- Determining an entire set of principals who can access a resource is expensive (might have to check all principal's capability lists for a resource)
- System must be able to create, store, and retrieve capabilities to overcome forgery problem

Inheriting access control:
With capabilities, you can create processes with limited privileges vs. with ACLs, in which a process inherits the identity of its parent procss. 

Using both ACLs and capabilities:
In a typical Linux system, upon a process requesting to do a system call, the OS checks the file's ACL initially. However, on subsequent reads/writes, the OS creates a data
structure that serves as a capability indicating that the process has certain privileges for a file (which is attached to the process' PCB). When the file is closed, the PCB-
capability is delted. The cost of managing capabilities for all accessible objects is avoided because the capability is only set up after a successful ACL check. If the object
is never accessed by a process, then the ACL is never checked and no capability required. 

- Mandatory and Discretionary Access Control:
Who decides what the access control of a computer resource should be? For most people, they would say the resource owner, for the owner file, they would say themselves, for a
system resource, the system administrator should determine them, but for some systems, the parties who care most about information security may decide. For example, take
the military, and if any information is Top Secret (it doesn't mattter who created the file), the overall charge of information security in the organization makes the decisions.

Discretionary access control => Access control (whether almost anyone or almost no one) is at the discretion of the owning user
Mandatory access control => Mandated by an authority, who can override the desires of the owner of the information (i.e. military)

- Practicalities of Access Control Mechanisms: 
Most systems use discretionary access control. A typical computer can have 100s or 1000s or millions of files, and having each user individually set the access control permissions
would be infeasible. Thus, for Linux for example, when a user creates a file, they establish default access permissions. If desired later on, the owner can alter the initial ACL,
but experience shows that users rarely do. By default, you should leave access control permissions alone, unless you know what you are doing. 

- Role-based access control (RBAC):
Large institutions discovered that when trying to use standard access control methods they had a hard time, as different roles had different privileges. For example, doctors
might not have the same privileges as pharmacists, and thus we need a better approach to giving access control based on permissions for a given role. 

RBAC is the idea that allows groups of users to be dealt with in one operation when managing access control. For example, programmers in a company should have access to a new 
library, but janitors should not => so, the privilege is assigned to the Programmer role in one operation. If the programmer is promoted to Management, then remove the Programmer
role for him. RBAC appears to be like using groups in ACLs, but it is more formal. For example, there is a new authentication step when taking on an RBAC role, in which taking
role A requires relinquishing privileges for role B. RBAC systems may offer finer granularity than mereley reading/writing a file, as a particular role (for example a salesman) 
may be permitted to add a purchase record for a particular product to a file => this is called "type enforcement" => associates detailed access rules to particular objects using
"security context" of that object. 

- Linux setUID:
In Linux, you can build a minimal RBAC system using a feature called "privileged escalation" => which allows careful extension of privileges a particular program can make use of.
This is known as "SetUID" => it allows a program to run with privileges associated with a different user. The privileges are only granted during the run of that program, and lost
upon exit. The Linux sudo command offers a general approach, allowing osme designated users to run certain programs under another identity. 

=======
- Cryptography:
Thus far we have discussed the security goals, security policies, the authentication mechanisms, the access control mechanisms needed to enforce the security for the given computer
system. However, the OS only has control over its own system, and the periperhal devices attached to it. It doesn't have control over systems and interfaces with other computer
systems. To deal with the security issues of communicating with other computer systems, we need to assume that we are going to lose some data, or that there is an attacker who
is trying to steal our data. Thus, we can perhaps assume that the system that we are communicating with is also secure, but the problem occurs when the data is in transit between
the two secure systems. Therefore, if we can secure the data in the middle, we can have as strong a secure system as possible. How to protect data that isn't under the control
of either of the systems? This can be achieved by making the data unreadable to anyone who may be snoopin on the network via "cryptography".

Cryptography => a technique to convert data from one form (its ordinary, plaintext form) to another (its encrypted, cryptograhpic form). There also needs to be a way to make the
cryptographic form readable again by the other system that we are sending to. Thus, there needs to be some algorithm to encrypt and decrypt the data. The basic idea is that you
take a piece of data and use an algorithm ("cipher"), usually with a second piece of information ("key") to convert the data into a different form. The new form should look
nothing like the old form, and upon running another algorithm (perhaps the same one), the data can be returned to its original form. Thus, given the "plaintext", and using a key
and an encryption cipher, we convert the data to "ciphertext". And vice versa using a decryption algorithm. 

For encryption to be useful, it must be deterministic => a particular plaintext using the same key and cipher should always produce the same ciphertext, and vice versa. In 
addition, it should be very hard to determine the plaintext given the the cipher without the key. Without the key, the attackers shouldn't be able to retrieve the password from
the encrypted version, even if they know the cryptographic hashing algorithm used. Thus, the main point about cryptography: CRYPTOGRAPHY ONLY WORKS IF THE KEY IS KEPT SECRET. If
the attacker knows the key, as they already know the hashing algorithm used, and if they get the hash, they can easily obtain the password. Thus, the key must be kept secret for
encryption to work. Now, given key secrecy, if the attacker gets ahold of the encrypted data, there is nothing he can do as it is unreadable. If he were to alter the data, then
he would be altering the encrypted form, so the changes would make it so that the data wouldn't be decrypted to its original form. Thus the other system, upon receiving the info
and decrypting it, sees that it is unreadable, and determines that someone must have tampered with the data. 

There are two types of key cryptography: symmetric and asymmetric (or public key).

Symmetric Cryptography:
This is the form of cryptography upon which there is a connection between people who want to exchange information, and they use the same key (or copies of it) to dod so. In this
encryption, if you have a key that is shared with your friend. Now, you get a random piece of data from someone who didn't mention who it was. You try each of your keys, and for
a specific symmetric key that you share with your friend, the data decrypts and is readable. As you and that friend are the only ones that have that key, this means that the friend
was the one who sent the encrypted data. Thus cryptography can be used for authentication. 

In this form of cryptography, the same key (or copies of the same key) are used for encryption and decryption. 

Public Key Cryptography:
Symmetric key cryptography has the downside that to authenticate a piece of information, you need to know the key used to encrypt it. This means that we need to communicate and 
send our keys to everyone that we want to communicate with, which must be a secret action, as this key can decrypt and encrypt data. Thus, with public key cryptography, two sets
of keys are used: the private key and the public key. The private key is the one that a person keeps private, and there is only one copy of that key that the person owns. Attached
to this key are a bunch of public keys that can be distributed to everyone, and that is very well known (not private). For example, Microsoft could use their private key to 
encrypt a software update, and send it to all their users, who use their public keys to decrypt the message, and see the actual update. This way, the secrecy of key exchange
doesn't need to occur, as only the public keys are distributed, and that everyone knows the update came from the valid source of Microsoft as Microsoft is the only owner of the
private key used to encrypt the data. The other way works as well, in which the private key can decrypt, and the public key can encrypt. This allows anyone to send encrypted
messages to the owner of the private key, who is the only one that can decrypt the message. This allows for secret communication to be achieved both ways. 

However, using only a public key pair and sending from the private key owner to the public key owners is not secret communication. This is because everyone knows the public key, 
and thus can decrypt the data sent. To make a dual-way secret communication, the people communicating should use their own key pairs. This was person 1, encrypts the data with
their private key (now person 2 (along with anyone else) can decrypt it), and then also encrypt it with person 2's public key (now only person 2 can decrypt it). 

Unlike symmetric cryptography, public key cryptography is very expensive to compute computationally. Thus, PK can't be used in everything (despite being more secure than SK). Key
distribution is a trick procedure and even has its faults with PK. 

- Cryptographic Hashes:
Cryptographic hashes convert data from one form to an encrypted form. It is a necessity that these hashes don't produce any hash collisions, and that the hashing is one-way (i.e.
you can't get the password given the hash), any change to any input would result in an unpredictable change to the resulting hash function (even changing the data by a single bit,
would produce a completely different result). Cryptograhic hashing doesn't necessarily involve the use of keys. They can be used for data integrity as well as authenticity. 

- At-rest data encryption:
what if someone can get access to some of our hardware without going through the OS? If the data stored on that hardware is encrypted, and the key isn't there, then the 
cryptography will protect the data. It is called at-rest because the data is encrypted in place and isn't sent between machiens. Data can be stored on a disk drive, etc. If we
want to maintain its security, then we don't want to store it as plaintext, but rather in its encrypted form. However, if the data is encrypted, the machine would have to first
decrypt it, which would result in a lot of overhead. There is a form of cryptography called "homomorhpic cryptography" => allows operations to be performed on the encrypted form
of the data without decrypting it. 

A main use of at-rest encryption is called "full disk encryption" => in which the entire contents of a storage device is encrypted. Generally at boot time, th decryption key or
info to obtain it are requested by the user from the OS. If the user provides the right information, then the keys are made available. When data is placed on disk, it is encrypted,
and when data is moved off of the disk, it is decrypted. The data is never placed on disk in a decrypted form. After the initial request to obtain the keys at boot time, the
process of decryption and encryption is completely transparent to the users. 

Full-data at-rest encryption is used to secure a device's data perhaps if the device were stolen. As the other device doesn't have the key to decrypt it. 

Cryptography Analysis:
Cryptography is very computationally expensive and would incur a lot of overhead, the more it is used. 

================================================================================================================================================================================
Topic 16: Distributed systems: Goals, challenges, approaches:

- Introduction:
Distributed systems have changed the face of the world. Using a simple web broswer, or interacting with Google or Facebook, is actually interacting with a complex serve made
up of 1000s of machines, all cooperating to service the site. New challenges and goals arise with distributed systems. In terms of failure, well machines, disks, networks, and
software fail all the time; however, for distributed systems we want to make it appear to the client as if it never failed. The idea of having a bunch of machines connected 
together helps solve the problem of component failures, as the system as a whole can be made so that it is running, while some components fail, and its work is redistributed
perhaps to other components in the system. 

- Goals:
1. Improved reliability and availability:
Providing 24/7 service to the client despite the constant failures of the components making up the system (i.e. disk, software, network, CPU, etc). Thus the system is reliable as
it is always running and guarantees to the client that it is still working, and it is available as it provides 24/7 service
2. Scalability and performance:
We want the system to achieve low latency as well as high bandwidth to meet the growing demands of the user applications
3. Security:
Just like with any single system, distributed systems face the need to have security as when you connect to a remote site, you want to be assured that the remote party is who
they say they are, and that your communication and data is kept private and secure

- New aspect of distributed systems: communication:
How should machines within a distributed system communicate with one another?

The basic idea of modern networking is that communications is fundamentally unreliable => packets are regularly lost, corrupted, or otherwise don't reach their destination. These
types of packet loss or corruption could be due to errors in transmission due to network failures router errors, but they can also be due to buffering within the endpoint, in which
the router chooses to "drop" certain packets due to being overflowed with packets. If the end host becomes overwhelmed with packets, it may crash or may choose to just drop some
packets. Packet loss is fundamental in networking

Unreliable Communications Layer:
"End-to-end argument" => when packets are being lost, don't deal with the problem, but let the applications (which were designed to deal with this issue) handle it. An example
of an unreliable layer is "UDP/IP" networking. In UDP, a process uses the "sockets" API in order to create a "communication endpoint", and processes on other machiness send UDP
"datagrams" to the original process (where a datagram is a fixed-size messsage up to some max size). The client sends messages to the server, which then responds with replies. In,
UDP, packets get lost and thus don't reach their destination; however, the sender is never informed of the loss. UDP deals with packet loss by using a "checksum" to detect that
the number of packets sent should match the number of packets received, and if they don't match, then some packet loss occurred. For networking, a simple cheksum is just to sum
up the bytes of a chunk of data. Before sending the data, compute the checksum over the number of bytes of the message, and send both the messsage and the checksum to the
destination. The destination computes a checksum of its own of the bytes received, and compares it to the checksum received, to see if the number of bytes are the same. 

Reliable communication layers:
However, some applications want a more reliable communication setup, in which the loss of packets is handled, and perhaps retransmitted. Thus, when a client sends some message
to the server, it wants to be informed that the server did in fact receive the full message. This is achieved via an "acknowledgement (or ACK)", in which the receiver sends a short
ACK message back, which serves as a receipt that it received the message. What should the sender do if it doesn't receive an ack? In this case, a "timeout" is enacted, in which
the sender sends the message, and then starts the timer, which goes for a specified length of time. If the ack doesn't arrive within the period of time, then the sender can retry
the send . For this to work, the sender must keep a copy of the message around, in case it has to retry the send. This ack approach is known as the "timeout/retry" approach. 

However, there can be problems with the timeout/retry approach, in which the actual message is delivered; however, the ack gets lost. From the perspective of the sender, it didn't
receive an ack, and thus thinks the receiver didn't receive the message; however, for the receiver, it did receive the message, and sent back an ack (not knowing the ack was lost).
Thus, the receiver may receive the same message twice, which may not be okay (e.g. if you are downloading a file, and receive some duplicate bytes). Thus, we are aiming for message
to be received exactly once. To achieve this goal, the sender could generate some unique ID for each message, and thus the receiver keeps track of the ID's of messages received. 
Upon receiving a duplicate, it can ack to the sender that it received the message, but not process the duplicate. However, this technique is costly, as it has to have extra
memory to keep track of all IDs. Another approach is by using a "sequence counter", in which the sender and receiver agree upon a start value for a counter that each side 
maintains. When a message is sent, the current value of the counter is sent along with the message (the counter serves as the ID). After the message is sent, the reader then
increments the counter. If the ID of the received message matches the receiver's counter, it acks the message and passes it back to the sender. If the ack is lost, the sender
will resend message N, but the receiver counter is N + 1, as it incremented it upon sending the ack (which was never received). Thus, it can conclude that the message is a 
duplicate, so it sends the ack, doesn't increment the counter, and doesn't process the duplicate message.  

The most common reliable communication layer is "TCP/IP". TCP uses some of the techniques described above, but it also handles control flow such as restricting the sender, if the
receiver is overwhelmed with processing messages, or it can handle multiple requests at the same time. 

Communication Abstractions:
"Distributed shared memory (DSM) systems" => enables processes on different machines to share a large, virtual address space. This turned a distributed computation into something
that looks like a multi-threaded application, however the threads are present on different machines (instead of different processors). Most DSM systems work through the virtual
memory system of the OS. When a page is accessed on one machine, two things can happen: 1. the page is already local on the machine, thus the data is fetched quickly, or 2. the
page is currently on some other machine, and a page fault occurs, thus fetching the page. DSM systems aren't widely used today as they have a number of problems. The biggest
problem is how it handles failures. For example, if amachine fails; what happens to the pages on that machine? What if the data structures are spread across the machines, and
a machine failed. Obviously, losing a chunk of your address space is a big problem, and thus DSM isn't the best idea. In addition, it has a performance problem. As the memory is
meant to be shared, it was meant to be cheap to access memory. However, if the memory was present on a different machine, then an expensive page fault would ensue (i.e. must fetch
the data from a machine that could be very far away). Thus, programmers had to make it so that almost no communication between the machines took place, which basically defeats
the purpose of the DSM approach. Thus, it is trash af.

- Remote Procedure Call (RPC):
DSM systems were based on OS abstractions of using threads and shared memory to set up the distributed system. However, they performed very poory, so a programming langauge (PL)
abstraction was used instead, and created a very good idea known as "Remote procedure call (RPC)". 

The goal of RPC: make the process of executing code on a remote machine as simple and straightforward as calling a local function. 

Therefore, to the client, a procedure call is made (with the same interface), and some time later, results are returned. Again, abstractions are made to hide the implementation
details from the client, thus making it a more robust process. On the server-side, it defines some routines that it wishes to export. The rest of the RPC magic is handled by
the RPC system, which generates two pieces: the "stub generator (or protocol compiler)" and the "run-time library". 

Stub Generator:
The job is simple => remove some of the pain of packing function arguments and results into messages by automating the process. This automation reduces the errors that could occur
by writing the code by hand, and also the stub compiler could optimize the code and improve performacne. The input to the generator is simply the set of calls a server wishes to
export to the clients. The stub generator takes an interface consisting of the functions the server wishes to export, and generates a few different pieces of code. For the client,
it generates the "client stub" => which contains the functions specified in the interface. Thus, a client wishing to use the RPC service to communicate with the remote system, just
needs to link with the client stub, and then call into it in order to use RPC. 

Internally, each of the functinos in the client stub do all the work needed to perform the RPC. However, to the client, the code appears as a function call. Internally, lets do
an example: for function called x():
- Create a message buffer => a contiguous array of bytes of some size
- Pack the needed information into the message buffer => this is the process known as "marshaling" in which the arguments that the client provided to fill the need of the function
parameters provided are packaged into the message buffer
- Send the message to the destination RPC server => The communication with the RPC server, and the details required to make it operatre correctly are handled by the RPC run-time
library
- Wait for reply => function calls are usually "synchronous", so await the call completion
- Unpack return code and other arguments => If the function just returns a return code, then we are done. If the function returns complex results (e.g. lists), then the client stub
must unpack the return values => in a process known as "unmarshaling"
- Return to caller => Finally, return from the client stub back into the client code

Server stub portion:
- Unpack the message => "unmarshaling" of the data received from the client message. Extract the function identifier, and the arguments
- Call into actual function => The remote function is actually executed. The RPC runtime calls into the function specified by the ID and passes in the arguments
- Package results => "marshal" the arguments in the message buffer to return to the client
- Send reply => RPC server stub sends the message back to the client stub, which gives it to the client

Issues to consider with RPC:
1. What if the arguments to pass to a function are complex (i.e. passing pointers to a buffer perhaps)?
In this case, RPC is passed a poitner, and needs to be able to figure out how to interpret the pointer. For example, with the write() system call, the second argument is a pointer
to a buffer containing information to write to the given file descriptor. The RPC package can accomplish this goal by having well known types such as a buffer_t type that is
used to pass chunks of data given a size (so it will not pass a pointer, but the actual chunks of data to the server). 

2. Concurrency:
A simple server just waits for requests in a simple, sequential loop, and handles requests one at a time. However, this is inefficient, and thus RPC servers may be set up
concurrently by using a "thread pool", which utilizes "worker threads." Thus, when a message arrives to the server, it is dispatched to one of the worker threads, while a main, or
master thread, keeps receiving requests, and dispatching them to workers. This setup enables concurrent execution to take place within the server. 

Run-Time Library:
The run-time library handles much of the heavy lifting of the RPC system => the performance and reliability are handled by this bro. It faces several challenges:

1. Naming:
The simplest approaches build on existing naming systems (e.g. hostnames and port numbers provided by the current IPs). There needs to be a way for a client to access a specific
remote service by RPC via a port number of IP address.

2. Communication Transport Layer:
Does the RPC build upon a reliable protocol such as TCP, or unreliable such as UDP? 
Building RPC atop TCP can be a major roadblock in performance. TCP involves the use of retries/timeouts to make the communication reliable. This added work would result in acks
being sent across the RCP in addition to the messages already being delivered, which would incur a lot of overhead. Thus, most RPC packages are built on top of UDP (or other
unreliable layers). This makes performance more efficient, but adds the responsibility of providing reliability to the RPC system. The RPC uses some form of timeout/retry with acks
as discussed above, but does so perhaps on a different channel. 

3. Long-running remote calls:
Given our timeout machinery, a long-running remote call might appear as a failure to the client, and thus the client might do a retry. A solution to this would be to have the
sender send an "explicit ack", in which it notifies the client that it received the message. Then, instead of doing retries, the client can "poll" the server, by asking whether
the server is done with the request, and continue to wait. 

4. Large arguments: fragmentation:
The run-time must handle procedure calls with large arguments, larger than can fit into a single packet. A solution would be to "fragment" the large argument across multiple 
packets on the client side, and then on the server side, "reassemble" the pieces into the large argument. 

5. Byte ordering:
Some machines use big endian while others use little endian. Both are valid implementations, and to deal with this byte ordering issue, RPC packages provide a well-defined
endianness within their message formats. Using the "eXternal Data Representation (XDR)" layer can provide the endianness functionality. XDR works as follows => if the machine
sending or receiving the message matches the endianness of the XDR, then messages are sent and received normally. If, the machines have different endianness, then each piece
of information in the message is converted to the XDR format. 

6. Asynchronous or Synchronous:
Typical RPCs are sync, and only allow for a single request to be out-standing. However, some RPCs use async, in which the client uses a remote service via RPC, and can then
make calls to other RPCs, or do other computations, while waiting for the request to return. 

- RPC Interoperability:
There are tons of different systems running different OSes, running different OS versions, having different endianness, etc. Thus, RPC needs to be able to ensure that all changes
are upwards compatible, so that they can support new RPC versions for newer systems, while still supporting the older versions. Even betteer, RPC can support polymorphism, where
it speaks different languages depending on the protocol version of the client trying to use it. 

============
- Distributed System Security:

Introduction: 
An OS can only control its own machine's resources. Thus, with distributed systems, there is an increased problem with security:
1. Are the other machines in the distributed system who they say they are, and actually implement security policies
2. Even if all machines within a distributed system are secure, the network always gives a chance for network snooping
To combat these goals, cryptography will be the major tool used to secure data in transit and at the other system.

Role of Authentication:
How can we enforce that the other machines in the distributed system will implement our security policies? 
We can't, but we can try to arrange to agree on policies, and hope people follow through. All we can do is trust some parties will behave well, and that others won't, in which
if detected, we take some recourse action. As we are dealing with distributed system, most likely the systems are at a distance from each other over a network. For a trust-based
solution to work, we need to verify that the data did in fact come from the desired source, and that the data I sent to server A can validate that the data did in fact come from
me. This is an authentication job, and can be achieved by the use of passwords and/or keys. In both cases, this is authentication based on what you know (cryptographic hash plus
a salt), and on what you have (public key). When do use each?

1. Use passwords as authentication when => there is a large number of parties who need to authenticate to one party:
The reason not to use the key in this case, is because a bunch of people know the public key, but supposedly only a given party knows their password. 
2. Use PK cryptography when => when a single party authenticates itself to a bunch of users:
The reason for this is because all the users have the public key, and only one party has the private key. Thus, the only one who can encrypt a message that can be read by the users
is the party trying to be authenticated. 

How to use passwords and keys in distributed systems?:
Well if we are using passwords to authenticate, then we still need keys. This is because the passwords are traveling across some unreliable network, and even if it is a 
cryptographic hash plus a salt, it can be easily taken, and then can be potentially cracked by network snoopers. Thus, password authentication still makes use of keys to encrypt
the data while it is on the network, and to decrypt it upon arrival. Thus, we need a strong key distribution technique so that only the person we want to get the key actually gets
it.

PK Authentication in Distributed Systems:
The public key doesn't need to be a secret, but we need to be sure that it really belongs to our partner. The technique on actually authenticating web sites and other entities
on the Internet is by the use of "certificates" => which is a signed bundle of bits that validates that the public key does in fact belong to the given party. The certificate 
contains information about the party owning the public key, the actual public key, and other information (e.g. expiration date). The entire certificate is run through a 
cryptographic hash, and the result is encrypted with the trusted third party's private key, which digitally signs the certificate. Thus, the certificate is what powers the Internet
as it allows for systems to authenticate other systems, and thus communicate with them, by first trusting in a trusted party that signed the certificate. The trusted parties 
are known as "Certificate Authorities (CAs)" and are a widely trusted company. 

Example of a company obtaining a certificate, and then other parties authenticating the company:
Consider Frobazz Inc which wants to obtain a certificate for its public key. Frobazz pays top dollar to Acmesign Co, (which is a widely trusted CA). Acmesign checks that Frobazz
is legit, and the people requesting the certificate are actual reps of Frobazz. After validating this, Fromaz makes sure that the public key they are about to embed in the
certificate is the one that Frobazz actually wants to use. It then runs Frobazz's name, the public key, and other information through a top cryptographic hashing algorithm. It then
encrypts the just generated hash with its own private key (i.e. Acmesign's private key), to produce the digital signature. Acmesign then takes the generated hash, Acmesign's own
identity, and the signature into a certificate and gives it to Frobazz. 

Now Frobazz wants to authenticate itself to other customers. If customers already have Frobazz's public key, then no need for the certificate. If they don't, then Frobazz sends
the certificate to the customer, who examines the certificate, sees it was generated by Acmesign, and runs it through the same hashing algorithm. They then use Acmesign's public
key to decrypt the hash and the signature to obtain the acual hash, and verify that it was actually Acmesign. If all goes well, then the customer now has the public key of Frobazz,
and public key authentication can take place. If the two hashes aren't exactly the same, then the customer can see that Frobazz is shady af, or the data was perhaps tampered with.

Public Key Authentication Chicken-and-egg Problem:
The whole basis of this approach presumes that there is some trusted CA that signed the certificate of the company we are trying to authenticate. We have the CA's public key, but
how do we know that the key actually belongs to the CA. This is solved via "bootstrapping it". Basically, we need to start off with some public keys of companies that we can trust.
Thus, when installing a piece of software, it most likely comes with the public keys of several hundred trusted authorities. Thus, when authenticating a party, you check to see
if contains one of the trusted authorities that you trust. 

Replay attacks:
Now that we have authenticated some remote machine, and have their public keys, we are guaranteed that something they sent to me encrypted with their private key will only decrypt
with their public key (that I have), came from them at some point. However, there is such an attack known as a "replay attack" => in which a network snooper makes a copy of a
legitimate message and then sends it to me at some later point in time. In this case, I may receive the same message twice, and then perform some actions based on the message
that the legitimate site didn't ask for. To combat this issue, we can make it so that each message has a unique ID, and thus, we don't respond to duplicate messages. This feature
is built into standard cryptographic protocols such as (SSL). Note, that PK cryptography is very expensive, and we want to stop using it as soon as possible. This can be achieved
via SSL and TLS (talked about later). 

Password Authentication for Distributed Systems:
The other common option for authentication is using passwords. These are used when a bunch of users are trying to authenticate themselves to a single user. Thus, each user
has their own password and username, and they need to provide it to the other system. However, this information should be encrypted before being put out on the network. Thus,
what key should we encrypt them with? Well assuming that we obtained Frobazz' public key, we can encrypt it with this, and thus only Frobazz can decrypt it. However, this alone
could be subject to replay attacks, and thus we need to use SSL/TLS (which is discussed later). 

Problems with password authentication (and key authentication):
So, the initial message or authentication was verified to be legit from the legit system trying to be authenticated. However, this just serves as the initial authentication, but
future messages still need to be encrypted. The reason being is that anyone can make and send a message. We need to keep validating that the messages are indeed legit. However,
we don't want to keep using key encryption as it is very slow and would hurt performance. Thus, we will do a two-part proces:
1. Initial authenticity (using password or key)
2. Use another application to tie subsequent interactions to initial authenticity 
Enter SSL/TLS

- SSL/TLS:
A standard way to communicate between processes in modern systems is by the use of sockets. The same process holds true for distributed systems. Thus, a logical idea is to
add cryptographic protection to the communications using sockets. Thus, we can add the "Secure Socket Layer (SSL)" ontop of TCP or UDP. SSL didn't get everything right the first
time and then created the "Transport Layer Security (TLS)", which is like version 2 of SSL, but we can just refer to it as SSL. 

Idea => move encrypted data through an ordinary socket. There are a number of steps to get the process setup, despite the simplified idea. In addition, SSL is designed to support
a large number of cryptographic ops and many different ciphers, and multiple methods for key exchange and authentication. There is a large number of setup calls to setup SSL, which
would set up the connection, as well as choose the cipher to be used, etc. The last requirement of using SSL is how to securely distribute the cryptographic key that will be used
for the given session. 

Setup:
1. Start up negotiation between the client and the server. Each party might only be able to handle particular ciphers, hashes, key distribution strategies, etc. based on their
version of SSL. Thus this step => establishes the cipher to use, the techniques to transfer keys, etc. 
2. Authenticate the server (and optionally the client)
3. Creating and distributing a new symmetric key => Most common method is for the client to obtain a certificate containing the server's public key and use the public key to verify
the authenticity of server messages. With the certificate in hand, the Diffie-Hellman key exchange can proceed. Usually, the server is unsure of the authenticity of the client,
as the client usually doesn't have a certificate. This is fine, as this just sets up the encrypted communication, and then after it is set up the server can ask the client for
a password to authenticate. 
4. Running the communication through the chosen cipher using that key

Types of Key Exchange:
"Diffie-Helman Key Exchange" => use of a shared secret key, and agree upno two numbers, which you plug into some function

=========
- RESTful Interface:
Is one way of providing interoperability between computer systems and the internet. REST-compliant web services allow requesting systems to access and manipulate textual
representations of web resources using a uniform and predefined set of stateless operations
		- Stateless (server doesn't store state), cacheable (client can cache response), layered (client doesn't know if it is connected to end server, or an intermediary,
		  and scalability (can support a large number of resources and clients).
==========
- Peter Deutsch's "Seven Falacies" of Distributed Computing:
1. The network is reliable:
- Subroutine calls always happen. Messages and response are not guaranteed to be delivered.
2. Latency is zero:
- The time to make a subroutine call is negligible. The time for a message exchange is 1,000,000x greater or more
3. Bandwidth is infinite:
- In-memory data copies can be performed at very high rates. Network throughput is limited, as a lot of clients can saturate the switches, links
4. The network is secure:
- Have to worry about the network (i.e. man-in-the-middle, reply attacks), and the other systems which could be malicious, but also DDOS attacks
5. Topology doesn't change:
- Distributed applications must be able to deal with an ever-changing set of connections to an ever-changing set of partners
6. There is one administrator:
- There may not be a single database of all known clients. Different systems may be administered with different privileges
7. Transport cost is zero:
- Network infrastructure isn't free
8. The network is homogeneous:
- The distributed system is made up of tons of nodes that can run different OSes, different OS versions, different ISAs, word lengths, byte orderings, etc. 

===============================================================================================================================================================================
Topic 17: Remote Data, Synchronization, Security:

- Distributed File Systems:

- Network File System (NFS):

Introduction:
One of the first uses of distributed client/server computing was in the realm of distributed file systems. In this environment, there are a number of client one machines, and 1 
or a few servers, in which the server stores the data on its disks, and the clients request data through protocol messages. The server has the disks, and the clients send messages
across the network to access their directories and files on the disks. This setup allows for clients to share data easily. In addition, they allow for "centralized administration",
in which backuping up of files can be done by the few servers instead of by a ton of clients. Lastly, a distributed file system allows for security, as you can securely lock
all the servers in a locked machine room to prevent certain types of problems. 

Basic Distributed File System:
The cient/server distributed file system has more components that the single machine disk file systems studied thus far. On the clien side, exists a client application which 
access files and directories through the "client-side file system". A client app issues system calls to the client-side file system using the same interface (i.e. open(), read(),
write(), ...) to access the files stored on the server. Thus, to the client, the file system appears to be a typical file system => thus distributed file systems like everything
else provides transparency to the file system. The client-side file system is used to service the system calls from the client, and then send the messages to the "server-side
file system (or simply file server)" to read/write particular blocks. The client-side file system receives the data it requested of the file server, and returns it to the client.
Note that both sides make use of caching to speed up I/O opertions, and reduce unnecessary network traffic. 

Thus, there are two important pieces to a client/server distributed file system:
1. Client-side file system 
2. File server

- Network File System (NFS):
One of the earliest and successful distributed systems was developed by Sun Microsystems. Sun developed an "open protocol" => which specified the exact message formats that clients
and servers would use to communicate. 

Main goal of NFSv2 => design a protocol that is "simple and fast server crash recovery". With multiple-clients and a single server, this makes a great deal of sense, as any minute
the server is down makes "all" client machines unproductive. 

Keys to Fast Crash Recovery: Statelessness
"Stateless" protocol => the server doesn't keep track of anything about what is happening at each client. The server doesn't know for example, what blocks the client is caching,
or which files the client currently has open. It simply doesn't track anything about what clients are doing. Thus, each request from the client contains all the information
necessary to complete the request. In a "stateful" protocol, there is some piece of "shared state (or distributed state)" that contains shared data between the client and the
server. This leads to complicated crash recovery, as if a client were to issue two reads, and the server crashes after the first, then some recovery protocol would need to be done
which takes a lot of time to finish. Due to the reason of reducing crash recovery time, NFS is designed to be stateless => each client operatoin contains all the info needed
to complete the request. In terms of crash recovery, the server just reboots, and at worst, the client retries the request

NFSv2 Protocol:
Key to understanding the design of the NFS protocol is the "file handle" => which is used to uniquely describe the file/directory a particular op is going to operate upon. A file
handle consists of three components: 1. volume identifier (which file system), 2. inode number, 3. generation number (needed when inode numbers are reused) => which comprise a 
unique identifer to a file/directory. The client upon asking a request passes some parameters and the file handle, which the server fills the file handle and returns it to the
client, along with the other data requested. The lookup request to the server passes the file handle, and asks the server to return the information the file handle requires (i.e.
volume number, inode number, generation number). Once the file handle is available, the client can do reads/writes. 

From Protocol to  Distributed File System:
The client-side file system tracks open files, and generally translates app requests into relevant protocol messages. The file server simply responds to each protocol message. 
There are thre things to note:
1. The client tracks all relevant "state" for the file access => mapping of fd to FNS file handle, current file pointer
2. When sending long pathname (i.e. /home/remmzi/foo.txt), the client sends three LOOKUPs
3. The server receives a protocol message containing all the information needed to complete the request

Handling Server Failure with Idempotent Operations:
"Idempotency" => the property that the effect of performing the operation multiple times is equivalent to the effect of performing the operation a single time. Thus idempotent
operations allow for operations to be retried with no penalty or side effects. When a client sends a mesage to the server, a reply may not be received, which could be due to
packet loss, or perhaps a server crash. In either case, clients are left wondering what they sould do when the server doesn't reply in a timely manner. In NFSv2, clients handle
this problem with a "retry" request. After sending the request, the client sets a timer to go off at some time. If the timer goes off before a reply is received, the client
assumes that the server didn't get the request, and then retries the operation. If the server replies, then all good. If it doesn't, then send another request. LOOKUP and READ
requests are trivially idempotent. Also, note that WRITEs are also idempotent. Thus, the client can handle all timeouts in a unified way. 

Some operations are hard to make idempotent. For example, a client wants to create a directory, and then reply to the client it did, but the reply gets lost. Then the client after
some timeout will retry the request, and this time instead of the event re-succeeding, a failure occurs. 

Improving Performance: Client-side Caching:
Sending read/write requests across the network can lead to performance problems as the network is generally slow. To increase performance we can use client-side caching that caches
both data and metadata read from the server. Thus, the first access would be expensive, but subsequent updates (if in the cache) would be greatly faster and reduce network traffic.
The cache can also serve as a buffer for writes to do "write-back" and immediate reporting, and also large transfers. This however leads to a cache consistency problem as multiple
clients are using the caches. 

Cache Consistency Problem:
For example, consider two clients and a single server. Client 1 reads file F, and puts a copy in its cache. Client 2 now overwrites file F, to F2. Then a third client comes into
play, and is about to access file F. There are several problems that could arise:
1. Client 2 buffers its writes, and hasn't actually updated disk yet, and then Client 3 reads the file and gets the "stale" version => "update visibility"
2. "Stale cache" => Client 2 flushes its writes to the file server, and the server is up-to-date; however, client 1 has the old version of the file, and thus a stale copy. 

The cache consistency problems are solved via "flush-on-close (or close-to-open) consistency" to solve the update visibility issue, and some check to solve the stale-cache problem.
1. Update visibility:
Clients implement close-to-open (flush-on-close) consistency semantics => when a file is written to and then closed, the client flushes updates (i.e. dirty pages in the cache) to
the server. NFS ensures that a subsequent open from another node will see the latest file version, but only on a close. This makes sense, but has an edge case that results in
wasted writes to the server. For example, if a temporary or short-lived file was created in the client cache, and then was deleted in the cache, upon closing, the cache content
would still be flushed to the sever, which is a waste of CPU time. 
2. Stale cache:
Clients first check to see if a file has changed before using its cached contents. Specifically, when opening a file, the client-side file system issues a GETATTR request to the
server to fetch the file's attributes. The attributes contain info about the last server modification time, and thus the client can check to see if the file it has in the cache
is an old version, or is up-to-date. However, solving the stale cache problem this way lead to the NFS server being flooded with GETATTR requests. To solve this situation, an
"attribute cache" was added to each client. The client would still validate the file before accessing it, but most often would just look in the attribute cache to fetch the attr.
The attributes for a particular file were placed in the cache when the file was first accessed, and then timeout after a certain time period. However, this implementation still
stupid as the client may have an old version of a file, but only because the attribute cache hadn't timed out yet, which led to some odd behavior. 

Server-side buffering:
The file server also kept a cache in its memory, so that it can reduce the time going to disk. Note that for writes, the server can't report an "immediate-reporting" success by 
just storing the data in the cache and then writing it later. Instead, it has to wait for the write to be persisted to disk before replying with a yes. It can't do write-back as
the server is stateless, and if a crash occurred, it wouldn't know that it dropped the request, but it already told the client that the data was on disk. The main bottleneck then
of NFS is writes. 

========
- Distributed Synchronization & Distributed locks/leases:
When working within a single system, we had synchronization guarantees. Everything arn under the same operating system, or each thread ran under a given process, so we could use
locks to make events in critical sections mutually exclusive and guarantee atomicity. However in distributed systems, we have different processes running on different systems, and
there is no shared memory to use the typical locks => spatial synchronization has changed. 
In addition, you can't "totally order" spatially separated events, whuch makes the before or after atomicity lose its meaning => temporal separation. In addition, there can be
independent modes of failures, in which one partner (system) could die, but the others continue. Different clients can see different values at different times. Moreover, on a
single system, if a process dies, then the OS can free up its locks. However, if the system dies, the OS dies, and thus the locks are kept with the lost process.

One solution to this distributed synchronization issue is the use of distributed locks => "leases" => which centralizes the synchronization. A single server is responsible for
issuing the locks to the systems in the distributed system. The deadlock cases can be dealt with by replacing locks with leases. By default, the only way that a lock can be 
released by a process or thread is if the owner released the lock. There is no timeout, and if the process or lock is stuck doing a lengthy operation, then others will have to 
wait a long time for the long to be free. Leases on the other hand are timed-locks => they are the same as locks (i.e. grant the owner exclusive access to the resource until he
chooses to release it), but they also can expire, at which time the lease is taken from the process holding it. 

For normal operation, a lease works the same as a lock. Someone who wants exclusive access to a resource requests a lease from the central server, and when the resource is 
available the server grants the lease to the requestor. When the requestor is done, the lease is released and made available to someone else. The difference between locks and
leases is that locks work on the "honor system" => the only way an owner gave up the lock is if they finished with it. Leases provide this, but also when ever a lease owner tries
to access a resource, the resource manager checks two things: 1. do you have the lease, and 2. did the time of the lease expire yet. If the time expired, then the resource manager
will refurse to accept the request. 

Therefore, leases provide a serious boost over locks. If deadlock were to occur, or an owner of a lease were to crash, then in time, it is guaranteed that the lease will expire, 
and some other system can access the desired resource. 

Problems with leases:
1. If a lease-owner is in the middle of an update to a critical section, and the lease expires, then the operation won't complete, and the resource will be left in an inconsistent
state. Can solve this by implementing all-or-none transactions, or using "roll back" => to go to the state prior to the aborted lease
2. Choosing the lease period is important:
   - If too short, then owners would need to renew it many times to complete a session
   - If too long, it may take the system a long time to recover from a failure

- Evaluating Lease Performance:
1. Mutual exclusion => leases provide this with additional enforcement (i.e. checking time stamp and then kicking the owner out if expired during operation)
2. Performance => Remote operations are always expensive. However, if lease requests are rare and cover large number of operations, then this can be efficient
3. Fairness => Depends on the policies by the remote lock manager (i.e. queue or priority queue based)
4. Progress => Leases remove possibility of deadlocks. If a lease-owner dies, then other guys have to wait for lease to expire
5. Robustness => More robust than those in single-systems

Opportunistic Locks:
Another approach is to have an un-timed lease. In this way, the system is making use of the fact that for most resources, contention is rare, and thus locking code is highly
unlikely. Therefore, give a lease to an individual for as long as they want. When another system comes along requesting the lease, then revoke the lease and give it to the new
guy. This can be more efficient, and result in zero system recovery time. However, untimely new requestors could lead to situations where owners are in the middle of updates,
and have their leases revoked. 

- Distributed Transactions:
Atomic transactions => guaranteed to be uninterrupted, and all-or-none. They solve multiple-udpate race conditions, where all updates are made part of a transaction. The updates
are journaled but not actually made yet. After all the updates are made, the transaction is committed, or otherwise the transaction is aborted. Thus, the resource manager needs
to guarantee some "all-or-none" atomicity for multiple line operations. At the end of the transaction, you either commit or you abort. If atomicity isn't ensured, then someone
who gets preempted while working on a resource would leave the resource in an inconsistent state. All the operations are predicated on the state of the object, and thus the
transaction must also be interrupted, so the value doesn't change. 

Transaction => is just a list, and nothing happens until I say commit, at which it happens or it doesn't. The resource manager guarantees "all-or-none" as everything is committed
to the journal (just like with data journaling file systems). Thus, even if crashes occurred, just consult the journal and do replays. 

Single system atomic transaction:
The client will say to the server I want to do a transaction, the server allocates a list. The client says put this and this in the transaction, in which nothing is happening, but
the server is writing these updates to the journal. Finally, when the client says commit, the server puts that in the journal, and then acks the transaction, and tells the client
that eventually it will happen (i.e. write it to disk). If the client aborts, or a timeout occurs, then just clear out the list, and the transaction doesn't happen.

Problems: reliable, but not durable. If disk fails, information is lost. We thus want multiple copies of the data, despite a system failure.

Distributed system transactions:
We need multi-node transaction protocols that deal with the inconsistencies. One node might see the commit, but another node might not. And thus, when a crash occurred, did the ack
occur, or didn't. We need a multi-phase commit process for multiple nodes.

- Two Phase commit:
The protocol works by assigning one node as the "coordinator" (the guy wanting to do the transaction), and a network of other nodes => the "cohorts". The protocl assumes that
there is some journaling going on, to recover from crashes. The protocol is initiated by the coordinator after the last step of the transaction is reached, and the cohorts then
respond with an "agreement" message or an "abort" message depending on whether the transaction has been processed successfully at the cohort. 

Two part:
1. "Commit request phase (or voting phase)":
   a. Coordinator sends a "query to commit" message to all cohorts and waits until it has received a reply from all cohorts ("I would like to do this transaction")
   b. Cohort look and are like can we get the resources, can we get the locks, if yes, then they approve the query. Send ack to coordinator
2. "Commit Phase":
   c. Coordinator waits for all cohorts to reply, if all cohorts give an ack, then the coordinator can now proceed with the transaction (sends commit message to all cohorts)
   d. Cohorts see the commit message, and proceed to carry out the tranaction. Each cohort sends an ack to the coordinator
   e. The coordinator completes the transaction when all acks are received
- Note: If any cohort votes no in the commit-request phase (or the coordinator times out), the coordinator sends a rollback message to all cohorts (i.e. revert to last update 
before this transaction), and the cohorts all undo the transaction on their logs, release the resources/leases, then send an ack to the coordinator, who undoes the transaction
- Note: when a cohort says no, the most common reason is that a resource is locked

Pros:
- Achieves consensus => transaction only succeeds if agreed upon by cohorts
- Achieves all-or-none atomicity => all resources locked from the proposal to the commit

Problems:
We get to the point where everyone acked, or some people acked, and then the coordinator died. At this time, the coordinator got the locks and took up the resources, but the
coordinator died. The commit/abort never comes, as the coordinator dies => resources locked forever leading to a deadlock situation
- Unbounded delays if coordinator dies

- Three-phase commits:
Phase 1: Proposal
- Give proposal to cohort (and waits for cohorts to respond)
- Difference => coordinator is just asking if the cohort thinks they would want to do, doesn't actually bind anything
Phase 2: Preparation to commit
- All proposals acked to the proposal
- Now give "startCommit" message to all cohorts
- Cohorts go forward with the commit
- Difference => even if a time out, cohorts go on with the commit
Phase 3: Actual commit and confirmation:
- Can still be aborted by the coordinator
- Will commit by default (but coordinator tells them to commit to not have to wait the 3 seconds)
- Confirm from coordinator means all cohort agrees

Three Phase Commit Analysis:
The added phase is just a proposal, at which time the cohorts respond, but don't actually acquire any locks or anything. Thus, if the coordinator dies or times out, there will
be no deadlock. The second phase is the prepare to commit, where the cohorts will grab the locks and resources. Now, even if the coordinator dies, they will continue with the 
commit.

Pros:
- Achieves consensus
- Achives all-or-none atomicity
- Non-blocking => automatically commit or abort after timeout
- Can tolerate node failures => even if coordinator dies, carry on with the commit

Problems:
If all cohorts don't have consensus. But they all acked to the proposal right? However, the coordinator just asked if it was acceptable, and then after that the coordinator told
one guy that they were doing it, but then the coordinator died, and the other cohort guys never received the command. Therefore, the consensus was not reached, due to some form
of network partitioning

- Distributed Consensus:
Achieving simultaneous, unanimous agreement (even in the presence of node and network failures).

General idea => every interested member broadcasts their nomination. And there is a rule for evaluating proposals (fixed and well known rule). After allowing a certain time to
allow for proposals, each voter acks the best proposal it has seen. If a proposal has a majority of the votes, the proposing member broadcasts a claim that the question has been
resolved (i.e. he broadcasts, I win). Now, each party that agrees with the winner. Election is over, as the quorom acknowledges the result. 

Distributed consensus:
Achieves a simulatenous, unanimous agreement (even in the presence of node and network failures). They may take a long time to converge on results, but it is guaranteed that
"eventual consistency" will be achieved. 

- Peer-to-peer client/server model:
I have file systems, I have a network, you do to, we cooperate and share. Resources are all local, but make my resources available to other people.

- Thin-client:
Extremely limited capabilities, most resources are remote. And the thin-client is just enough to exploit them

- Cloud services:
Why are you obsessed with servers? Clients access services rather than resources. Stop talking about servers and computers, and the cloud will provide whatever it is that you
want. The cloud model says don't think about the servers, we will use the resources from elsewhere

- Remote File Transfer:
Can use explicit commands to copy remote files (i.e. scp()). Or the use of File transfer protocols (i.e. FTP, SFTP). Advantages are that it is efficient, and doesn't need OS 
support. The disadvantage is that it lacks transparency. Can be done all in user mode though. 

================================================================================================================================================================================
Topic 18: Remote Data Performance and Robustness:

- Andrew File System (AFS):
In addition to the Network File System learned about created by Sun, there exists AFS whose main goal was to => scale (i.e. how can one design a distributed file system such
that a server can support as many clients as possible?)

How does it achieve scalability? => By its design of the protocol between client and servers. For example, NFS couldn't support aas much as clients, as it forced clients to check
with the server periodically to determine if cached contents have changed (incurring CPU and network bandwdith). In addition, NFS had cache-consistency problems, but AFS makes
it simple => when a file is opened, a client gets the latest consistent copy from the server

AFSv1:
Consists of "whole-file caching" on local disk of the client machine accessing a file. When an open() of a file occurs, the entire file is fetched from the server and stored in
a file on local disk. Thus subsequent read/writes are redirected to the local file system (requiring no network communication, and is much faster than going to remote). Upon close,
the file is flushed back to the server with modifications if they took place. This is an obvious contrast to NFS which caches blocks, and AFS caches in local disk, while NFS
caches in memory. 

For example, when a client first calls open(), the AFS client-side code will use the Fetch protocol to send a message to the server with the full pathname of the file, and will
return the entire file and cache it on the local disk of the client (writing to local disk). Now all subsequent reads/writes are local in AFS and require no communication with
the server. Now, that the calls are strictly local, upon subsequent requests, the data retrieved can be read/written to an in-memory cache thus speeding up performance even more.
When finally finished with a file, the client calls close(), and if any updates were made, the client flushes the new version back to the server via the Store protocol message.

For subsequent accesses to the same file, the procedure would be much more efficient. First the client-side code issues the TestAuth protocol message asking if the file has
been modified since the last time the client has touched it. If not, then the client can just go to the cached copy on disk. If it has changed, then repeat the above process. 

Problems with AFSv1:
Measurement is the key to understanding how systems work and how to improve them, and thus the designers of AFS found flaws and created AFSv2. Two main problems were found:
1. Path traversal costs are too high:
When performing a Fetch or Store protocol request, the client passes the entire pathname to the server. The server, in order to access the file, must peform a full pathname
traversal, first starting at the root directory, until it finds the desired file location. With many clients accessing the server at once, the designers of AFS found that the
server was spending too much CPU time walking directory paths
2. Client issues too many TestAuth protocol messages:
Similar to the GETATTR protocol messages of NFS, AFSv1 generated a large amount of traffic checking if the local cached version of a given file was up-to-date. Thus, servers spent
a lot of time telling clients whether their cached versions were OK, or not. Most of the time, the answer was that the file had not changed (and thus the waste of time).
3. Load was not balanced across servers => solved by introducing volumes
4. The server used a single distinct process per client thus inducing context switches and high overheads => solved by using threads in AFSv2

Improving Protocol:
The two main problems: pathname traversals and TestAuth checks limited the scalability of the file system, and the server CPU became the bottleneck due to the overflow of
requests from clients and traversing pathnames. Enter AFSv2:

AFSv2:
Introduced the notion of a "callback" => used to reduce the number of client/server interactions. Callbacks => are a promises from the server to the client that the server will
inform the client when a file that the client has cached is modified. Thus, this piece of "state" makes it so that the client doesn't need to contact the server so often to see
if the cached copy is still valid. Rather, the client assumes the file is valid until notified otherwise. Thus, this process of using callbacks instead of using TestAuth messages
is analogous to instead of polling the server repeatedly, and burning CPU time, and increasing client/server network traffic, instead have the server interrupt the client when
a file has been modified, at which time the client will deal with the modified file problem. This was way more efficient as there is a much smaller number of servers than clients,
and thus the server notifies the clients instead of a bombardment of client TestAuth messages building up a large amount of traffic. In addition, most often the file is in fact
the same as the one the client has cached, and thus the callbacks would take place much less frequently than with the TestAuth which may have occurred very often from very many
clients. 

In addition, AFSv2 added the notion of a "file identifier (FID)" (which is similar to the NFS "file handle"). Instead of using pathnames to specify a file, the FID was used 
instead. The FID consists of the volume identifier, the file identifier, and a "uniquifier" (to enable reuse of the volume and file IDs when a file is deleted). Therefore, instead
of having to send whole pathnames to the server and letting the server walk the pathname to find the desired file, the client would walk the pathname, caching as it went along 
parts of it, and hopefully reduce the load on the server. For example, if a client accessed the file /home/remzi/notes.txt, the client would first Fetch the directory contents
of home, put them in the local-disk cache, and set up a callback on home. It would then Fetch the remzi directory contents and set a callback on remzi. And lastly, it would fetch
the file it is interested in (i.e. notes.txt), cache it on local disk, and set a callback, and finally return a file descriptor to the calling application. 

The key difference here vs. NFS is that each fetch set up a callback with the server, thus ensuring the server would notify the client of a change in its cached state. This is
beneficial because (despite the obvious slow first access which generates a lot of messages and callbacks), for subsequent accesses to the file or a lot of other content now
on disk, there required no server interaction. 

Cache consistency:
When discussing NFS, there were two aspects of cache consistency: 1. update visibility (when will the server be updated with a new version of a file?) and 2. cache staleness (once
the server has a new version, how long before clients see the new version instead of an older cached copy?). 

Because of callbacks and whole-file caching, the cache consistency provided by AFS is easy to describe and understand. There are two important cases to consider:
1. Consistency between processes on different machines:
Between different machines, AFS makes updates visible at the server and invalidates cached copies at the exact same time => happens when the updated file is closed. For example,
a client opens a file and writes to it. When he closes the file, the new file is flushed to the server (and thus visible). At this point, the server then makes the callbacks for
any client that has a cached copy (thus ensuring no stale copies of a file). Subsequent opens onthos clients requires a re-fetch of the new version of the file. 
2. Consistency between processes on the same machine:
When processes are on the same machine and have cached copies of the same file, then writes to a file are immediately visible to the other processes. 

Edge case: different processes on different machines doing writes to the same file at the same time:
When processes on different machines are modifying a file at the same time, AFS employs the "last writer wins" approach => whichever process calls close() last, will have their
changes persist to the server file system. The one that called close() earlier would have their changes lost. This ensures some form of atomicity, as it ensures that the file
contains all of the changes made by one the processes, and not some inconsistent state of having some of both process' changes. With NFS, this is not the case. NFS employs block
based protocols, and thus writes can flush individual blocks to the server as clients are updating a file. Thus, the final file could end up a mix of updates from both clients.

Crash Recovery:
Note that crash recovery with AFS is more involved than with NFS as AFS has some state it stores (i.e. in the form of callbacks). There are two types to note here: 1. client crash
and 2. server crash:

1. Client crash:
As clients set up call backs on files that they have cached on local-disk, they are relying on the server to notify them when cached files are modified on the server. Now, suppose
that the client is booting up, or has crashed perhaps, well during this time the server is unable to contact it, and thus the client, might miss callbacks if they came during that
time. To handle this, the client should think all its files are suspect (perhaps modified), and thus make TestAuth protocol messages to the server of all files that are cached. 
2. Server crash:
Callbacks are kept in server memory; and thus, when a server reboots, it doesn't know which clients have which files cached. Thus, upon a server restart, each client must realize
that the server has crashed and treat all of their cache contents as suspect, and then make TestAuth message requests. Thus, upon a server crash, the server could tell every client
that they should issue a TestAuth protocol request. Another approach is to have client check that the server is alive periodically using the "heartbeat" message. With NFS, clients
hardly notice a server crash, as the server is stateless. However, with AFS, the server does store some state, and thus, upon crashing, there needs to be a recovery procedure.

AFSv2 Scale and Performance:
Version 2 was found to be much more scalable due to the callback and the file identifier to reduce the traversals of pathnames. In addition, AFS had fast client-side performance
and less write traffic between the client and server, as it made use of whole-file caching. As, the clients also had an in-memory cache, the reads/writes were more beneficial from
having the local disk copy. Moreover, AFS handled cache consistency problems more efficiently and more robustly. To deal with this problem, the callbacks made it so that when a
client called close on a file, the server would be updated, be made visible to all clients, and then a callback would tell all clients that their cached copy is stale, and to get
an updated copy of the file. Lasty, AFS was slower when it came to crash recovery, but this is to be expected as AFS servers stored some form of state (i.e. callbacks). 

======
- Authentication servers:
In a large distributed system, keeping track of info about all of the authorized users and what each is authorized to (i.e. capabilities or ACLs) is a lot of work, memory, and 
would incur a good deal of overhead. The use of PK certificates is a means that a server can use to authenticate itself, but it is impractical to have all actors be registered
in this way. A public key certificate is a type of "credential". We can use a similar technique to get trusted "capabilities" => where the principal would get authenticated and
given a capability at the same time:
      - Actor would contact authentication server, and describe the mode of access it wants to perform on an object
      - The authentication server would both authenticate the principal and determine whether the actor had this mode of access
      - If successful cyck, then the principal would be given a "work ticket" => capability describing the principal, the permitted access operation, an expirtation date, and 
      	a digital signature from the server
      - The principal would present the work  ticket to the appropriate server
      - The server wouldn't have to do authentication or authorization, but just verify the work ticket. 
The process of using work tickets removes the overhead and responsibility of the application server to do authentication and authorization and instead give that job to another
server, whose job it is to do such a thing. These work tickets are unforgeable => they can't be created by random agents as the authentication server signed it with his private
key. However, servers just check the validity of the work ticket, and not necessarily the holder, and thus work tickets can be stealable. To satisfy this end, the authentication
server generates a session key that is encrypted with a copy of the client's public key and the server's public key.

Thus, each work ticket contains a session key that is generated by the authentication server, and when the application server receives the work ticket, it can verify the 
authentication server's digital signature and trust the client has been authenticated. 

- Kerberos:
Instead of using public key encryption, symmetric encryption could be used as it is a cheaper operation

=======
- Distributed File System Goals:
For distributed file systems, it is the process of having some remote file system in which there are a bunch of clients connected to it. The remote file system is controlled by
a server, and has a bunch of disks to store data from many clients. Each of the clients can access any of the disks the server contains. However, unlike with local disk storage,
you can't just read and write in the normal sense. Instead, you have to send messages across a network as the file system is at a remote location and is not attached to the given
client system. Network communication, sending messages back and forth between the client and the server, is a costly process. 

Goals:
- Security:
As this is a distributed system, the communication between nodes within the system usually take place across unreliable networks. The systems are usually at remote locations from
each other, and network communication needs to take place. The network (when data is in transit) is usually not very secure, and leaves open the potential for network snoopers to
perform attacks on the data in transit. There are various forms of attacks that could take place on data being sent over a network with the two main ones being: "man-in-the-middle"
attacks and "replay" attacks. Man in the middle attacks are when there is some attacker who is listening in on a given network. Now, he can do various things. He can either just
copy the bytes being sent over the network, and try to crack passwords or encrypted data, or he could tamper with the data. Replay attacks are when an attacker takes a copy of a
legit message sent over the network, and then at a later time, replays the message being sent (i.e. sends the copy to the other system), which would cause the system to perform
some action on the message that the server that sent the message isn't expecting at this time. These forms of attacks can be stifled or better secured by the use of encryption.
Data should never be sent as plaintext over a network, and should be encrypted at all times using some cryptographic hashing algorithm. By encrypting the data, it becomes 
unreadable to potential attackers. Usually, as two systems are communicating with one another, the encrypted data needs to be decrypted (or readable) by the system that the data
is being sent to. And thus, there needs to be a way that the receiving system can decrypt the data. This process can be achieved by the use of key cryptography: either symmetric
key cryptography where both sides have symmetric (the same key) that is used to both encrypt and decrypt data, or with asymmetric (or public key) cryptography in which there is
a private key and a public key. The whole idea of cryptography is that the keys be kept secret as they are the main means by which the cryptography works. The hashing algorithm
is usually widely known, and thus, the only form of secrecy and protection is in the key. Thus it should be kept secret. Keys are distributed to other parties using certificates
which are protected by a trusted Certification Authority to oversee the process. Anyway, I got side-tracked, but the security goal of distributed file systems is to make sure
that the data being sent/received from a file system is not tampered with and is.
- Performance:
As with all computer systems or file systems, performance is a main goal that pretty much needs to be achieved. If a file system is slow, and takes a while to access, then clients
will be less likely to use it, and it will be pretty useless. Distributed file systems involve the client/server model, which can only be achieved by the use of messages that are
sent to and from the server and client in order to communicate operations that are requested to be performed on the given data of the file system. However, network communication
and messages in general is a very costly process. Thus, it would be ideal to limit the amount of communication between the clients and the servers, and to only communicate when
necessary. Often with performance, it is a goal to improve the latency (single client response-time), but also to improve the throughput or file system bandwidth. However, these
are usually conflicting goals, with increasing the bandwidth being the more desirable goal as we want the entire file system to have fast performance at the expense of individual
file requests. To reduce the amount of time that communication between the client and server occurs, distributed file systems can make use of caching: both reads and writes. There
are various types of caching (i.e. blocks or whole files), but the idea is that by caching information, you can read directly from the in-memory (or local disk) cache, instead of
having to make requests to the remote server everytime, which would lead to more traffic on the network. With reading, a technique called "read ahead" caching can be achieved, 
where a process may request to read some piece of data (perhaps a sector in size), and instead you retrieve a larger amount of data (perhaps 4 or 5 sectors), and store it in the
cache. By retrieving more than what was requested, this allows for subsequent read requests to only have to go to the fast in-memory cache, instead of having to make a request
to the server each time. In terms of writing, there are different times of write caching. The most popular in terms of performance (for both local file systems and distributed) is
to make use of write-back caching and immediate reporting. This is the procedure in which a process may issue to the OS to do some store to the file system (write to the file 
system), but instead of doing it right away, you say that you will do it right away, but you store the write information in the cache. You store a lot of writes together, or you
"aggregate writes", and then write them all out to the file system in a large transfer size. This is obviously more efficient and reduces the amount of overhead because instead of
doing a bunch of little writes (which adds to the network traffic), you only do a single write (which occur much less infrequently). Therefore, the client/server interaction is yet
again reduced. In addition, whole file caching can be utilized to also boost performance. Instead of requesting many reads of a file from the server, you read in the whole file
into a cache (perhaps on local disk). Thus, future reads (or writes) are faster as they are local or use the in-memory cache, and the reads don't have to be requested to the server
now as they are local. Thus, this reduces the amount of network traffic. In addition, by writing whole files at once, the amount of write requests to the file system is reduced.
Lastly, bandwidth performance aspect can be improved by having many servers instead of a single one. A single server if communicating with many clients has limited throughput as it
has to process all those requests. However, if there are more servers, then the requests can be spread over all the servers, and thus the throughput can be improved. If the files
are striped, then reads could be more parallel and be done at once if the requested data is striped throughout the disks.
- Reliability:
Reliability can be achieved in the form of data mirroring or by using some parity/erasure coding. By default, a single disk to store a file system isn't very reliable because there
is only a single copy. Thus, if the file system were to crash or lose some bits, they could never be recovered. Thus, there could be multiple copies of the file system that are in
different locations around the world to increase the reliability. Also, Redundant Array of Inexpensive Disks (RAID) systems can be used to provide more reliability by increasing
the number of disks that store the information. Now, if all we did was use striping across all the RAID disks, then we can exploit better performance, but it wouldn't provide any
form of reliability or redundancy. The technique of mirroring can be used, which in RAID systems, or just by having backup file systems on disk, you have multiple copies of the 
same data. Thus, when writing to some file, you would actually write the same data to multiple copies of the file system. This makes it so that, if one disk were to fail, then
there would be another copy. Another form of reliability can come in the form of parity/erasure coding. In RAID systems, this makes use of one of the disks to server as the parity
code, which performs some parity operation on the actual file system, and stores the computation on the parity disk. The parity disk contains the necessary data, thus if a single
block of a stripe were to be lost or corrupted, it could be recovered by using the other blocks of the stripe and the parity block. This makes it so that an entire disk could fail,
and then using parity coding, the entire disk (a whole column of a stripe) could be recovered. 
- Robustness:
This is especially important in distributed systems. By design (and there is no way to make perfect components), CPUs, file systems, servers, networks fail/crash from time to time,
it is inevitable. Thus, instead of trying to build a perfect system, you make it more robust by having fail-over procedures, back-up copies, recovery protocols, and other 
techniques put in place, thus when one component fails or some corruption is detected, the system can continue to be available and still be functional and reliable.
- Consistency:
Unlike with a single writer or a single file system (with no clients, perhaps a local one), it is easy to keep the file system consistent. You make writes atomic, or do some data
journaling, thus when a write is in the middle of taking place, and some failure occurs, it can be recovered and put in some consistent state. However, with distributed systems,
there are multiple writes and multiple readsers, and thus we want the file system or the client's caches to be in some relatively consistent state, thus they know that they are
operating on up-to-date data of the file system, and not some old copies. This can be achieved by using various techniques such as cache timeouts (in the NFS case), or by using
write-back protocol, and updating visibility and removing stale caches at the same time.
- Scalability:
We want distributed file systems to be scalable. We want a particular server to serve as much as clients as possible. This makes the file system more usable by a larger number of
people. However, scalability and performance go hand-in-hand. We don't want to just service more clients, but we want to do efficiently and in a high performance manner. 
- Availability:
Lastly, we want a file system to always be available to service clients (or as much as possible). Thus, when a server crashes, we want it to be able to boot up as quickly as
possible, get back into sync with its clients, and begin to accept requests again from the clients. The fastest way to achieve this end is by having stateless servers. Stateless
servers maintain no information about what clients are doing (i.e. what files they have cached, what file they just accessed, etc.). Instead, the servers are presented with 
requests that give them everything they need to know about completing the requests. Upon finishing the request, the server moves on to the next one. Thus, if the server crashed,
there need not be any detailed recovery procedure to reattain consistency. At the worst case, some clients have to reissue requests. NFS makes use of the stateless server, while
AFS has a server that is near stateless, but stores writeback information. This is fine though, as when either the client or the server crashes, all we have to do is have all
the clients (or just the one that crashed) send the TestAuth protocol to revalidate or update their cached contents. In addition, we want to have idempotent operations, which means
that carrying out the operation over and over again has the same result as if it were just carried out once. Thus, this keeps things consistent, and when crashing of the server 
occurs, or some packets are lost in transit, we can just have the clients reissue the request with no harm being doing. Lastly, and most importantly, upon a server or remote
file system crashing, we want to make sure that the file system data isn't lost. Thus, we need to have copies of the file system (if the disks failed), or other servers that can
takeover requests from the clients of the crashed server, until the server is back up and running. Thus, we need a fail-over procedure.

- Andrew File System vs. the Network File System:
These are the two types of distributed file systems that were discussed. 