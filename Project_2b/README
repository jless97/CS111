Identification:
    NAME: Jason Less
    EMAIL: jaless1997@gmail.com
    ID: 404640158

Slip Days Used: 1

Description of included files (Features):
—————————————————————————————————————————
(1) lab2_list.c: This C program involves the use of threading to update a
shared doubly-linked list. The threads perform different operations on the
list by inserting, looking up, deleting, and getting the length of the list.
This program also supported yield and synchronization options to present
another synchronization/race condition problem on a data structure instead
of a single variable. In addition, a new option (i.e. list) was added to 
support the use of a specific set of sublists to divide up the main list. 
Unlike Project 2A, which used coarse-grained locking, this list option
provides finer-grained locking to take place to allow for more parallelism.
At the end of the program, the list should have length of 0, given the 
correct synchronicity of the threads.

(2) SortedList.c: This file consisted of the implementation of the doubly
linked list (used for lab2_list.c). It was implemented as a circular list,
and supported insert, lookup, delete, and list length operations. 

(3) SortedList.h: Header file for SortedList.c

(4) lab2_list.csv: A CSV record for different tests ran for lab2_list that
consisted of the name of the test, number of threads, number of operations,
number of lists, total number of operations, total run time, average
run time, and average wait for lock time.

(5) lab2b_1.png: Image showing the throughput vs. the number of threads for
mutex and spin-lock synchronized list options.

(6) lab2b_2.png: Image showing the mean time per mutex wait and the mean time
per operation for mutex-synchronized list operations.

(7) lab2b_3.png: Image showing successful iterations vs. threads for each 
synchronized option.

(8) lab2b_4.png: Image showing the throughput vs. the number of threads for
mutex synchronized partitioned lists.

(9) lab2b_5.png: Image showing the throughput vs. the number of threads for
spin-lock synchronized partitioned lists.

(10) lab2_list.gp: gnuplot data reduction script for lab2_list that was used
to generate the graphs (i.e. *.png files).

(11) sample.sh: Test script used to test the programs and generate the .csv
file

(12) profile.out: The execution profiling report that shows where time was
spent in the un-partitioned spin-lock implementation.

(13) Makefile: To build the program (lab2_list), but also has five other 
targets: tests (to test and generate the .csv files), graphs
(to generate the .png files), dist (to make the tarball), clean (to 
remove all generated programs and output), and profile (to use the profiling
tool to generate profile.out).

Testing Methodology:
————————————————————
The programs were tested using similar methods to previous projects. After
each system call, I had prints to stderr in case any failed, with 
descriptive methods. In addition, when debugging the code, I used printf()
statements throughout to isolate and track down the problems. Finally, the
sample.sh test script was used to make sure that the desired output was
correct and in proper format. 

Citations and Research:
———————————————————————
- The TA discussion section/slides, Piazza responses, and man pages for the
  used APIs were especially useful for this project
  - A good deal of my concerns (and questions) were answered with fairly 
    good clarity from other students, and TA’s
- The websites recommended for studying before beginning the project were
  the main source I used when implementing this project.
- To help when creating the doubly-linked list, I referred to Carey 
  Nachenberg’s slides from CS32. This was the main reference used for the
  SortedList.c implementation.
- In addition, I referred to one of my previous projects from CS35L to 
  make sure that I was passing the threads to the function in the proper
  way, and looping through in correct increments.
- The .gp files from last project were used as a reference (as recommended)
  to generate the .gp file for this project. Using the previous .gp file,
  I played around with the settings to get the desired format. The graphs
  were created with reference to the TA’s discussion slides as well.
- Lastly, the Arpaci text was referenced (specially the chapters discussing
  race conditions and locking methods) when doing this project.

Lab Questions:
——————————————
2.3.1: Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list
tests?

I believe that most of the cycles are being spent actually performing the list
operations (i.e. insert, lookup, delete, length). As there are only 1 and 2 
threads in these tests, the contention for the resources (locks) are fairly
low. Thus, the time is spent working on the list, instead of having to wait
to get the desired resources.

Why do you believe these to be the most expensive parts of the code?

As discussed in the previous question, there is low contention for the locks,
so the threads (1 or 2) are able to get their locks faster (i.e. don’t have
to waste time spinning or being blocked, waiting for locks), and perform
their list operations at a higher rate than if more threads were introduced.

Where do you believe most of the time/cycles are being spent in the high-thread
spin-lock tests?

I believe that most of the cycles are being spent waiting for locks (i.e.
spinning). As the number of threads for the test is high, there would be an
increased contention for the locks. Therefore, the threads would be spending
a lot more time trying to acquire the locks than performing the list operations.
The reason for this assumption is due to the fact that the critical section
for any number of threads in the program is constant, and thus, the added time
is spent getting the locks.

Where do you believe most of the time/cycles are being spent in the high-thread
mutex tests?

I believe that most of the cycles are being spent blocked (i.e. sleep/wait) for
the mutexes to be released. It is the clear that the spin-lock implementation
with high threads wastes a lot of time spin-waiting for the locks. Similarly,
the increased number of threads, resulting in higher resource contention, also 
leads to a decrease in throughput, and more wasted cycles for the mutex 
implementation as the threads also have to wait for locks to be released before
proceeding into the critical section. 

2.3.2: Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock
version of the list exerciser is run with a large number of threads?

Most of the cycles are being spent in my listOperations function, but more 
specifically at the lines where the threads are trying to acquire the locks.
As there are a large number of threads, there is a high contention rate for
the locks, and thus, most of the cycles are being spent spinning, while
trying to acquire the locks. 

Why does this operation become so expensive with large numbers of threads?

This operation becomes increasingly expensive as the number of threads 
increases due to the fact that a large number of threads means a higher
contention rate for the resources. A higher contention rate, in combination
with spin locks, means that the threads will spend an increased amount of
time spinning/waiting for the locks to be released, so that they can acquire
them.

2.3.3: Mutex Wait Time:
Look at the average time per operation (vs # threads) and the average 
wait-for-mutex time (vs # threads).
Why does the average lock-wait time rise so dramatically with the number of
contenting threads?

The average lock-wait time rises so dramatically with the number of contending
threads for the same reasons as discussed in previous questions (i.e. larger
number of threads results in more contention for resources, and more time
waiting for locks to be released). As the goal of mutexes is to provide 
mutual exclusion, and prevent simultaneous updates to a critical section, this
means that only one thread can have access to a specific critical section (and
lock) at any given time. Therefore, more threads results in more waiting.

Why does the completion time per operation rise (less dramatically) with the
number of contending threads?

The completion time per operation rises less dramatically with the number of
competing threads due to the fact that there are always threads executing their
list operations, despite other threads having to wait for the locks. This 
provides some overlap in the mean completion time of operations, as there are
always operations being completed despite some threads having to wait for locks.
Moreover, this reduces the overall overhead imposed by the increased number of
threads as it is guaranteed that some threads are making progress in the program.
In contrast to Project 2a, which was implemented as a coarse-grained lock, thus
reducing the amount of threads that can be doing work in parallel, this list 
made use of fine-grained locking, and thus more threads are able to make progress
in parallel (i.e. leading to a less dramatic rise in completion time per op).

How is it possible for the wait time per operation to go up faster (or higher)
than the completion time per operation?

This is possible because the wait time per operation is an accumulation of all
the time that all of the threads are spending waiting for locks to be released.
This value is taken from each individual thread clock. On the other hand,
the completion time per operation is strictly per-process, and is a value
taken from the wall clock. Moreover, the wait time is a result of a thread
contending with other threads to acquire the lock (and for large number of 
threads, more waiting). However, the completion time is per operation, and thus
only relies on the time to complete a single operation.

2.3.4: Performance of Partitioned Lists:
Explain the change in performance of the synchronized methods as a function of 
the number of lists.

As shown from the graphs (lab2b_4.png and lab2b_5.png), the throughput increases
as the number of lists increases. With an increase in the number of lists, the
contention for resources is decreased, which is a direct result of fine-grained
locking. Instead of having just one large list, and putting a single lock (i.e.
coarse-grained locking), which reduces the parallelism of threads operating on
the list, multiple lists means that more threads can do work in parallel. This
makes more locks are available to access different critical sections (i.e. 
insert, lookup, delete, length), which reduces contention for resources.

Should the throughput continue increasing as the number of lists is further
increased? If not, explain why not.

The throughput will not continue to increase as the number of lists is further
increased. The reason for this is because threads are used to exploit parallel
operations, and make use of the cores provided by a computer system. However,
there is a limit to the number of cores a machine has, and eventually the number
of threads will overwhelm the available cores, and negatively affect the 
reason for using threads (i.e. parallelism). For example, if a machine has 16
cores, and 16 threads are used, then the throughput would be incredibly high, 
as increased parallelism is being exploited, and less resource contention occurs.
However, if the number of threads increases for the same number of cores, then
this would lead to more resource contention on the individual cores, resulting
in increased resource contention, more time waiting for locks, and less time
executing list operations.

It seems reasonable to suggest the throughput of an N-way partitioned list
should be equivalent to the throughput of a single list with fewer (1/N) 
threads. Does this appear to be true in the above curves? If not, explain 
why not.

It seems reasonable to assume this because in one case, if the number of
lists is increased, then the throughput would also increase due to less
resource contention and increased parallelism on the list, and in the other
case, a single list with fewer threads would have increased throughout due
to the fact there would be less resources contention due to the lower number
of threads (despite the coarse-grained lock). So, both cases result in 
increased throughput. Based off of my curves, this appears to slightly support 
the suggestion as the 1-way partitioned list with 1 thread has a throughput 
value that is roughly similar to a 4-way partitioned list with 4 or more threads.
However, this throughput equivalency of a 1-way partitioned list with 1 thread 
with 8- and 16-way partitioned lists on a higher number of threads is less 
similar (but the suggestion seems reasonable to assume).